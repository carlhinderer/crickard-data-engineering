----------------------------------------------------------------------------
|  CHAPTER 7 - FEATURES OF A PRODUCTION PIPELINE                           |
----------------------------------------------------------------------------

- Staging of Files

    - This first type of staging involves data in files following extraction from a source,
        usually a relational database.  These are often CSV files.


    - For example, we need to build a data pipeline that takes sales from the website and puts them
        into the data warehouse once an hour.  

      However, if some part of our processing fails, we cannot just re-query the database to get
        the same data.  This is because it is an operational database, and data is always changing.
        This is why we often stage data in files after retrieving it from the database - so we always
        have an original copy of the data.


    - Note that the way we have been using Airflow so far has taken this exact approach.



- Staging in Databases

    - While staging in files is useful for extraction, we often stage data in a database before
        loading it into a data warehouse.  Often we stage it into a replica of the data
        warehouse, and this way we can run some checks to make sure the results are as expected
        before moving it over to the production data warehouse.


    - Note that you can stage data at any point in your pipeline, not just at the beginning and end.
        Any place that you have staged data is much easier to debug, and it is also a place you can
        pick back up and retry after failures.


    - We have mostly used ETL processes in this book.  There is a growing shift towards ELT, in
        which you put data immediately into a database after extraction, and do all transformations
        on the copy.  This is especially useful if you're using SQL for the transformations.



- Validating Data with Great Expectations

    - If you have data saved in a file or database, you have the perfect time to validate it.
        We'll use the 'Great Expectations' library, which allows us to create declarative 
        validations.

      Note that we need Jupyter to view the generated documentation.


        # requirements.txt
        -----------------------------
        great_expectations
        jupyter


    - Now we can use it.  First, we'll create a new project directory and generate a file with
        CSV data.

        $ mkdir peoplepipeline
        $ cd peoplepipeline
        $ python generate_data.py


    - Now, we'll run the initializer for Great Expectations.

        $ great_expectations init


    - We'll create a new data source.

        $ great_expectations datasource new

        Settings:
          > File
          > pandas
          > Data file path
          > people.validate (or any name)


    - We'll create a new suite of expectations.

        # Create suite
        $ great_expectations suite new

        # Edit suite
        $ great_expectations suite edit people.validate.suite


    - Here are some examples of expectations we can use:

        # Expectations for schema
        batch.expect_table_row_count_to_be_between(max_value=1100, min_value=900)
        batch.expect_table_column_count_to_be_equal(value=8)

        batch.expect_table_columns_to_match_ordered_list(
            column_list=['name', 'age', 'street', 'city', 'state', 'zip', 'lng', 'lat']
        )
        

        # Expectations for individual columns
        batch.expect_column_values_to_not_be_null('age')

        batch.expect_column_min_to_be_between('age', max_value=19, min_value=17)
        batch.expect_column_max_to_be_between('age', max_value=81, min_value=79)
        batch.expect_column_mean_to_be_between('age', max_value=51.151, min_value=49.151)
        batch.expect_column_median_to_be_between('age', max_value=52.0, min_value=50.0)

        batch.expect_column_quantiles_to_be_between(
            'age',
            quantile_ranges={
                'quantiles': [0.05, 0.25, 0.5, 0.75, 0.95],
                'value_ranges': [[21, 23], [34, 36], [50, 52], [64, 66], [76, 78]]
            }
        )



- Great Expectations Outside the Pipeline

    - A 'tap' is how Great Expectations creates executable Python files to run against your
        expectation suite.  To create a new one:

        $ great_expectations tap new people.validate peoplevalidatescript.py


    - Note that if we are using Airflow, we can just call our expectation suite programatically.

        import sys

        from great_expectations import DataContext

        from airflow.exceptions import AirflowException
        from airflow import DAG
        from airflow.operators.bash_operator import BashOperator
        from airflow.operators.python_operator import PythonOperator
        
        def validateData():
            context = DataContext("/home/paulcrickard/peoplepipeline/great_expectations")
            suite = context.get_expectation_suite("people.validate")

            batch_kwargs = {
                "path": "/home/paulcrickard/peoplepipeline/people.csv",
                "datasource": "files_datasource",
                "reader_method": "read_csv",
            }

            batch = context.get_batch(batch_kwargs, suite)
            results = context.run_validation_operator("action_list_operator", [batch])

            if not results["success"]:
                raise AirflowException("Validation Failed")



- Building Idempotent Data Pipelines

    - When your pipeline fails, it must be able to be rerun to produce the same results.  There
        should not be duplicate records because your pipeline ran multiple times.


    - One approach is to use an upsert operation when copying data.  That way, if a record
        already exists, it will not get created again.


    - Another option is to create a new index (appended with a timestamp) every time your
        pipeline is run.  That way you'll just get a new index without changing any older ones.