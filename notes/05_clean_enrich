----------------------------------------------------------------------------
|  CHAPTER 5 - CLEANING, TRANSFORMATION, & ENRICHING DATA                  |
----------------------------------------------------------------------------

- Exploratory Data Analysis with pandas

    - In this chapter, we'll use e-scooter data from the City of Albuquerque.

        import pandas as pd

        df = pd.read_csv('scooter.csv')


    - Now, we can explore the data.

        # DataFrame schema
        df.columns
        df.dtypes

        # Look at data
        df.head()
        df.tail()


    - To select a subset of columns:

        # Look at more columns in results
        pd.set_option('display.max_columns', 500)

        # Look at a column of the DataFrame
        df['DURATION']


    - To select a subset of rows:

        # Look at several columns of the DataFrame
        df[['trip_id', 'DURATION', 'start_location_name']]

        # Look at a random sample of rows
        df.sample(5)

        # Take a slice
        df[:10]      # First 10 rows
        df[10:]      # Everything after the first 10 rows
        df[3:9]      # Rows 3-8

        # Get a row by row number
        df.loc[34221]

        # Get a value by row number and column name
        df.at[2, 'DURATION']


    - To select rows based on a condition:

        # Select based on a condition
        df.where(df['user_id'] == 8417864)

        # Alternate syntax
        df[(df['user_id'] == 8417864)]


    - You can also combine conditional statements:

        one = df['user_id'] == 8417864
        two = df['trip_ledger_id'] == 1488838
        df.where(one & two)



- Analyzing the Data with pandas

    - The 'describe' method will return summary statistics about each column.

        # Summary statistics about all columns
        df.describe()

        # Summary statistics about a single column
        df['start_location_name'].describe()


    - To get counts of each distinct values in a column:

        # Get frequencies as counts
        df['DURATION'].value_counts()

        # Get frequencies as percentages
        df['DURATION'].value_counts(normalize=True)

        # Separate data into bins and return bin counts
        df['trip_id'].value_counts(bins=10)



- Handling Common Data Issues with pandas

    - We may need to drop rows and columns in our data set.

        # Drop a column (and do it in place in the DataFrame)
        df.drop(columns=['region_id'], inplace=True)

        # Drop a row
        df.drop(index=[34225],inplace=True


    - Instead of dropping an entire column or row, we may want to drop them conditionally using
        'dropna'.

        'dropna' Arguments
        ------------------------
        axis      # Rows(0, default) or Columns(1)
        how       # Whether to drop row if any column is missing (any, default) or only if all are
        thresh    # Number of nulls that must be present to drop
        subset    # Specify a list of rows or columns to search
        inplace   # Modify existing DataFrame in place (defaults to False)

    
    - We'll drop rows that are missing a 'start_location_name' in our data.

        # Get rows that have missing start_location_name
        df['start_location_name'][(df['start_location_name'].isnull())]

        # Drop rows with 'start_location_name' missing
        df.dropna(subset=['start_location_name'],inplace=True)


    - We can also put a default value in place of nulls in either columns or rows.

        df.fillna(value='00:00:00', axis='columns')


    - Here, we take all the rows in which start_location and end_location are both null, and we
        fill them in with default values.

        startstop = df[(df['start_location_name'].isnull()) & (df['end_location_name'].isnull())]

        value = {'start_location_name': 'Start St.', 'end_location_name': 'Stop St.'}
        startstop.fillna(value=value)
        startstop[['start_location_name','end_location_name']]


    - Here, we drop all rows where the month is 'May'.

        # Get all rows where month is May
        may = df[(df['month'] == 'May')]

        # Use index from those rows to drop them
        df.drop(index=may.index, inplace=True)

        # Verify May rows have been dropped
        df['month'].value_counts()



- Creating and Modifying Columns

    - The 'rename' method can be used to rename columns.

        # Rename a column
        df.rename(columns={'DURATION':'duration'}, inplace=True)

        # Rename multiple columns
        df.rename(columns={'DURATION':'duration', 'region_id':'region'}, inplace=True)


    - To modify all the values in a column:

        # Make all values in a column uppercase
        df['month'] = df['month'].str.upper()


    - Here, we create a new column, and assign it a value of either the 'trip_id' or 'No' based on 
        the row's 'trip_id'.

        # Create a new column 'new_column' with values '1613335' or 'No'
        df.loc[df['trip_id'] == 1613335, 'new_column'] = '1613335'

        # Verify results
        df[['trip_id', 'new_column']].head()


    - Here, we split a DateTime column into separate date and time columns:

        # Get datetimes
        new = df['started_at'].str.split(expand=True)

        # Split into date and time columns and add to original DataFrame
        df['date'] = new[0]
        df['time'] = new[1]


    - We may just want to convert the DateTime format to make it easier to process.

        # Convert DateTime format
        df['started_at'] = pd.to_datetime(df['started_at'], format='%m/%d/%Y %H:%M')

        # Now we can use it in comparisons
        when = '2019-05-23'
        df[(df['started_at'] > when)]



- Enriching Data

    - Right now, we have starting locations as addresses, not coordinates.

        # Get top 5 most frequent starting locations
        new=pd.DataFrame(df['start_location_name'].value_counts().head())

        new.reset_index(inplace=True)
        new.columns=['address','count']


    - We'll get the address in a format that lets us look them up in our geocode file.

        # We only need the street address part
        n = new['address'].str.split(pat=',',n=1,expand=True)

        # Replace @ with and
        replaced = n[0].replace('@', 'and')

        # Replace address field
        new['street'] = replaced


    - Now, we'll read our file with the mapping from street addresses to coordinates.

        # Get geocode table
        geo = pd.read_csv('geocodedstreet.csv')

        # Join the data together
        merged = pd.merge(new, geo, on='street')

        # Columns in new DataFrame
        merged.columns