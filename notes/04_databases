----------------------------------------------------------------------------
|  CHAPTER 4 - WORKING WITH DATABASES                                      |
----------------------------------------------------------------------------

- Setting Up the SQL Database

    # Create database
    > CREATE DATABASE dataengineering;

    # Create table
    > CREATE TABLE users (
      name TEXT,
      id INTEGER,
      street TEXT,
      city TEXT,
      zip TEXT
    );



- Using psycopg2

    - First, we have to install the psycopg2 client:

        # Install client
        $ pip install psycopg2

        # Precompiled binary version
        $ pip install psycopg2-binary


    - Now, we can create a connection:

        import psycopg2 as db

        conn_string = "dbname='dataengineering' host='localhost' user='postgres' password='postgres'"
        conn = db.connect(conn_string)
        cur = conn.cursor()



- Inserting Data into Postgres

    - Now, we can insert data.

        query = "insert into users (id,name,street,city,zip) values(%s,%s,%s,%s,%s)"
        data = (1, 'Big Bird', 'Sesame Street', 'Fakeville', '12345')

        # We can use 'mogrify' to see query with arguments bound if we want
        cur.mogrify(query, data)

        # Now insert data
        cur.execute(query, data)

        # Commit transaction
        conn.commit()


    - To insert multiple rows:

        from faker import Faker

        fake = Faker()
        data = []
        i = 2

        for r in range(1000):
            data.append((i, fake.name(), fake.street_address(), fake.city(), fake.zipcode()))
            i += 1

        # Convert array into tuple of tuples
        data_for_db = tuple(data)

        # We can use 'executemany' to put all inserts into a single transaction
        cur.executemany(query, data_for_db)
        conn.commit()



- Selecting Data from Postgres

    - Now we can select with psycopg2:

        query = 'select * from users'
        cur.execute(query)

        # Iterate over results
        for record in cur:
            print(record)

        # Can use to get all records instead
        records = cur.fetchall()

        # Fetch a specified number of records
        records = cur.fetchmany(number_to_return)

        # Fetch a single record
        record = cur.fetchone()

        # Get number of records returned
        cur.rowcount

        # Current record position
        cur.rownumber


    - To write the results to a file:

        f = open('fromdb.csv', 'w')
        cur.copy_to(f, 'users', sep=',')
        f.close()


    - To select results into a pandas DataFrame:

        import pandas as pd

        df = pd.read_sql('select * from users', conn)
        df.to_json(orient='records')



- Using ElasticSearch

    - Install the elasticsearch library.

        $ pip install elasticsearch


    - To check the version of the library we are using:

        import elasticsearch

        elasticsearch.__version__


    - Note that we need to increase the memory map areas count to run the ElasticSearch cluster
        locally.

        $ sudo sysctl -w vm.max_map_count=262144
        $ docker-compose up



- Inserting Data into ElasticSearch

    - To insert Faker data into ElasticSearch:

        from elasticsearch import Elasticsearch
        from faker import Faker

        # Running locally
        es = Elasticsearch('https://elastic:espw1234@localhost:9200', 
                           ca_certs=False,
                           verify_certs=False)
        fake = Faker()

        doc = { "name": fake.name(),
                "street": fake.street_address(),
                "city": fake.city(),
                "zip": fake.zipcode() }

        res = es.index(index="users", document="doc")
        print(res['result']) # created


    - Now, we'll bulk insert 999 more users.

        from elasticsearch import helpers

        actions = [
            {
                '_index': 'users',
                '_document': {
                    'name': fake.name(),
                    'street': fake.street_address(),
                    'city': fake.city(),
                    'zip': fake.zipcode()
                }
            }
            for x in range(998)
        ]

        res = helpers.bulk(es, actions)
        print(res)


    - We can use Kibana to verify our new index.

        http://localhost:5601
          > Stack Management
          > Index Management



- Querying ElasticSearch

    - To query data from ElasticSearch:

        query = {"match_all": {}}
        res = es.search(index="users", query=query, size=10)

        # Print results
        for doc in res['hits']['hits']:
            print(doc['_source'])


    - Getting results back in JSON makes them easy to work with:

        from pandas.io.json import json_normalize

        df = json_normalize(res['hits']['hits'])


    - The 'match_all' query will return all results.  The 'match' query is used to match on a
        specific field.

        query = {'match': {'name':'Victoria Cobb'}}
        res = es.search(index="users", query=query, size=10)
        print(res['hits']['hits'][0]['_source'])


    - You can also use Lucene syntax for queries:

        res = es.search(index="users", q="name:Victoria Cobb", size=10)
        print(res['hits']['hits'][0]['_source'])



- More Advanced ElasticSearch Queries

    - ElasticSearch will tokenize strings with spaces in them, splitting them into multiple
        strings to search.  For example, if we search for 'Jamesberg', we'll get 'Jamesberg' 
        and 'Lake Jamesberg' back as results.

        # Get City Jamesberg - Returns Jamesberg and Lake Jamesberg
        query = {"match":{"city":"Jamesberg"}}}


    - You can use Boolean queries to specify multiple search criteria. For example, you can
        use must, must not, and should before your queries.

        # Get City Jamesberg but filter out zip code 63792
        query = {"bool":{"must":{"match":{"city":"Jamesberg"}},
                         "filter":{"term":{"zip":"63792"}}}}



- Using 'scroll' to Handle Larger Results

    - We may have a situation where we have 10,000 results and we need all of them.  
        ElasticSearch provides the 'scroll' method, which allows you to iterate over the 
        results to get them all.


    - Since we have 1000 records, we will retrieve 500 records at a time.  We have to pass in
        the length of time we want to make the paginated results available for.

        query = {"match_all": {}}

        res = es.search(
            index = 'users',
            query = query
            scroll = '20m',
            size = 500
        )

        # Save the '_scroll_id' and size of result set for use later
        sid = res['_scroll_id']
        size = res['hits']['total']['value']

        # Iterate through all the results
        while (size > 0):
            # Retrieve next batch of results
            res = es.scroll(scroll_id = sid, scroll = '20m')

            # Save details for next iteration
            sid = res['_scroll_id']
            size = len(res['hits']['hits'])

            # Do something with results
            for doc in res['hits']['hits']:
                print(doc['_source'])