----------------------------------------------------------------------------
|  CHAPTER 4 - WORKING WITH DATABASES                                      |
----------------------------------------------------------------------------

- Setting Up the SQL Database

    # Create database
    > CREATE DATABASE dataengineering;

    # Create table
    > CREATE TABLE users (
      name TEXT,
      id INTEGER,
      street TEXT,
      city TEXT,
      zip TEXT
    );



- Using psycopg2

    - First, we have to install the psycopg2 client:

        # Install client
        $ pip install psycopg2

        # Precompiled binary version
        $ pip install psycopg2-binary


    - Now, we can create a connection:

        import psycopg2 as db

        conn_string = "dbname='dataengineering' host='localhost' user='postgres' password='postgres'"
        conn = db.connect(conn_string)
        cur = conn.cursor()



- Inserting Data into Postgres

    - Now, we can insert data.

        query = "insert into users (id,name,street,city,zip) values(%s,%s,%s,%s,%s)"
        data = (1, 'Big Bird', 'Sesame Street', 'Fakeville', '12345')

        # We can use 'mogrify' to see query with arguments bound if we want
        cur.mogrify(query, data)

        # Now insert data
        cur.execute(query, data)

        # Commit transaction
        conn.commit()


    - To insert multiple rows:

        from faker import Faker

        fake = Faker()
        data = []
        i = 2

        for r in range(1000):
            data.append((i, fake.name(), fake.street_address(), fake.city(), fake.zipcode()))
            i += 1

        # Convert array into tuple of tuples
        data_for_db = tuple(data)

        # We can use 'executemany' to put all inserts into a single transaction
        cur.executemany(query, data_for_db)
        conn.commit()



- Selecting Data from Postgres

    - Now we can select with psycopg2:

        query = 'select * from users'
        cur.execute(query)

        # Iterate over results
        for record in cur:
            print(record)

        # Can use to get all records instead
        records = cur.fetchall()

        # Fetch a specified number of records
        records = cur.fetchmany(number_to_return)

        # Fetch a single record
        record = cur.fetchone()

        # Get number of records returned
        cur.rowcount

        # Current record position
        cur.rownumber


    - To write the results to a file:

        f = open('fromdb.csv', 'w')
        cur.copy_to(f, 'users', sep=',')
        f.close()


    - To select results into a pandas DataFrame:

        import pandas as pd

        df = pd.read_sql('select * from users', conn)
        df.to_json(orient='records')



- Using ElasticSearch

    - Install the elasticsearch library.

        $ pip install elasticsearch


    - To check the version of the library we are using:

        import elasticsearch

        elasticsearch.__version__


    - Note that we need to increase the memory map areas count to run the ElasticSearch cluster
        locally.

        $ sudo sysctl -w vm.max_map_count=262144
        $ docker-compose up



- Inserting Data into ElasticSearch

    - To insert Faker data into ElasticSearch:

        from elasticsearch import Elasticsearch
        from faker import Faker

        # Running locally
        es = Elasticsearch('https://elastic:espw1234@localhost:9200', 
                           ca_certs=False,
                           verify_certs=False)
        fake = Faker()

        doc = { "name": fake.name(),
                "street": fake.street_address(),
                "city": fake.city(),
                "zip": fake.zipcode() }

        res = es.index(index="users", document="doc")
        print(res['result']) # created


    - Now, we'll bulk insert 999 more users.

        from elasticsearch import helpers

        actions = [
            {
                '_index': 'users',
                '_document': {
                    'name': fake.name(),
                    'street': fake.street_address(),
                    'city': fake.city(),
                    'zip': fake.zipcode()
                }
            }
            for x in range(998)
        ]

        res = helpers.bulk(es, actions)
        print(res)


    - We can use Kibana to verify our new index.

        http://localhost:5601
          > Stack Management
          > Index Management



- Querying ElasticSearch

    - To query data from ElasticSearch:

        doc = { "query": {"match_all":{}} }
        res = es.search(index="users", body=doc, size=10)

        # Print results
        for doc in res['hits']['hits']:
            print(doc['_source'])


    - Getting results back in JSON makes them easy to work with:

        from pandas.io.json import json_normalize

        df = json_normalize(res['hits']['hits'])


    - The 'match_all' query will return all results.  The 'match' query is used to match on a
        specific field.

        doc = {"query":{"match":{"name":"Ronald Goodman"}}}
        res = es.search(index="users", body=doc, size=10)
        print(res['hits']['hits'][0]['_source'])