----------------------------------------------------------------------------
|  CHAPTER 3 - READING AND WRITING FILES                                   |
----------------------------------------------------------------------------

- CSVs with the Standard Library

    - To write a CSV with 1000 rows:

        from faker import Faker
        import csv

        output = open('data/data.CSV','w')
        fake = Faker()

        header = ['name','age','street','city','state','zip','lng','lat']
        mywriter = csv.writer(output)
        mywriter.writerow(header)

        for r in range(1000):
            mywriter.writerow([fake.name(),
                               fake.random_int(min=18, max=80, step=1), 
                               fake.street_address(), 
                               fake.city(),
                               fake.state(),
                               fake.zipcode(),
                               fake.longitude(),
                               fake.latitude()])
            output.close()


    - To read a CSV:

        import csv

        with open('data/data.csv') as f:
            myreader = csv.DictReader(f)
            headers = next(myreader)

            for row in myreader:
                print(row['name'])



- CSVs with Pandas

    - Writing CSVs with pandas

        data = {'Name': ['Paul','Bob','Susan','Yolanda'], 'Age': [23,45,18,21]}
        df = pd.DataFrame(data)
        df.to_csv('data/fromdf.csv', index=False)


    - Reading CSVs with pandas

        import pandas as pd
        df = pd.read_csv()('data.CSV')



- JSON with the Standard Library

    - Writing JSON 

        import JSON

        output = open('data/data.json', 'w')
        fake = Faker()

        all_data = {}
        all_data['records'] = []

        for x in range(1000):
            data = { "name" :   fake.name(),
                     "age" :    fake.random_int(min=18, max=80, step=1),
                     "street" : fake.street_address(),
                     "city" :   fake.city(),
                     "state" :  fake.state(),
                     "zip" :    fake.zipcode(),
                     "lng" :    float(fake.longitude()),
                     "lat" :    float(fake.latitude()) }
            all_data['records'].append(data)

        json.dump(all_data, output)


    - Reading JSON

        with open('data/data.json', 'r') as f:
            data = json.load(f)
            first_record = data['records'][0]
            print(first_record)



- JSON with pandas

    - For simple JSON files with one record per line, we can just use 'pd.read_json' and
        'pd.to_json'.  However, our example is slightly more complex since our records are 
        nested in a 'records' dictionary.


    - For more involved parsing:

        import pandas.io.json as pd_json

        f = open('data.json', 'r')
        data = pd_json.loads(f.read())
        df = pd_json.json_normalize(data, record_path='records')

        # Gets the first 2 rows, grouped by attribute
        df.head(2).to_json()

        # Gets the first 2 rows, grouped by row
        df.head(2).to_json(orient='records')



- Airflow DAG Presets

    a) @once
    b) @hourly – 0 * * * *
    c) @daily – 0 0 * * *
    d) @weekly – 0 0 * * 0
    e) @monthly – 0 0 1 * *
    f) @yearly – 0 0 1 1 *



- Specifying Relationships in Airflow

    # Set downstream (equivalent)
    print_starting_task.set_downstream(csv_to_json_task)
    print_starting_task >> csv_to_json_task

    # Set upstream (equivalent)
    csv_to_json_task.set_upstream(print_starting_task)
    csv_to_json_task << print_starting_task



- Building a CSV to JSON Pipeline in Airflow

    - First, we create a simple DAG and put it in a new DAGs directory.



