----------------------------------------------------------------------------
|  CHAPTER 3 - READING AND WRITING FILES                                   |
----------------------------------------------------------------------------

- CSVs with the Standard Library

    - To write a CSV with 1000 rows:

        from faker import Faker
        import csv

        output = open('data/data.CSV','w')
        fake = Faker()

        header = ['name','age','street','city','state','zip','lng','lat']
        mywriter = csv.writer(output)
        mywriter.writerow(header)

        for r in range(1000):
            mywriter.writerow([fake.name(),
                               fake.random_int(min=18, max=80, step=1), 
                               fake.street_address(), 
                               fake.city(),
                               fake.state(),
                               fake.zipcode(),
                               fake.longitude(),
                               fake.latitude()])
            output.close()


    - To read a CSV:

        import csv

        with open('data/data.csv') as f:
            myreader = csv.DictReader(f)
            headers = next(myreader)

            for row in myreader:
                print(row['name'])



- CSVs with Pandas

    - Writing CSVs with pandas

        data = {'Name': ['Paul','Bob','Susan','Yolanda'], 'Age': [23,45,18,21]}
        df = pd.DataFrame(data)
        df.to_csv('data/fromdf.csv', index=False)


    - Reading CSVs with pandas

        import pandas as pd
        df = pd.read_csv()('data.CSV')



- JSON with the Standard Library

    - Writing JSON 

        from faker import Faker
        import json

        output = open('data/data.json', 'w')
        fake = Faker()

        all_data = {}
        all_data['records'] = []

        for x in range(1000):
            data = { "name" :   fake.name(),
                     "age" :    fake.random_int(min=18, max=80, step=1),
                     "street" : fake.street_address(),
                     "city" :   fake.city(),
                     "state" :  fake.state(),
                     "zip" :    fake.zipcode(),
                     "lng" :    float(fake.longitude()),
                     "lat" :    float(fake.latitude()) }
            all_data['records'].append(data)

        json.dump(all_data, output)


    - Reading JSON

        with open('data/data.json', 'r') as f:
            data = json.load(f)
            first_record = data['records'][0]
            print(first_record)



- JSON with pandas

    - For simple JSON files with one record per line, we can just use 'pd.read_json' and
        'pd.to_json'.  However, our example is slightly more complex since our records are 
        nested in a 'records' dictionary.


    - For more involved parsing:

        import pandas.io.json as pd_json

        f = open('data.json', 'r')
        data = pd_json.loads(f.read())
        df = pd_json.json_normalize(data, record_path='records')

        # Gets the first 2 rows, grouped by attribute
        df.head(2).to_json()

        # Gets the first 2 rows, grouped by row
        df.head(2).to_json(orient='records')



- Airflow DAG Presets

    a) @once
    b) @hourly – 0 * * * *
    c) @daily – 0 0 * * *
    d) @weekly – 0 0 * * 0
    e) @monthly – 0 0 1 * *
    f) @yearly – 0 0 1 1 *



- Specifying Relationships in Airflow

    # Set downstream (equivalent)
    print_starting_task.set_downstream(csv_to_json_task)
    print_starting_task >> csv_to_json_task

    # Set upstream (equivalent)
    csv_to_json_task.set_upstream(print_starting_task)
    csv_to_json_task << print_starting_task



- Building a CSV to JSON Pipeline in Airflow

    - First, we'll create a shared data folder and add it to our Airflow docker-compose.yml.
        This way, we can easily pass data files to Airflow.


    - Next, we create a simple DAG and put it in the local DAGs directory created by the Airflow
        docker-compose, which is also mapped into the Airflow sercvices.


    - Now, when we start the web server, we can see our DAG in the list of DAGS to run.



- Handling Files Using Nifi Processors

    - Creating flows in Nifi takes more steps than creating simple conversion in Airflow.  The 
        tradeoff is that the GUI makes the series of steps understandable, even to non-developers.


    - In this flow, we'll use these steps:

        1. GetFile (to read the 'data.csv' file)
        2. SplitRecord (separates each row into a separate FlowFile)
        3. QueryRecord (runs a sql query to filter the rows we want)
        4. ExtractText (search through an attribute with a regex to find user's name)
        5. UpdateAttribute (add a name attribute to each row with the user's name)
        6. PutFile (save the updated csv records to disk)

      We'll create a data pipeline that reads in the 'data.csv' file, runs a query for all people
        over 40, and record the results to a file.
