-----------------------------------------------------------------------------
| CHAPTER 7 - INGESTION                                                     |
-----------------------------------------------------------------------------

- Data Ingestion

    - 'Data ingestion' is the process of moving from one place to another.  If DE pipelines, it
        implies data is being moved from source systems into storage to support the DE lifecycle.


    - A 'data pipeline' is the combination of architecture, systems, and processes that move data
        through the stages of the DE lifecycle.



- Key Engineering Considerations

    - Questions to ask about ingestion

        - What’s the use case for the data I’m ingesting?

        - Can I reuse this data and avoid ingesting multiple versions of the same dataset?

        - Where is the data going? What’s the destination?

        - How often should the data be updated from the source?

        - What is the expected data volume?

        - What format is the data in? Can downstream storage and transformation accept this format?

        - Is the source data in good shape for immediate downstream use?  That is, is the data of good
            quality? What post-processing is required to serve it?  What are data-quality risks 
            (e.g., could bot traffic to a website contaminate the data)?

        - Does the data require in-flight processing for downstream ingestion if the data is from a 
            streaming source?        


    - Architectural considerations

        - Bounded versus unbounded

        - Frequency

        - Synchronous versus asynchronous

        - Serialization and deserialization

        - Throughput and elastic scalability

        - Reliability and durability

        - Payload

        - Push versus pull versus poll 



- Bounded vs Unbounded Data

    - 'Unbouded data' is data as it exists in reality, events happen either sporadically or
        continuously.  'Bounded data' is a convenient way of bucketing data across some boundary, such
        as time.


    - All data is unbounded until it is bounded.  Business processes have long imposed artificial
        bounds on data by cutting discrete batches.


    - Streaming systems are a tool for preserving the unbouded nature of data, so that subsequent
        steps can process it continuously.



- Frequency

    - The data-ingestion frequency is a critical decision when designing pipelines.  Ingestion processes
        can be batch, micro-batch, or real-time.


    - On the slow end, a business might ship it's tax data to an accounting firm once a year.  On the
        faster side, a CDC system could retrive new log updates from a source database once a minute.
        An IoT sensor might ingest and process events in seconds.


    - ML models are still typically trained on a batch basis, although continuous online training is
        becoming more common.



- Synchronous vs Asynchronous Ingestion

    - With 'synchronous ingestion', the source, ingestion, and destination have complex dependencies 
        and are tightly coupled.  This is common in older ETL systems.

        Process A -> Process B -> Process C   (Sequentially, if one fails we stop)


    - For one example of how this can go wrong, one company had a pipeline with dozens of tightly
        coupled workflows that took > 24 hours to complete.  If any one stage failed, the entire
        pipeline had to be restarted from the beginning.  Any time there was a problem, reports would
        be delayed for days.


    - With 'asynchronous ingestion', dependencies can operate at the level of individual events,
        similarly to the microservices approach.  Individual events become available in storage as
        soon as they are ingested individually.


    - Asynchronous ingestion example

        App  ->  Kinesis Data Stream  ->  Beam  ->  Kinesis Data Stream  ->  Kinesis Firehose  ->  S3


        1. A web app emits events into Kinesis data stream (which acts as a buffer here)

        2. The stream is read by Beam, which parses and enriches events

        3. Beam forwards the event to a second Kinesis data stream

        4. Kinesis Data Firehose rolls up events and writes objects to S3



- Serialization and Deserialization

    - When ingesting data, we need to ensure the destination can deserialize data from the source.



- Throughput and Scalability

    - In theory, ingestion should never be a bottleneck.  In practice, bottlenecks are pretty standard.
        Design system to grow and shrink according to desired throughput.  This is most easily done
        using managed tools to handle the scaling.


    - If a source database goes down, and then comes back up and attempts to backfill the lapsed
        data loads, will your ingestion be able to keep up with the backlog?


    - Data generation rarely happens at a constant rate, and often ebbs and flows.  We need to be able
        to handle the bursts.



- Reliability and Durability

    - We need to build an appropriate level of redundancy and self-healing based on the cost of losing
        data.



- Payload

    - A 'payload' is the dataset you're ingesting and has characteristics such as kind, shape, size,
        schema, data types, and metadata.

        Kind = type (ie tabular, image, video, text) and format (ie csv, parquet, jpeg)

        Shape = dimensions (ie number of rows/columns, RGB values for pixels, # of channels in audio)

        Size = number of bytes in a payload (bytes to TB, may be compressed if large)

        Schema and Data Types = ie relational DB schema, API contract

        Metadata = data about data


    - Making changes to schemas requires good communication with consumers.  'Schema registries' are
        metadata repositories use to maintain schemas and integrity constraints.



- Push vs Pull vs Poll Patterns

    - A 'push' strategy involves a source system sending data to a target, while a 'pull' strategy
        involves the target reading data directly from the source.

     'Polling' involves periodically checking a data source for any changes.  When changes are 
        detected, the data is pulled by the target.



- Batch Ingestion Considerations

    - Batch ingestion, which ivolves processing data in bulk, is often a convenient way to ingest data.
        Data is ingested by taking a subset of data from a source system, either based on a time
        interval or the size of accumulated data.


    - 'Time-interval batch ingestion' is widespread in traditional ETL.  The pattern is often to
        process data once a day, but other intervals are used also.


    - 'Size-based batch ingestion' is common when dta is moved from a streaming-based system into
        object storage.  The size criteria could be size in bytes of number of events.


    - Commonly-used batch ingestion patterns include:

        - Snapshot or differential extraction

        - File-based export and ingestion

        - ETL vs ELT

        - Inserts, updates, and batch size

        - Data migration



- Snapshot or Differential Extraction

    - DE's need to decide whether to take full snapshots of a source system or 'differential'
        (aka 'incremental') updates.


    - With differential updates, we pull only the changes and updates since the last read from the
        source system.


    - Differential updates save a lot of target storage and network bandwidth, but full snapshots are
        still common because of their simplicity.



- File-Based Export and Ingestion

    - Data is often moved between databases and systems using files.  Data is serialized into files,
        and the files are provided to an ingestion system.


    - This is a push-based ingestion patern, since the data export is done on the source side.


    - Advantages of this:

        - May be undesirable to access source system directly for security reasons

        - Source system has complete control over what gets exported


    - Common file-exchange methods include SFTP, EDI, and SCP.



- ETL vs ELT

    - 'Extract' is getting data from the source system.  It may be push or pull-based.


    - Once the data is extracted, it can be transformed (ETL) before loading it into a storage
        destination, or simply loaded into storage for future transformation (ELT).



- Insert, Updates, and Batch Size

    - Batch-oriented systems often perform poorly when users attempt to perform many small-batch
        operations rather than a smaller number of large operations.

      While it is common to insert one row at a time in OLTP databases, this a bad pattern for OLAP
        databases.  Running lots of small in-place operations are even worse for performance.


    - Some technologies are purposely-built for high insert rates, like Apache Druid.  Know the
        characteristics of the tools you are going to use.



- Data Migration

    - Migrating data from one system to another is not trivial (it might even be PB), and data needs
        to be moved in bulk.


    - Schema management, and handling the differences in schemas between the systems, is crucial
        during migrations.  Test ingestion of small amounts of data to find issues before undertaking
        the entire migration.


    - File or object storage is often an excellent intermediate place for transferring data.



- Message and Stream Ingestion Considerations

    - Schema Evolution

        - Schema evolution is common when handling event data.  Fields may be added or removed, or
            value types might change.  An IoT sensor might get a firmware update, and suddently
            start sending additional fields.

        - If you event-processing framework has a schema registry, use it to version your schemas.

        - A dead-letter-queue can help you investigate issues with events that are not properly
            handled.

        - Also, try to communicate with upstream stakeholders to learn about schema changes before
            they actually happen.


    - Late-Arriving Data

        - Event data may arrive late for a variety of reasons.  Do not assume that ingestion or
            process time is the same as event time.

        - To handle late-arriving data, set a cutoff time for when late-arriving data will no longer
            be processed.


    - Ordering and Multiple Delivery

        - Messages may be delivered out of order and more-than-once.


    - Replay

        - 'Replay' allows readers to request a range of messages from the history, allowing you to
            rewind your event history to a particular point in time.

        - This is very helpful when you need to re-ingest or reprocess data for a specific time 
            range.

        - For example, RabbitMQ typically deletes messages after all subscribers have consumed them.
            Kafka, Kinesis, and Pub/Sub all support event retention and replay.


    - TTL

        - A key parameter is the 'maximum message retention time', aka the TTL.  TTL is a configuration
            you'll set for how long you want your events to live before they are acknowledged and
            ingested.

        - Any message not ingested after it's TTL is automatically deleted.

        - This is helpful to reduce backpressure and unnecessary event volume in your event-ingestion
            pipeline.

        - Google Pub/Sub supports retention up to 7 days.  Kinesis supports up to 1 year.  Kafka can
            be configured for indefinite retention.


    - Error Handling and Dead-Letter Queues

        - An event might be sent to a nonexistent topic or queue, the size might be too large, or it
            might be past it's TTL.  Events that cannot be ingested should be re-routed to a 
            dead-letter queue.

        - We should look at the events in this queue to diagnose why this is occurring and resolve
            the problem.


    - Consumer Pull and Push

        - A consumer subscribing to a topic can get events in 2 ways: push and pull.

        - Kafka and Kinesis only support pull subscriptions.  Subscribers read messages from a topic
            and confirm when they have been processed.  This is the default choice for DE applications.

        - Pub/Sub and Rabbit MQ support pull and push, in which the services write messages to a
            listener.


    - Location

        - It is often desirable to integrate streaming across several locations for enhanced redundancy
            and to consume data close to where it is generated.

        - We need to balance this across the costs of moving data across regions and running
            analytics on a combined dataset.  Data egress costs can spiral quickly.



- Direct Database Connection

    - Data can be pulled directly from databases for ingestion by querying and reading over a network
        connection.  This connection is commonly made using ODBC or JDBC.


    - ODBC uses a driver hosted by a client accessing the database to translate commands issued to the
        standard ODBC API into commands issued to the database. The database returns query results over
        the wire, where the driver receives them and translates them back into a standard form and read
        by the client.


    - JDBC is very similar to ODBC, except that the driver is written in Java.  This is very portable
        since it runs on the JVM, and is compiled using the JIT.


    - JDBC provides extraordinay portability.  ODBC drivers are shipped as OS and architecture-native
        binaries.  DB vendors must maintain versions for each architecture/OS version that they wish to
        support.  

      On the other hand, vendors can ship a single JDBC driver compatible with any JVM language or
        JVM data framework.  It is also commonly used in non-JVM languages like Python.


    - Many data frameworks can parallelize several simultaneous connections and partition queries to
        pull data in parallel.  This puts extra load on the DB, though.


    - Since these drivers are row-based and struggle with nested data, many databases now export
        directly to Parquet/ORC/Avro or through REST APIs.



- Change Data Capture

    - CDC is the process of ingesting changes from a source DB system.  For example, we might have a
        source PostgreSQL that ingests table changes for analytics.


    - Batch-Oriented CDC

        - If the database table has an 'updated_at' field containing the last time a record was written
            or updated, we can query the table to find all the updated rows since a specific time.

        - A key limitation is that while we have the current state of each row that was changed, we
            don't have the entire history of changes.

        - For instance, you get a current bank account balance, but you don't get all the debits and
            credits.

        - We can mitigate this problem using an insert-only schema, where each transaction is recorded
            as a new record in the table.


    - Continuous CDC

        - Continuous CDC captures all table history and can support near-real-time ingestion.  CDC
            treats each write to the DB as an event.

        - One of the most common approaches with OLTP databases is 'log-based CDC'.  The DB's binary
            log records every change to the DB sequentially.  A CDC tool can read this log and
            send events to a target such as Kafka.

        - Some cloud DBs support a simple CDC paradigm where every change to the DB automatically 
            triggers a serverless function or write to event stream.


    - CDC and Database Replication

        - CDC can be used to replicate between databases.  Events are buffered into a stream and
            asynchronously written to the second database.

        - Many databases use synchronous replication instead.  This keeps replicas fully in sync
            and supports consistent read replicas.

        - Read replicas are often used in batch ingestion to avoid overloading the primary DB.


    - CDC Considerations

        - CDC puts an extra load on the primary data store, and we should be aware of this to avoid
            operational problems.

        - We may want to run queries during off-hours or using read replicas.



- APIs

    - A typical organization may have hundreds of external data sources such as SaaS platforms or
        partner companies.


    - Since no hard standards exist for data interchange over APIs, DE's can expect to spend a lot
        of time reading docs, communicating with data owners, and writing/maintaining API connection
        code.


    - 3 trends are slowly improving this situation:
    
        1. Many vendors are now providing client libraries for various PLs

        2. Numerous data connector platforms are now available as SaaS or open source platforms, which
             offer frameworks for creating custom connectors

        3. Emergence of data sharing through standard platforms like BigQuery, Snowflake, Redshift, or S3


    - Reserve your custom connection work for API's that aren't well-supported by existing frameworks.



- Message Queues and Event-Streaming Platforms

    - Message queues and event-streaming platforms are widespread ways to ingest real-time data from 
        web and mobile applications, IoT sensors, and smart devices.


    - A 'message' is handled at the individual event level, and is meant to be transient.  Once a 
        message is consumed, it is acknowldeged and removed from the queue.

      A 'stream' ingests events into an ordered log.  The log persists for as long as you wish, 
        allowing events to be queried over various ranges, aggregated, and combined with other streams
        to create new transformations published to downstream consumers.


        Producer1  ->  Consumer1  \
                                      Combine Data  ->  Publish 1 + 2 To Producer3
        Producer2  ->  Consumer2  /


    - Batch processing usually involves static workflows (ingest data, store it, transform it, serve it).

      Messages and streams are fluid.  Ingestion can be nonlinear, with data being published, 
        consumed, replenished, and re-consumed.


    - When building real-time data pipelines, messages and events should flow with as little latency
        as possible.  Make sure to provide sufficient resources for this.  Managed services work well 
        for real-time ingestion.



- Managed Data Connectors

    - If you find yourself writing a connector to an API, you should first look into whether one has
        already been created.  

    - Vendors and OSS projects typically have lots of prebuilt connector options.



- Moving Data with Object Storage

    - Object storage is a multitenant system in public clouds, and it supports storing massive amounts
        of data.  This makes it ideal for moving data in and out of data lakes, between teams, and
        between organizations.


    - You can provide short-term access to an object with a signed URL, giving a user temporary
        permission.  This makes object a very secure and optimal way to handle file exchange.



- EDI (Electronic Data Interchange)

    - DE's will also likely encounter 'EDI'.  This usually refers to a somewhat archaic means of
        file exchange, such as by email or flash drive.  Some sources do not support more modern means
        of transport due to old systems or human process limitations.

    - DE's can try to make these systems as automated as possible.  For instance, they can set up a
        cloud-based email server that saves files to storage as soon as they are received.  This can
        trigger orchestration processes to ingest and process data.



- Databases and File Export



- Practical Issues with Common Formats



- Shell

    - 


- SSH


- SFTP and SCP


- Webhooks


- Web Interface


- Web Scraping


- Transfer Appliances for Data Migration


- Data Sharing