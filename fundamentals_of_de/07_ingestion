-----------------------------------------------------------------------------
| CHAPTER 7 - INGESTION                                                     |
-----------------------------------------------------------------------------

- Data Ingestion

    - 'Data ingestion' is the process of moving from one place to another.  If DE pipelines, it
        implies data is being moved from source systems into storage to support the DE lifecycle.


    - A 'data pipeline' is the combination of architecture, systems, and processes that move data
        through the stages of the DE lifecycle.



- Key Engineering Considerations

    - Questions to ask about ingestion

        - What’s the use case for the data I’m ingesting?

        - Can I reuse this data and avoid ingesting multiple versions of the same dataset?

        - Where is the data going? What’s the destination?

        - How often should the data be updated from the source?

        - What is the expected data volume?

        - What format is the data in? Can downstream storage and transformation accept this format?

        - Is the source data in good shape for immediate downstream use?  That is, is the data of good
            quality? What post-processing is required to serve it?  What are data-quality risks 
            (e.g., could bot traffic to a website contaminate the data)?

        - Does the data require in-flight processing for downstream ingestion if the data is from a 
            streaming source?        


    - Architectural considerations

        - Bounded versus unbounded

        - Frequency

        - Synchronous versus asynchronous

        - Serialization and deserialization

        - Throughput and elastic scalability

        - Reliability and durability

        - Payload

        - Push versus pull versus poll 



- Bounded vs Unbounded Data

    - 'Unbouded data' is data as it exists in reality, events happen either sporadically or
        continuously.  'Bounded data' is a convenient way of bucketing data across some boundary, such
        as time.


    - All data is unbounded until it is bounded.  Business processes have long imposed artificial
        bounds on data by cutting discrete batches.


    - Streaming systems are a tool for preserving the unbouded nature of data, so that subsequent
        steps can process it continuously.



- Frequency

    - The data-ingestion frequency is a critical decision when designing pipelines.  Ingestion processes
        can be batch, micro-batch, or real-time.


    - On the slow end, a business might ship it's tax data to an accounting firm once a year.  On the
        faster side, a CDC system could retrive new log updates from a source database once a minute.
        An IoT sensor might ingest and process events in seconds.


    - ML models are still typically trained on a batch basis, although continuous online training is
        becoming more common.



- Synchronous vs Asynchronous Ingestion

    - With 'synchronous ingestion', the source, ingestion, and destination have complex dependencies 
        and are tightly coupled.  This is common in older ETL systems.

        Process A -> Process B -> Process C   (Sequentially, if one fails we stop)


    - For one example of how this can go wrong, one company had a pipeline with dozens of tightly
        coupled workflows that took > 24 hours to complete.  If any one stage failed, the entire
        pipeline had to be restarted from the beginning.  Any time there was a problem, reports would
        be delayed for days.


    - With 'asynchronous ingestion', dependencies can operate at the level of individual events,
        similarly to the microservices approach.  Individual events become available in storage as
        soon as they are ingested individually.


    - Asynchronous ingestion example

        App  ->  Kinesis Data Stream  ->  Beam  ->  Kinesis Data Stream  ->  Kinesis Firehose  ->  S3


        1. A web app emits events into Kinesis data stream (which acts as a buffer here)

        2. The stream is read by Beam, which parses and enriches events

        3. Beam forwards the event to a second Kinesis data stream

        4. Kinesis Data Firehose rolls up events and writes objects to S3



- Serialization and Deserialization

    - When ingesting data, we need to ensure the destination can deserialize data from the source.



- Throughput and Scalability

    - In theory, ingestion should never be a bottleneck.  In practice, bottlenecks are pretty standard.
        Design system to grow and shrink according to desired throughput.  This is most easily done
        using managed tools to handle the scaling.


    - If a source database goes down, and then comes back up and attempts to backfill the lapsed
        data loads, will your ingestion be able to keep up with the backlog?


    - Data generation rarely happens at a constant rate, and often ebbs and flows.  We need to be able
        to handle the bursts.



- Reliability and Durability

    - We need to build an appropriate level of redundancy and self-healing based on the cost of losing
        data.



- Payload

    - A 'payload' is the dataset you're ingesting and has characteristics such as kind, shape, size,
        schema, data types, and metadata.

        Kind = type (ie tabular, image, video, text) and format (ie csv, parquet, jpeg)

        Shape = dimensions (ie number of rows/columns, RGB values for pixels, # of channels in audio)

        Size = number of bytes in a payload (bytes to TB, may be compressed if large)

        Schema and Data Types = ie relational DB schema, API contract

        Metadata = data about data


    - Making changes to schemas requires good communication with consumers.  'Schema registries' are
        metadata repositories use to maintain schemas and integrity constraints.



- Push vs Pull vs Poll Patterns

    - A 'push' strategy involves a source system sending data to a target, while a 'pull' strategy
        involves the target reading data directly from the source.

     'Polling' involves periodically checking a data source for any changes.  When changes are 
        detected, the data is pulled by the target.



- Batch Ingestion Considerations

    - Batch ingestion, which ivolves processing data in bulk, is often a convenient way to ingest data.
        Data is ingested by taking a subset of data from a source system, either based on a time
        interval or the size of accumulated data.


    - 'Time-interval batch ingestion' is widespread in traditional ETL.  The pattern is often to
        process data once a day, but other intervals are used also.


    - 'Size-based batch ingestion' is common when dta is moved from a streaming-based system into
        object storage.  The size criteria could be size in bytes of number of events.


    - Commonly-used batch ingestion patterns include:

        - Snapshot or differential extraction

        - File-based export and ingestion

        - ETL vs ELT

        - Inserts, updates, and batch size

        - Data migration



- Snapshot or Differential Extraction

    - DE's need to decide whether to take full snapshots of a source system or 'differential'
        (aka 'incremental') updates.


    - With differential updates, we pull only the changes and updates since the last read from the
        source system.


    - Differential updates save a lot of target storage and network bandwidth, but full snapshots are
        still common because of their simplicity.



- File-Based Export and Ingestion

    - Data is often moved between databases and systems using files.  Data is serialized into files,
        and the files are provided to an ingestion system.


    - This is a push-based ingestion patern, since the data export is done on the source side.


    - Advantages of this:

        - May be undesirable to access source system directly for security reasons

        - Source system has complete control over what gets exported


    - Common file-exchange methods include SFTP, EDI, and SCP.



- ETL vs ELT

    - 'Extract' is getting data from the source system.  It may be push or pull-based.


    - Once the data is extracted, it can be transformed (ETL) before loading it into a storage
        destination, or simply loaded into storage for future transformation (ELT).



- Insert, Updates, and Batch Size

    - Batch-oriented systems often perform poorly when users attempt to perform many small-batch
        operations rather than a smaller number of large operations.

      While it is common to insert one row at a time in OLTP databases, this a bad pattern for OLAP
        databases.  Running lots of small in-place operations are even worse for performance.


    - Some technologies are purposely-built for high insert rates, like Apache Druid.  Know the
        characteristics of the tools you are going to use.



- Data Migration

    - Migrating data from one system to another is not trivial (it might even be PB), and data needs
        to be moved in bulk.


    - Schema management, and handling the differences in schemas between the systems, is crucial
        during migrations.  Test ingestion of small amounts of data to find issues before undertaking
        the entire migration.


    - File or object storage is often an excellent intermediate place for transferring data.



- Message and Stream Ingestion Considerations

    - Schema Evolution

        - Schema evolution is common when handling event data.  Fields may be added or removed, or
            value types might change.  An IoT sensor might get a firmware update, and suddently
            start sending additional fields.

        - If you event-processing framework has a schema registry, use it to version your schemas.

        - A dead-letter-queue can help you investigate issues with events that are not properly
            handled.

        - Also, try to communicate with upstream stakeholders to learn about schema changes before
            they actually happen.


    - Late-Arriving Data

        - Event data may arrive late for a variety of reasons.  Do not assume that ingestion or
            process time is the same as event time.

        - To handle late-arriving data, set a cutoff time for when late-arriving data will no longer
            be processed.


    - Ordering and Multiple Delivery

        - Messages may be delivered out of order and more-than-once.


    - Replay

        - 'Replay' allows readers to request a range of messages from the history, allowing you to
            rewind your event history to a particular point in time.

        - This is very helpful when you need to re-ingest or reprocess data for a specific time 
            range.

        - For example, RabbitMQ typically deletes messages after all subscribers have consumed them.
            Kafka, Kinesis, and Pub/Sub all support event retention and replay.


    - TTL

        - A key parameter is the 'maximum message retention time', aka the TTL.  TTL is a configuration
            you'll set for how long you want your events to live before they are acknowledged and
            ingested.

        - Any message not ingested after it's TTL is automatically deleted.

        - This is helpful to reduce backpressure and unnecessary event volume in your event-ingestion
            pipeline.

        - Google Pub/Sub supports retention up to 7 days.  Kinesis supports up to 1 year.  Kafka can
            be configured for indefinite retention.


    - Error Handling and Dead-Letter Queues

        - An event might be sent to a nonexistent topic or queue, the size might be too large, or it
            might be past it's TTL.  Events that cannot be ingested should be re-routed to a 
            dead-letter queue.

        - We should look at the events in this queue to diagnose why this is occurring and resolve
            the problem.


    - Consumer Pull and Push

        - A consumer subscribing to a topic can get events in 2 ways: push and pull.

        - Kafka and Kinesis only support pull subscriptions.  Subscribers read messages from a topic
            and confirm when they have been processed.  This is the default choice for DE applications.

        - Pub/Sub and Rabbit MQ support pull and push, in which the services write messages to a
            listener.


    - Location

        - It is often desirable to integrate streaming across several locations for enhanced redundancy
            and to consume data close to where it is generated.

        - We need to balance this across the costs of moving data across regions and running
            analytics on a combined dataset.  Data egress costs can spiral quickly.



- Direct Database Connection

    - Data can be pulled directly from databases for ingestion by querying and reading over a network
        connection.  This connection is commonly made using ODBC or JDBC.


    - ODBC uses a driver hosted by a client accessing the database to translate commands issued to the
        standard ODBC API into commands issued to the database. The database returns query results over
        the wire, where the driver receives them and translates them back into a standard form and read
        by the client.


    - JDBC is very similar to ODBC, except that the driver is written in Java.  This is very portable
        since it runs on the JVM, and is compiled using the JIT.


    - JDBC provides extraordinay portability.  ODBC drivers are shipped as OS and architecture-native
        binaries.  DB vendors must maintain versions for each architecture/OS version that they wish to
        support.  

      On the other hand, vendors can ship a single JDBC driver compatible with any JVM language or
        JVM data framework.  It is also commonly used in non-JVM languages like Python.


    - Many data frameworks can parallelize several simultaneous connections and partition queries to
        pull data in parallel.  This puts extra load on the DB, though.


    - Since these drivers are row-based and struggle with nested data, many databases now export
        directly to Parquet/ORC/Avro or through REST APIs.



- Change Data Capture

    - CDC is the process of ingesting changes from a source DB system.  For example, we might have a
        source PostgreSQL that ingests table changes for analytics.


    - Batch-Oriented CDC

        - If the database table has an 'updated_at' field containing the last time a record was written
            or updated, we can query the table to find all the updated rows since a specific time.

        - A key limitation is that while we have the current state of each row that was changed, we
            don't have the entire history of changes.

        - For instance, you get a current bank account balance, but you don't get all the debits and
            credits.

        - We can mitigate this problem using an insert-only schema, where each transaction is recorded
            as a new record in the table.


    - Continuous CDC

        - Continuous CDC captures all table history and can support near-real-time ingestion.  CDC
            treats each write to the DB as an event.

        - One of the most common approaches with OLTP databases is 'log-based CDC'.  The DB's binary
            log records every change to the DB sequentially.  A CDC tool can read this log and
            send events to a target such as Kafka.

        - Some cloud DBs support a simple CDC paradigm where every change to the DB automatically 
            triggers a serverless function or write to event stream.


    - CDC and Database Replication

        - CDC can be used to replicate between databases.  Events are buffered into a stream and
            asynchronously written to the second database.

        - Many databases use synchronous replication instead.  This keeps replicas fully in sync
            and supports consistent read replicas.

        - Read replicas are often used in batch ingestion to avoid overloading the primary DB.


    - CDC Considerations

        - CDC puts an extra load on the primary data store, and we should be aware of this to avoid
            operational problems.

        - We may want to run queries during off-hours or using read replicas.



- APIs

    - A typical organization may have hundreds of external data sources such as SaaS platforms or
        partner companies.


    - Since no hard standards exist for data interchange over APIs, DE's can expect to spend a lot
        of time reading docs, communicating with data owners, and writing/maintaining API connection
        code.


    - 3 trends are slowly improving this situation:
    
        1. Many vendors are now providing client libraries for various PLs

        2. Numerous data connector platforms are now available as SaaS or open source platforms, which
             offer frameworks for creating custom connectors

        3. Emergence of data sharing through standard platforms like BigQuery, Snowflake, Redshift, or S3


    - Reserve your custom connection work for API's that aren't well-supported by existing frameworks.



- Message Queues and Event-Streaming Platforms

    - Message queues and event-streaming platforms are widespread ways to ingest real-time data from 
        web and mobile applications, IoT sensors, and smart devices.


    - A 'message' is handled at the individual event level, and is meant to be transient.  Once a 
        message is consumed, it is acknowldeged and removed from the queue.

      A 'stream' ingests events into an ordered log.  The log persists for as long as you wish, 
        allowing events to be queried over various ranges, aggregated, and combined with other streams
        to create new transformations published to downstream consumers.


        Producer1  ->  Consumer1  \
                                      Combine Data  ->  Publish 1 + 2 To Producer3
        Producer2  ->  Consumer2  /


    - Batch processing usually involves static workflows (ingest data, store it, transform it, serve it).

      Messages and streams are fluid.  Ingestion can be nonlinear, with data being published, 
        consumed, replenished, and re-consumed.


    - When building real-time data pipelines, messages and events should flow with as little latency
        as possible.  Make sure to provide sufficient resources for this.  Managed services work well 
        for real-time ingestion.



- Managed Data Connectors

    - If you find yourself writing a connector to an API, you should first look into whether one has
        already been created.  

    - Vendors and OSS projects typically have lots of prebuilt connector options.



- Moving Data with Object Storage

    - Object storage is a multitenant system in public clouds, and it supports storing massive amounts
        of data.  This makes it ideal for moving data in and out of data lakes, between teams, and
        between organizations.


    - You can provide short-term access to an object with a signed URL, giving a user temporary
        permission.  This makes object a very secure and optimal way to handle file exchange.



- EDI (Electronic Data Interchange)

    - DE's will also likely encounter 'EDI'.  This usually refers to a somewhat archaic means of
        file exchange, such as by email or flash drive.  Some sources do not support more modern means
        of transport due to old systems or human process limitations.

    - DE's can try to make these systems as automated as possible.  For instance, they can set up a
        cloud-based email server that saves files to storage as soon as they are received.  This can
        trigger orchestration processes to ingest and process data.



- Databases and File Export

    - Export from some database systems involves full table scans, which will cause performance issues.
        Sometimes, export queries can be broken into smaller exports by querying over key ranges or
        one partition at a time.


    - Major cloud data warehouses are highly optimized for direct file export. For example, Snowflake, 
        BigQuery, Redshift, and others support direct export to object storage in various formats.



- Practical Issues with Common Formats

    - CSV is ubiquitious, but highly error-prone:

        - Data with commas can cause problems
        - DE's must specify delimiter, quote characters, and escaping string data
        - No schema support
        - No nested structures

      In production system, should specify these things in file metadata.  Automatic tools provided by
        cloud environments are likely to crash.


    - More robust and expressive formats include Parquet, Avro, Arrow, ORC, and JSON.  These formats
        natively encode schema information and handle arbitrary strings without intervention.


    - For columnar DBs, columnar formats (Parquet, Arrow, ORC) allow more efficient data export, 
        because columns can be directly transcoded between formats.


    - These formats are generally more optimized for query engines.  For instance, the Arrow format
        is designed to map data directly into processing engine memory, providing high performance in
        data lake environments.


    - The problem with these new formats is that they are not natively supported by source systems.
        So, DE's usually end up with CSVs and lots of exception handling.



- Shell

    - The 'shell' is an interface you use to execute commands to ingest data.  The shell can be used
        to script workflows for virtually any software tool, and shell scripts are used extensively
        in ingestion processes.


    - This approach might be fine if you have a single instance and a small amount of data.


    - In addition, cloud vendors usually have robust CLI tools.  You may be able to run your ingestion
        processes using AWS CLI, for instance.



- SSH

    - SSH is a protocol that can be used to add security to our ingestion processes.  SSH can be used
        for file transfer with SCP.  It can also be used to allow secure, isolated connections to
        DBs.


    - Application DBs should never be directly exposed to the internet.  Instead, engineers can set up
        an intermediate host to connect to the DB in question.  This 'bastion host' (aka 'intermediate 
        host') is exposed to the internet, but it is locked down to only allow access from specified 
        IP addresses to specified ports.

      To connect to the DB, a remote machine opens an SSH tunnel to the bastion host, and then connects
        from the host machine to the DB.



- SFTP and SCP

    - Sending data from SFTP and SCP are techniques DE's should be familiar with, even though they are
        rarely used.  It may be the only way to share data with a partner business that has outdated 
        systems.


    - SFTP makes engineers cringe at the security implications, and if someone is using regular FTP
        you should reject it.  Security analysis is critical in these situations.


    - SCP is a file-exchange protocol that runs over SSH.  It can be a secure file-transfer option if
        configured correctly.



- Webhooks

    - 'Webhooks' are often referred to as 'reverse APIs'.  The data provider makes API calls rather
        than receiving them.  It is the data consumer's responsibility to provide an API endpoint
        for the provider to call.


        Webhook Data   ->  Serverless  ->  Event-streaming  ->  Stream     ->  Object
        Source             Function        Platform             Processor      Storage


    - This approach can be brittle and difficult to maintain.  Off-the-shelf tools in platforms like
        AWS are more reliable.



- Web Interface

    - Web interfaces for data access remain a practical reality for DE's.  There might not be an API
        or file drop.  Someone has to manually access a web interface, generate a report, and 
        download the file to a local machine.


    - This has obvious drawbacks, like people forgetting to run the report or having their laptop die.
        Where possible, choose tools that allow for automation.



- Web Scraping

    - 'Web scraping' automatically extracts data from web pages by looking through the page's HTML
        elements.  This practice is widespread, but it is a murky area where legal and ethical lines
        are blurry.


    - If data is available some other way, you should do it.  If you do web scrape, be a good citizen
        and don't DoS the website, which could get your IP address blocked, your AWS account disabled,
        or lead to legal consequences.


    - Also, web pages constantly change their HTML structure, so you have to constantly update the
        scraper.



- Transfer Appliances for Data Migration

    - For massive amounts of data (> 100 TB), transferring data over the internet is slow and costly.
        At this scale, the fastest and most efficient way to move data is by truck.


    - Cloud vendors offer the ability to send your data via a box of hard drives.  Simply order a
        storage device called a 'transfer appliance', load your data from your servers, and send it
        to the cloud vendor, who will upload your data.


    - The suggestion is to consider this if your data is > 100 TB.  AWS even has 'Snowmobile', a
        transfer appliance sent to you in a semitrailer.


    - Transfer appliances are handy for creating hybrid-cloud or multicloud setups.  For instance,
        AWS Snowball supports input and output, and can be used to export data to GCP or Azure.


    - These approaches are for one-time data loading and should not be used on an ongoing basis.



- Data Sharing

    - 'Data sharing' is growing as a popular option for consuming data.  Data providers will provide
        datasets to third-party suppliers, whether free or at cost.


    - These datasets are read-only so that they can integrate with your own data, but you don't own
        the shared dataset and can't update it.


    - Some cloud platform even offer data marketplaces where companies and organizations can put their
        data up for sale.



- Stakeholders

    - Think of a DE pipeline as a business with suppliers and customers.  You may be able to offer
        simple ingestion processes that save departments like marketing a lot of time and money.


    - Invite more executive participation in the processes, and communicate the value of source
        data.  Highlight the contributions of software engineers that provide you with source
        data.


    - Honest communication with stakeholders, especially early in the process, is what adds value.



- Security

    - Moving data introduces extra security vulnerabilities.  Consider where data lives and where it
        is going.  


    - Data that needs to move within your VPC should use secure endpoints and never leave the confines
        of the VPC.


    - Use a VPN or dedicated private connection to send data between the cloud and an on-premises
        network.


    - If your data traverses the public internet, ensure that the transmission is encrypted.  It is
        always a good practice to encrypte data over the wire.



- Data Ethics, Privacy, and Compliance

    - Clients often ask if we should encrypt sensitive data in databases, which begs the question of
        whether we need that data in the first place.  We should always ask this question before
        ingesting sensitive data.

      We may be able to just drop sensitive fields from the ingestion.  Data cannot leak if it is
        never collected.


    - If we truly need to keep track of sensitive identities, it is common practice to apply
        tokenization to anonymize identites in model training and analytics.  If possible, we should
        hash at ingestion time.


    - In some cases, DE's cannot avoid working with sensitive data.  Some analytics systems must
        provide identifiable, sensitive information.  Aim for 'touchless production' in these cases,
        where engineers develop and test code on cleansed data in DEV/STAGE.  Automatate code
        deployments to PROD.


    - We may still have to look at sensitive PROD data to debug problems.  In these cases, require
        approval from at least 2 people to access that sensitive data.


    - Most cloud-based storage systems and nearly all databases encrypt data at rest and in motion
        by default.


    - When hashing, make sure to salt your data.  If someone has the email of a customer, we don't
        want them to easily hash it to identify the customer in our data.



- DataOps

    - Monitoring your data pipeline is crucial.  This is especially true for ingestion jobs.  If
        they fail, all downstream systems will be affected.


    - Valid data is the foundation to success in today's business.  Bad data is worse than no data,
        and can untold damage.  These disasters are sometimes called 'datastrophes'.


    - Data quality tests are harder to design than smoke tests for software (ie look at request
        failure rate or response latency for signs of problems.)

      With data ingestion, we might have to run more complicated statistical analysis to look for
        problems.


    - Managed data connectors, such as Fivetran, Matillion, and Airbyte have simplified the 
        ingestion process.  