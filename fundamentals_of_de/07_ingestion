-----------------------------------------------------------------------------
| CHAPTER 7 - INGESTION                                                     |
-----------------------------------------------------------------------------

- Data Ingestion

    - 'Data ingestion' is the process of moving from one place to another.  If DE pipelines, it
        implies data is being moved from source systems into storage to support the DE lifecycle.


    - A 'data pipeline' is the combination of architecture, systems, and processes that move data
        through the stages of the DE lifecycle.



- Key Engineering Considerations

    - Questions to ask about ingestion

        - What’s the use case for the data I’m ingesting?

        - Can I reuse this data and avoid ingesting multiple versions of the same dataset?

        - Where is the data going? What’s the destination?

        - How often should the data be updated from the source?

        - What is the expected data volume?

        - What format is the data in? Can downstream storage and transformation accept this format?

        - Is the source data in good shape for immediate downstream use?  That is, is the data of good
            quality? What post-processing is required to serve it?  What are data-quality risks 
            (e.g., could bot traffic to a website contaminate the data)?

        - Does the data require in-flight processing for downstream ingestion if the data is from a 
            streaming source?        


    - Architectural considerations

        - Bounded versus unbounded

        - Frequency

        - Synchronous versus asynchronous

        - Serialization and deserialization

        - Throughput and elastic scalability

        - Reliability and durability

        - Payload

        - Push versus pull versus poll 



- Bounded vs Unbounded Data

    - 'Unbouded data' is data as it exists in reality, events happen either sporadically or
        continuously.  'Bounded data' is a convenient way of bucketing data across some boundary, such
        as time.


    - All data is unbounded until it is bounded.  Business processes have long imposed artificial
        bounds on data by cutting discrete batches.


    - Streaming systems are a tool for preserving the unbouded nature of data, so that subsequent
        steps can process it continuously.



- Frequency

    - The data-ingestion frequency is a critical decision when designing pipelines.  Ingestion processes
        can be batch, micro-batch, or real-time.


    - On the slow end, a business might ship it's tax data to an accounting firm once a year.  On the
        faster side, a CDC system could retrive new log updates from a source database once a minute.
        An IoT sensor might ingest and process events in seconds.


    - ML models are still typically trained on a batch basis, although continuous online training is
        becoming more common.



- Synchronous vs Asynchronous Ingestion

    - With 'synchronous ingestion', the source, ingestion, and destination have complex dependencies 
        and are tightly coupled.  This is common in older ETL systems.

        Process A -> Process B -> Process C   (Sequentially, if one fails we stop)


    - For one example of how this can go wrong, one company had a pipeline with dozens of tightly
        coupled workflows that took > 24 hours to complete.  If any one stage failed, the entire
        pipeline had to be restarted from the beginning.  Any time there was a problem, reports would
        be delayed for days.


    - With 'asynchronous ingestion', dependencies can operate at the level of individual events,
        similarly to the microservices approach.  Individual events become available in storage as
        soon as they are ingested individually.


    - Asynchronous ingestion example

        App  ->  Kinesis Data Stream  ->  Beam  ->  Kinesis Data Stream  ->  Kinesis Firehose  ->  S3


        1. A web app emits events into Kinesis data stream (which acts as a buffer here)

        2. The stream is read by Beam, which parses and enriches events

        3. Beam forwards the event to a second Kinesis data stream

        4. Kinesis Data Firehose rolls up events and writes objects to S3



- Serialization and Deserialization

    - When ingesting data, we need to ensure the destination can deserialize data from the source.



- Throughput and Scalability

    - In theory, ingestion should never be a bottleneck.  In practice, bottlenecks are pretty standard.
        Design system to grow and shrink according to desired throughput.  This is most easily done
        using managed tools to handle the scaling.


    - If a source database goes down, and then comes back up and attempts to backfill the lapsed
        data loads, will your ingestion be able to keep up with the backlog?


    - Data generation rarely happens at a constant rate, and often ebbs and flows.  We need to be able
        to handle the bursts.



- Reliability and Durability

    - We need to build an appropriate level of redundancy and self-healing based on the cost of losing
        data.



- Payload

    - A 'payload' is the dataset you're ingesting and has characteristics such as kind, shape, size,
        schema, data types, and metadata.

        Kind = type (ie tabular, image, video, text) and format (ie csv, parquet, jpeg)

        Shape = dimensions (ie number of rows/columns, RGB values for pixels, # of channels in audio)

        Size = number of bytes in a payload (bytes to TB, may be compressed if large)

        Schema and Data Types = ie relational DB schema, API contract

        Metadata = data about data


    - Making changes to schemas requires good communication with consumers.  'Schema registries' are
        metadata repositories use to maintain schemas and integrity constraints.



- Push vs Pull vs Poll Patterns

    - 


- Batch Ingestion Considerations


- Snapshot or Differential Extraction


- File-Based Export and Ingestion


- ETL vs ELT


- Insert, Updates, and Batch Size


- Data Migration


- Message and Stream Ingestion Considerations


- Ways to Ingest


- Direct Database Connection


- Change Data Capture


- APIs


- Message Queues and Event-Streaming Platforms


- Managed Data Connectors


- Moving Data with Object Storage


- EDI (Electronic Data Interchange)


- Databases and File Export


- Practical Issues with Common Formats


- Shell


- SSH


- SFTP and SCP


- Webhooks


- Web Interface


- Web Scraping


- Transfer Appliances for Data Migration


- Data Sharing