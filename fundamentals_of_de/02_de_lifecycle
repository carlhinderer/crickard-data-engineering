-----------------------------------------------------------------------------
| CHAPTER 2 - DATA ENGINEERING LIFECYCLE                                    |
-----------------------------------------------------------------------------

- 5 Stages of DE Lifecycle

    1. Generation
    2. Storage
    3. Ingestion
    4. Transformation
    5. Serving Data



- Source System Considerations

    - What are the essential characteristics of the data source? Is it an application? A swarm of IoT 
        devices?

    - How is data persisted in the source system? Is data persisted long term, or is it temporary and 
        quickly deleted?

    - At what rate is data generated? How many events per second? How many gigabytes per hour?

    - What level of consistency can data engineers expect from the output data? If you’re running 
        data-quality checks against the output data, how often do data inconsistencies occur—nulls 
        where they aren’t expected, lousy formatting, etc.?

    - How often do errors occur?

    - Will the data contain duplicates?

    - Will some data values arrive late, possibly much later than other messages produced
        simultaneously?

    - What is the schema of the ingested data? Will data engineers need to join across several tables 
        or even several systems to get a complete picture of the data?

    - If schema changes (say, a new column is added), how is this dealt with and communicated to 
        downstream stakeholders?

    - How frequently should data be pulled from the source system? For stateful systems (e.g., a 
        database tracking customer account information), is data provided as periodic snapshots or 
        update events from change data capture (CDC)? What’s the logic for how changes are performed, 
        and how are these tracked in the source database?

    - Who/what is the data provider that will transmit the data for downstream consumption?

    - Will reading from a data source impact its performance?

    - Does the source system have upstream data dependencies? What are the characteristics of these 
        upstream systems?

    - Are data-quality checks in place to check for late or missing data?



- Storage Considerations

    - Is this storage solution compatible with the architecture’s required write and read speeds?

    - Will storage create a bottleneck for downstream processes?

    - Do you understand how this storage technology works? Are you utilizing the storage system 
        optimally or committing unnatural acts?  For instance, are you applying a high rate of random 
        access updates in an object storage system? (This is an antipattern with significant
        performance overhead.)

    - Will this storage system handle anticipated future scale? You should consider all capacity limits 
        on the storage system: total available storage, read operation rate, write volume, etc.

    - Will downstream users and processes be able to retrieve data in the required service-level 
        agreement (SLA)?

    - Are you capturing metadata about schema evolution, data flows, data lineage, and so forth? 
        Metadata has a significant impact on the utility of data. Metadata represents an investment in 
        the future, dramatically enhancing discoverability and institutional knowledge to streamline
        future projects and architecture changes.

    - Is this a pure storage solution (object storage), or does it support complex query patterns 
        (i.e., a cloud data warehouse)?

    - Is the storage system schema-agnostic (object storage)? Flexible schema (Cassandra)? Enforced 
        schema (a cloud data warehouse)?

    - How are you tracking master data, golden records data quality, and data lineage for data 
        governance? (We have more to say on these in “Data Management”.)

    - How are you handling regulatory compliance and data sovereignty? For example, can you store your 
        data in certain geographical locations but not others?



- Data Access Frequency

    - Retrieval patterns will vary greatly depending on the data being stored and queried.  We say that
        data access frequency determines the 'temperature' of the data.

        1. Hot data = retrieved many times per day, stored for quick access

        2. Lukewarm data = accessed once a week or month

        3. Cold data = seldom accessed, appropriate for storing in an archival system (cloud these days)



- Ingestion Considerations

    - What are the use cases for the data I’m ingesting? Can I reuse this data rather than create 
        multiple versions of the same dataset?

    - Are the systems generating and ingesting this data reliably, and is the data available when I 
        need it?

    - What is the data destination after ingestion?

    - How frequently will I need to access the data?

    - In what volume will the data typically arrive?

    - What format is the data in? Can my downstream storage and transformation systems handle this 
        format?

    - Is the source data in good shape for immediate downstream use? If so, for how long, and what may 
        cause it to be unusable?

    - If the data is from a streaming source, does it need to be transformed before reaching its 
        destination? Would an in-flight transformation be appropriate, where the data is transformed 
        within the stream itself?



- Batch vs Streaming

    - All data we deal with is inherently 'streaming'.  It is always produced and updated continuously
        at its source.  'Batch ingestion' is a convenient way of processing this stream in large
        chunks.


    - Streaming ingestion allows us to provide data to downstream systems in a continuous, real-time
        fashion.  The actual latency requirements vary based on domain and requirements.



- Batch vs Stream Ingestion Considerations

    - If I ingest the data in real time, can downstream storage systems handle the rate of data flow?

    - Do I need millisecond real-time data ingestion? Or would a micro-batch approach work, 
        accumulating and ingesting data, say, every minute?

    - What are my use cases for streaming ingestion? What specific benefits do I realize by implementing
        streaming? If I get data in real time, what actions can I take on that data that would be an 
        improvement upon batch?

    - Will my streaming-first approach cost more in terms of time, money, maintenance, downtime, and 
        opportunity cost than simply doing batch?

    - Are my streaming pipeline and system reliable and redundant if infrastructure fails?

    - What tools are most appropriate for the use case? Should I use a managed service (Amazon Kinesis, 
        Google Cloud Pub/Sub, Google Cloud Dataflow) or stand up my own instances of Kafka, Flink, Spark, 
        Pulsar, etc.? If I do the latter, who will manage it? What are the costs and trade-offs?

    - If I’m deploying an ML model, what benefits do I have with online predictions and possibly 
        continuous training?

    - Am I getting data from a live production instance? If so, what’s the impact of my ingestion process 
        on this source system?



- Transformation

    - Transformation is where we start to provide value to downstream data consumers.


    - Batch streaming is most popular, but streaming transformations are increasingly used.


    - Business logic is a major driver of transformations.  Data preparation, wrangling, cleaning, and
        enrichment add value for end consumers.


    - Data featurization for ML is another common transformation process.



- Transformation Considerations

    - What’s the cost and return on investment (ROI) of the transformation?

    - What is the associated business value?

    - Is the transformation as simple and self-isolated as possible?

    - What business rules do the transformations support?

    - Am I minimizing data movement between the transformation and the storage system during 
        transformation?



- Serving Data

    - Data only has value if it is consumed.  Data vanity projects can be a big risk for organizations.
        This was a hallmark of the big data era.


    - Analytics Use Case

        - Once data is stored and transformed, it can be used to generate reports or dashboards, or 
            used for ad hoc analysis.

        - 'Business Intelligence' uses collected data to describe a business's past and current state.
            Data is usually stored in a clean but somewhat raw form, allowing for ad hoc queries.

        - 'Operational Analytics' focuses on the fine-grained details of automation.  For instance,
            it could be a live view of inventory or real-time dashboard of website health.

        - 'Embedded Analytics' typically describes customer-facing analytics.


    - Machine Learning Use Case

        - Data needs to be of sufficient quality to support reliable feature engineering.  The
            requirements should be determined collaboratively.

        - Data needs to be discoverable so that data scientists can find it.

        - The organizational split between DS and DE has significant architectural implications.

        - The data set needs to properly resent the ground truth.  It should not be biased.


    - Reverse ETL Use Case

        - Reverse ETL is the process of taking the output of the DE lifecycle and feeding it back into
            the source system.

        - This is known as a bit of an antipattern, but sometimes it is necessary.



- Security

    - DE's must understand data and access security, and should use the principle of least privilege.


    - Data should be protected both in flight and at rest, by using encrpytion, tokenization, data
        masking, obfuscation, and access controls.