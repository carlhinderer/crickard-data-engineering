-----------------------------------------------------------------------------
| CHAPTER 3 - DESIGNING GOOD DATA ARCHITECTURE                              |
-----------------------------------------------------------------------------

- Enterprise Architecture, Defined

    - 'Enterprise architecture' is the design of systems to support change in the enterprise, achieved
        by flexible and reversible decisions reached through careful evaluation of trade-offs.


    - Creating 'two-way doors' that allow decisions to be reversible allow organizations to iterate
        and improve rapidly according to observed data.


    - Technical solutions only exist in support of business goals.



- Data Architecture, Defined

    - 'Data architecture' is the design of systems to support the evolving data needs of an enterprise, 
        achieved by flexible and reversible decisions reached through a careful evaluation of 
        trade-offs.


    - 'Operational architecture' encompasses the functional requirements of what needs to happen 
        related to people, processes, and technology.

      For example, what business processes does the data serve? How does the organization manage data
        quality? What is the latency requirement from when the data is produced to when it becomes 
        available to query?


    - 'Technical architecture' outlines how data is ingested, stored, transformed, and served along the 
        data engineering lifecycle. 

      For instance, how will you move 10 TB of data every hour from a source database to your data lake?



- Good Data Architecture

    - Good data architecture serves business requirements with a common, widely reusable set of building
        blocks while maintaining flexibility and making appropriate trade-offs. 


    - Bad architecture is authoritarian and tries to cram a bunch of one-size-fits-all decisions into a    
        big ball of mud.



- Principles of Good Data Architecture

    - AWS's principles for a 'Well-Architected Framework'

        1. Operational excellence
        2. Security
        3. Reliability
        4. Performance efficiency
        5. Cost optimization
        6. Sustainability


    - Google Cloud's 'Principles for Cloud-Native Architecture'

        1. Design for automation.
        2. Be smart with state.
        3. Favor managed services.
        4. Practice defense in depth.
        5. Always be architecting.


    - Principles for Good Data Architecture

        1. Choose common components wisely.
        2. Plan for failure.
        3. Architect for scalability.
        4. Architecture is leadership.
        5. Always be architecting.
        6. Build loosely coupled systems.
        7. Make reversible decisions.
        8. Prioritize security.
        9. Embrace FinOps.



- Principle #1 - Choose Common Components Wisely

    - Common components are anything with broad applicability across an organization:

        - Object storage
        - Version control
        - Observability
        - Monitoring and orchestration
        - Processing engines


    - Cloud platforms are the ideal place to adopt common components.


    - Architects need to be careful not to hamper the productivity of people working on domain-specific
        problems by forcing them into one-size-fits-all solutions.



- Principle #2 - Plan for Failure

    - Key terms for evaluating failure scenarios:

        1. Availability = % of time service or component is in operable state

        2. Reliability = probability of system meeting define performance standards over an interval

        3. Recovery Time Objective = max acceptable time for a system outage

        4. Recovery Point Objective = acceptable state after recovery



- Principle #3 - Architect for Scalability

    - We should be able to scale systems up and down elastically to handle different quantities of data.


    - We can even 'scale to zero', deleting the entire cluster when we are done with our task.
        Many serverless systems do this automatically.



- Principle #4 - Architecture is Leadership

    - Architects should be highly technically competent but delegate most individual contributor work
        to others.


    - Mentoring the development team to raise their level, so that they can take on more complex issues,
        is the most crucial job of an architect.



- Principle #5 - Always Be Architecting

    - Data architects don't just maintain the current state.  They constantly design new things in
        response to changes in business and technology.


    - We need deep knowledge of the current system (the 'baseline') and the 'target architecture', so
        that we can develop a 'sequencing plan' to get there.



- Principle #6 - Build Loosely Coupled Systems

    - Teams must be able to test, deploy, and change systems independently.


    - Loosely coupled software architecture:

        1. Systems are broken up into many smaller components.

        2. Systems interface with each other through abstraction layers (ie message buses or APIs).

        3. Internal changes of one component don't require changes to other components.

        4. There is no waterfall, global release.  Components are released separately.



- Principle #7 - Make Reversible Decisions

    - Given the pace of change in data systems, we should expect to be switching components regularly.
        Making reversible decisions will simplify your architecture and keep it agile.



- Principle #8 - Prioritize Security

    - 'Hardened-Perimiter' and 'Zero-Trust' Security Models

        - Traditional architectures place a lot of faith in perimeter security, a hardened network
            perimeter with 'trusted things' inside and 'untrusted things' outside.

        - There are several problems with this model.  Humans can be exploited, causing breaches.
            Also, the secure computer networks remain connected to the ouside world via email, phones,
            etc.  Insider attacks and spear fishing have always been a problem.

        - In a cloud-native environment, all assets are connected to the outside world to some degree.


    - The Shared Responsibility Model

        - Amazon emphasizes the 'shared responsibility model', which divides security into 2 parts:
            'security of the cloud' and 'security in the cloud'.

        - AWS is responsible for securing AWS infrastructure.  They provide services you can use
            securely.

        - Users are responsible for other factors like data sensitivity, the organization's requirements,
            and applicable laws and regulations.


    - All data engineers should think of themselves as security engineers.  Failure to assume these
        implicit responsibilities 



- Principle #9 - Embrace FinOps

    - 'FinOps' is a cloud management discipline that enables organizations to get maximum business
        value by having all parts of the organization collaborate on data-driven spending decisions.


    - In the cloud era, most data systems are pay-as-you-go, which can be much more cost effective
        than traditional capital expenditures.  However, this has to be carefully managed and
        monitored.



- Domains and Services

    - A 'domain' is a real-world subject area for which you're architecting.  A 'service' is a set of
        functionality whose goal is to accomplish a task.


    - A domain can contain multiple services.  For instance a sales system might have order, invoicing,
        and product services.



- Distributed Systems, Scalability, and Designing for Failure

    - 4 Characteristics of Data Systems

        1. Scalability
        2. Elasticity
        3. Availability
        4. Reliability



- Tiers, Monoliths, and Microservices

    - In a 'single-tier architecture', the application and DB are tightly coupled, residing on a single
        server.  This is useful for prototyping and development only.


    - In a 'multitier (aka n-tier) architecture' , the data and application are decoupled.  The layers
        are hierarchical, and upper layers are dependent on lower layers.


    - A 'monolith' consists of a single codebase containing both application logic and UI.


    - A 'microservices architecture' contains separate, decentralized, and loosely coupled services.



- User Access - Single vs MultiTenant

    - As a data engineer, you'll often have to consider how you want to share a data system.


    - The first factor to think about is performance.  If there are lots of tenants in a system, will
        there be a noisy neighbor problem?


    - The other thing to think about is security.  Does data from different domains need to be isolated?
        This depends on the domain.



- Event-Driven Architecture

    - Events are changes in state.  An event-driven workflow encompasses the ability to create, update,
        and move events across various points of the DE lifecycle.

        Producer  ->  Event Router  ->  Consumer


    - An 'EDA' uses an event-driven workflow to communicate across various services.


    - Any time we use loosely coupled services, this is a candidate for EDA.



- Brownfield vs Greenfield Projects

    - 'Brownfield projects' involve refactoring and reorganizing an existing architecture.  They are
        constrained by the choices of the present and past.  A thorough understanding of the legacy
        architecture is required.


    - Don't jump into a big-bang overhaul of a brownfield project without a careful plan.  Make
        reversible, high-ROI decisions.  Use the strangler pattern.


    - Some components will not be able to be changed, because they have too many dependencies.  Remember
        that 'legacy' is a condescending way to describe something that makes money.


    - 'Greenfield projects' allow a fresh start, unconstrained by prior architecture.  Make flexible
        and reversible decisions.  Avoid 'resume-driven development' that doesn't prioritize the
        project's goals.



- Data Warehouse

    - A 'data warehouse' is a central data hub used for reporting and analysis.  Data in a DW is 
        highly formatted and structured for analytics use cases.


    - Bill Inmon's original description in 1990 still holds: "a subject-oriented, integrated,
        nonvolatile, and time-variant collection of data in support of management’s decisions."


    - In the past, having a DW was expensive and labor-intensive, so only large companies had them.
        Pay-as-you-go cloud services have made them available to all companies.


    - A data warehouse has 2 main characteristics:

        1. It separates OLAP processes from OLTP databases.  This is critical for performance reasons.

        2. It centralizes and organizes data using ETL to pull from source systems.


        Data Sources  ->  ETL System  ->  Data Warehouse  ->  Data Mart


    - Traditionally, DW's are queried using the same SQL semantics used in relational DBs.  However,
        columnar architectures are increasingly used for performing queries over large amounts of
        data.


    - An 'MPP (Massive Parallel Processing) Database' splits procesing between different nodes and
        servers to scan massive amounts of data.



- Cloud Data Warehouses

    - AWS Redshift kicked off the cloud data warehouse revolution.  Instead of spending millions of
        dollars on an MPP system, customers could spin up a cluster and scale it up as demand grows.


    - Google BigQuery, Snowflake, and others popularized the idea of separating compute from storage.
        In this architecture, data is housed in object storage, allowing virtually limitless storage.


    - Cloud data warehouses now cover many use cases that required a Hadoop cluster in the very 
        recent past.  They can process PB-sized amounts of data in a single query.  The line between
        data warehouses and data lakes is starting to blur.



- Data Marts

    - A 'data mart' is a more refined subset of a data warehouse designed to serve analytics and
        reporting.  They are used for 2 reasons:

        1. It makes data more easily accessible for analysts and report developers.

        2. It provides a place to make additional transformations if queries require complex joins
             or aggregations.



- Data Lake

    - A 'data lake' is a place to simply dump all your data, structured and unstructured, into a
        central location.


    - The first version of data lakes started with HDFS.  Eventually, they started migrating towards
        cloud-based object storage.  When this data needs to be queried or transformed, you can
        spin up a cluster and pick your favorite technology (ie MapReduce, Spark, Ray, Presto, Hive).


    - This approach had serious shortcomings.  Many data lakes became data swamps as data grew to
        unmanageable size with little schema management, data cataloging, or discovery tools.


    - With things like GDPR, data being thrown around randomly without a catalog became a huge headache
        when user records needed to be deleted.


    - Large organizations that could affort to hire top talent found significant value in data lakes,
        but it wasn't cost-effective for other companies.



- Convergence, Next-Generation Data Lakes, and the Data Platform

    - Various players have sought to enhance the data lake concept.  For example, DataBricks has a
        'data lakehouse', which incorporates controls and data management while housing data in object
        storage and supporting a wide variety of query and transformation engines.  The data lakehouse
        also support ACID transactions.


    - Cloud data warehouses now use an approach very similar to data lakes.  They separate compute from
        storage, support PB-scale queries, store unstructured and semistructured data, and integrate
        with advanced processing technologies (ie Spark or Beam).


    - As this convergence continues, several vendors are offering a complete, integrated data platform.
        The clear leaders at this point are:

        - AWS
        - Google Cloud
        - Azure
        - Snowflake
        - DataBricks

