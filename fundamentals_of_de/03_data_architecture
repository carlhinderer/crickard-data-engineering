-----------------------------------------------------------------------------
| CHAPTER 3 - DESIGNING GOOD DATA ARCHITECTURE                              |
-----------------------------------------------------------------------------

- Enterprise Architecture, Defined

    - 'Enterprise architecture' is the design of systems to support change in the enterprise, achieved
        by flexible and reversible decisions reached through careful evaluation of trade-offs.


    - Creating 'two-way doors' that allow decisions to be reversible allow organizations to iterate
        and improve rapidly according to observed data.


    - Technical solutions only exist in support of business goals.



- Data Architecture, Defined

    - 'Data architecture' is the design of systems to support the evolving data needs of an enterprise, 
        achieved by flexible and reversible decisions reached through a careful evaluation of 
        trade-offs.


    - 'Operational architecture' encompasses the functional requirements of what needs to happen 
        related to people, processes, and technology.

      For example, what business processes does the data serve? How does the organization manage data
        quality? What is the latency requirement from when the data is produced to when it becomes 
        available to query?


    - 'Technical architecture' outlines how data is ingested, stored, transformed, and served along the 
        data engineering lifecycle. 

      For instance, how will you move 10 TB of data every hour from a source database to your data lake?



- Good Data Architecture

    - Good data architecture serves business requirements with a common, widely reusable set of building
        blocks while maintaining flexibility and making appropriate trade-offs. 


    - Bad architecture is authoritarian and tries to cram a bunch of one-size-fits-all decisions into a    
        big ball of mud.



- Principles of Good Data Architecture

    - AWS's principles for a 'Well-Architected Framework'

        1. Operational excellence
        2. Security
        3. Reliability
        4. Performance efficiency
        5. Cost optimization
        6. Sustainability


    - Google Cloud's 'Principles for Cloud-Native Architecture'

        1. Design for automation.
        2. Be smart with state.
        3. Favor managed services.
        4. Practice defense in depth.
        5. Always be architecting.


    - Principles for Good Data Architecture

        1. Choose common components wisely.
        2. Plan for failure.
        3. Architect for scalability.
        4. Architecture is leadership.
        5. Always be architecting.
        6. Build loosely coupled systems.
        7. Make reversible decisions.
        8. Prioritize security.
        9. Embrace FinOps.



- Principle #1 - Choose Common Components Wisely

    - Common components are anything with broad applicability across an organization:

        - Object storage
        - Version control
        - Observability
        - Monitoring and orchestration
        - Processing engines


    - Cloud platforms are the ideal place to adopt common components.


    - Architects need to be careful not to hamper the productivity of people working on domain-specific
        problems by forcing them into one-size-fits-all solutions.



- Principle #2 - Plan for Failure

    - Key terms for evaluating failure scenarios:

        1. Availability = % of time service or component is in operable state

        2. Reliability = probability of system meeting define performance standards over an interval

        3. Recovery Time Objective = max acceptable time for a system outage

        4. Recovery Point Objective = acceptable state after recovery



- Principle #3 - Architect for Scalability

    - We should be able to scale systems up and down elastically to handle different quantities of data.


    - We can even 'scale to zero', deleting the entire cluster when we are done with our task.
        Many serverless systems do this automatically.



- Principle #4 - Architecture is Leadership

    - Architects should be highly technically competent but delegate most individual contributor work
        to others.


    - Mentoring the development team to raise their level, so that they can take on more complex issues,
        is the most crucial job of an architect.



- Principle #5 - Always Be Architecting

    - Data architects don't just maintain the current state.  They constantly design new things in
        response to changes in business and technology.


    - We need deep knowledge of the current system (the 'baseline') and the 'target architecture', so
        that we can develop a 'sequencing plan' to get there.



- Principle #6 - Build Loosely Coupled Systems

    - Teams must be able to test, deploy, and change systems independently.


    - Loosely coupled software architecture:

        1. Systems are broken up into many smaller components.

        2. Systems interface with each other through abstraction layers (ie message buses or APIs).

        3. Internal changes of one component don't require changes to other components.

        4. There is no waterfall, global release.  Components are released separately.



- Principle #7 - Make Reversible Decisions

    - Given the pace of change in data systems, we should expect to be switching components regularly.
        Making reversible decisions will simplify your architecture and keep it agile.



- Principle #8 - Prioritize Security

    - 'Hardened-Perimiter' and 'Zero-Trust' Security Models

        - Traditional architectures place a lot of faith in perimeter security, a hardened network
            perimeter with 'trusted things' inside and 'untrusted things' outside.

        - There are several problems with this model.  Humans can be exploited, causing breaches.
            Also, the secure computer networks remain connected to the ouside world via email, phones,
            etc.  Insider attacks and spear fishing have always been a problem.

        - In a cloud-native environment, all assets are connected to the outside world to some degree.


    - The Shared Responsibility Model

        - Amazon emphasizes the 'shared responsibility model', which divides security into 2 parts:
            'security of the cloud' and 'security in the cloud'.

        - AWS is responsible for securing AWS infrastructure.  They provide services you can use
            securely.

        - Users are responsible for other factors like data sensitivity, the organization's requirements,
            and applicable laws and regulations.


    - All data engineers should think of themselves as security engineers.  Failure to assume these
        implicit responsibilities 



- Principle #9 - Embrace FinOps

    - 'FinOps' is a cloud management discipline that enables organizations to get maximum business
        value by having all parts of the organization collaborate on data-driven spending decisions.


    - In the cloud era, most data systems are pay-as-you-go, which can be much more cost effective
        than traditional capital expenditures.  However, this has to be carefully managed and
        monitored.



- Domains and Services

    - A 'domain' is a real-world subject area for which you're architecting.  A 'service' is a set of
        functionality whose goal is to accomplish a task.


    - A domain can contain multiple services.  For instance a sales system might have order, invoicing,
        and product services.



- Distributed Systems, Scalability, and Designing for Failure

    - 4 Characteristics of Data Systems

        1. Scalability
        2. Elasticity
        3. Availability
        4. Reliability



- Tiers, Monoliths, and Microservices

    - In a 'single-tier architecture', the application and DB are tightly coupled, residing on a single
        server.  This is useful for prototyping and development only.


    - In a 'multitier (aka n-tier) architecture' , the data and application are decoupled.  The layers
        are hierarchical, and upper layers are dependent on lower layers.


    - A 'monolith' consists of a single codebase containing both application logic and UI.


    - A 'microservices architecture' contains separate, decentralized, and loosely coupled services.



- User Access - Single vs MultiTenant

    - As a data engineer, you'll often have to consider how you want to share a data system.


    - The first factor to think about is performance.  If there are lots of tenants in a system, will
        there be a noisy neighbor problem?


    - The other thing to think about is security.  Does data from different domains need to be isolated?
        This depends on the domain.



- Event-Driven Architecture

    - Events are changes in state.  An event-driven workflow encompasses the ability to create, update,
        and move events across various points of the DE lifecycle.

        Producer  ->  Event Router  ->  Consumer


    - An 'EDA' uses an event-driven workflow to communicate across various services.


    - Any time we use loosely coupled services, this is a candidate for EDA.



- Brownfield vs Greenfield Projects

    - 'Brownfield projects' involve refactoring and reorganizing an existing architecture.  They are
        constrained by the choices of the present and past.  A thorough understanding of the legacy
        architecture is required.


    - Don't jump into a big-bang overhaul of a brownfield project without a careful plan.  Make
        reversible, high-ROI decisions.  Use the strangler pattern.


    - Some components will not be able to be changed, because they have too many dependencies.  Remember
        that 'legacy' is a condescending way to describe something that makes money.


    - 'Greenfield projects' allow a fresh start, unconstrained by prior architecture.  Make flexible
        and reversible decisions.  Avoid 'resume-driven development' that doesn't prioritize the
        project's goals.



- Data Warehouse

    - A 'data warehouse' is a central data hub used for reporting and analysis.  Data in a DW is 
        highly formatted and structured for analytics use cases.


    - Bill Inmon's original description in 1990 still holds: "a subject-oriented, integrated,
        nonvolatile, and time-variant collection of data in support of management’s decisions."


    - In the past, having a DW was expensive and labor-intensive, so only large companies had them.
        Pay-as-you-go cloud services have made them available to all companies.


    - A data warehouse has 2 main characteristics:

        1. It separates OLAP processes from OLTP databases.  This is critical for performance reasons.

        2. It centralizes and organizes data using ETL to pull from source systems.


        Data Sources  ->  ETL System  ->  Data Warehouse  ->  Data Mart


    - Traditionally, DW's are queried using the same SQL semantics used in relational DBs.  However,
        columnar architectures are increasingly used for performing queries over large amounts of
        data.


    - An 'MPP (Massive Parallel Processing) Database' splits procesing between different nodes and
        servers to scan massive amounts of data.



- Cloud Data Warehouses

    - AWS Redshift kicked off the cloud data warehouse revolution.  Instead of spending millions of
        dollars on an MPP system, customers could spin up a cluster and scale it up as demand grows.


    - Google BigQuery, Snowflake, and others popularized the idea of separating compute from storage.
        In this architecture, data is housed in object storage, allowing virtually limitless storage.


    - Cloud data warehouses now cover many use cases that required a Hadoop cluster in the very 
        recent past.  They can process PB-sized amounts of data in a single query.  The line between
        data warehouses and data lakes is starting to blur.



- Data Marts

    - A 'data mart' is a more refined subset of a data warehouse designed to serve analytics and
        reporting.  They are used for 2 reasons:

        1. It makes data more easily accessible for analysts and report developers.

        2. It provides a place to make additional transformations if queries require complex joins
             or aggregations.



- Data Lake

    - A 'data lake' is a place to simply dump all your data, structured and unstructured, into a
        central location.


    - The first version of data lakes started with HDFS.  Eventually, they started migrating towards
        cloud-based object storage.  When this data needs to be queried or transformed, you can
        spin up a cluster and pick your favorite technology (ie MapReduce, Spark, Ray, Presto, Hive).


    - This approach had serious shortcomings.  Many data lakes became data swamps as data grew to
        unmanageable size with little schema management, data cataloging, or discovery tools.


    - With things like GDPR, data being thrown around randomly without a catalog became a huge headache
        when user records needed to be deleted.


    - Large organizations that could affort to hire top talent found significant value in data lakes,
        but it wasn't cost-effective for other companies.



- Convergence, Next-Generation Data Lakes, and the Data Platform

    - Various players have sought to enhance the data lake concept.  For example, DataBricks has a
        'data lakehouse', which incorporates controls and data management while housing data in object
        storage and supporting a wide variety of query and transformation engines.  The data lakehouse
        also support ACID transactions.


    - Cloud data warehouses now use an approach very similar to data lakes.  They separate compute from
        storage, support PB-scale queries, store unstructured and semistructured data, and integrate
        with advanced processing technologies (ie Spark or Beam).


    - As this convergence continues, several vendors are offering a complete, integrated data platform.
        The clear leaders at this point are:

        - AWS
        - Google Cloud
        - Azure
        - Snowflake
        - DataBricks



- The Modern Data Stack

    - The modern data stack uses cloud-based, plug-and-play, easy-to-use, off-the-shelf components
        to create a modular and cost-effective data architecture.


        Data Sources  ->  Cloud-Based Data Connector  ->  Cloud DW  ->  BI and Visualization


    - Key outcomes of using a modern data stack are self-service, agile data management, and using
        open source tools.  These stacks have strong user bases and active communitities.



- Lambda Architecture

    - In the early-mid 2010s, the popularity of working with streaming data exploded with the 
        emergence of Kafka as a highly scalable message queue and frameworks like Storm and Samza
        for steaming/real-time analytics.

      Companies could perform new types of analytics and modeling on large amounts of data, user
        aggregation and ranking, and product recommendations.


    - DE's needed to reconcile batch and streaming data into a single architecture.  The lambda
        architecture was an early attempt to solve this problem.


    - In 'Lambda Architecture', you have systems operating independently of each other - batch,
        stream, and serving.

                                               [Serving]
         Source   ->  Stream Processing  ->  | Stream Query |  <->  Queries
         Systems  ->  Stream Processing  ->  | Batch Query  |


    - The source system is ideally immutable and append-only, sending data to 2 destinations for
        processing: stream and batch.

      In-stream processing intends to serve the data with the lowest possible latency in a “speed” 
        layer, usually a NoSQL database. 

      In the batch layer, data is processed and transformed in a system such as a data warehouse, 
        creating precomputed and aggregated views of the data. 

      The serving layer provides a combined view by aggregating query results from the two layers.


    - Managing multiple systems with different codebases is difficult, and this creates error-prone
        systems with code and data that are difficult to reconcile.  This approach has fallen out of
        favor for this reason.



- Kappa Architecture

    - Kappa Architecture was proposed by Jay Kreps as a solution to the shortcomings of Lambda
        Architecture.  The idea is to just use a stream-processing platform as the backbone for all
        data handling.  This facilitates a true EDA.


        Stream Source  ->  Stream Processing  ->  Serving Layer  ->  Queries


    - This was suggested in 2014, and still hasn't been widely adopted.  Lack of experience with
        streaming and the expense of streaming systems that can handle huge data volumes are the reason.
        Batch storage and processing remain much cheaper for large datasets.



- The Dataflow Model and Unified Batch and Streaming

    - Google has developed the 'Dataflow Model' and the Apache Beam framework to solve the batch/stream
        mismatch problem.


    - The core idea in the Dataflow model is to view all data as events, as the aggregation is performed 
        over various types of windows.


    - Ongoing real-time event streams are 'unbounded data'.  Data batches are simply bounded event
        streams, and the boundaries provide a natural window.


    - Engineers can choose from various windows for real-time aggregation, such as sliding or
        tumbling.  Real-time and batch processing happen in the same system using nearly identical code.


    - The philosophy of 'batch as a special case of streaming' is now more pervasive.  Frameworks like
        Flink and Spark have adopted a similar approach.



- Architecture for IoT

    - IoT data is generated from devices that collect data periodically or continuously from the
        surrounding environment and transmit it to a destination. IoT devices are often low-powered and
        operate in low-resource/low bandwidth environments.


    - A 'device' (aka a 'thing') is some physical hardware connected to the internet.  Devices include:

        - doorbell camera, smartwatch, or thermostat
        - an AI-powered camera that monitors an assembly line for defective components
        - a GPS tracker to record vehicle locations
        - a Raspberry Pi programmed to download the latest tweets
        - any device capable of collecting data from it's environment


    - Devices may also crunch data before sending it ('edge computing') or even run ML ('edge ML').


    - An 'IoT gateway' is a hub for connecting devices and securely routing devices to the appropriate
        destination on the internet.  The gateway is used (rather than just normal networking) to
        allow devices to connect using extremely little power.


        IoT swarm

        +  +  +  +      ->  Gateway   ->
        +  +  +         ->  Gateway   ->     Message Queue
        +  +  +  +  +   ->  Gateway   ->


    - Ingestion is dependent on late-arriving data, connection issues, data corruption, and schema
        disparitites.  Architectures and downstream analytics must account for this.


    - For remote sensors collecting scientific analysis for later processing, batch storage may be fine.
        For data that needs to analyzed in real-time, a message queue is more appropriate.



- Data Mesh

    - The 'data mesh' is a recent response to sprawling monolithic data platforms, such as centralized
        data lakes and data warehouses, and the 'great divide of data' between operational and
        analytical data.


    - The data mesh approach is to invert these challenges by using domain-driven design principles
        in data architecture.  The goal is to decentralize the modern data platform.


    - The 4 key components of the data mesh:

        1. Domain-oriented decentralized data ownership and architecture

        2. Data as a product

        3. Self-serve data infrastructure as a platform

        4. Federated computational governance