-----------------------------------------------------------------------------
| CHAPTER 8 - QUERIES, MODELING, & TRANSFORMATION                           |
-----------------------------------------------------------------------------

- Queries

    - A 'query' allows you to retrieve and act on data.  It gives you CRUD capabilities.


    - To create the database objects, we use DDL.

        CREATE DATABASE bar;
        CREATE TABLE bar_table;
        DROP TABLE bar_table;


    - To add and alter data within these database objects, we use DML.

        SELECT
        INSERT
        UPDATE
        DELETE
        COPY
        MERGE


    - To limit access to database objects, we use DCL (Data Control Language).

        GRANT
        DENY
        REVOKE


    - To control the details of transactions, we use TCL (Transaction Control Language).

        COMMIT
        ROLLBACK



- The Life of a Query

    - When we execute a SQL query, this is a summary of what happens:


        SQL Query  ->  Parsing and    ->  Query Planning    ->  Query      ->  Results
        Issued         Conversion to      and Optimization      Execution      Returned
                       Byte code


        1. The DB engine compiles the SQL, parses the code to check for proper semantics, and ensures
             the DB objects referenced exist and the current user has access to them.

        2. The SQL code is converted into byte code.  This bytecode expresses the steps that must be 
             executed on the database engine in an efficient, machine-readable format.

        3. The databaseâ€™s query optimizer analyzes the bytecode to determine how to execute the query,
             reordering and refactoring steps to use available resources as efficiently as possible.

        4. The query is executed, and results are produced.


    - A query optimizer's job is to optimize query performance by performing steps in an efficient
        order.  It will assess joins, indexes, data scan size, and other factors.



- Improving Query Performance - Optimizing JOIN Strategy and Schema

    - A single table is rarely useful on it's own.  We create value by combining it with other datasets.
        Joins are the most common way of combining datasets and creating new ones.


    - A common technique for improving query performance is to pre-join data.  If analytics queries 
        are joining the same data repeatedly, it makes sense to join the data in advance and have
        queries read from the pre-joined data.

      This may mean relaxing normalization to widen the tables.  We could also create materialized
        views with the joined data and have users query the views.


    - We should also consider the complexity of our join conditions.  We can improve complex joins
        in a few ways.

        1. Many row-oriented DBs allow you to index a result computed from a row.  For instance,
             Postgres allows you to create an index on a string field converted to lowercase.
             When the query optimizer sees a 'lower()' in the query, it will use the indes.

        2. We can use CTEs (Common Table Expressions) instead of nested subqueries or temporary
             tables.  This increases readability, and also can improve performance by not having
             to create intermediate tables.


    - Row Explosion

        - One obscure but frustrating problem is 'row explosion'.  This occurs when we have a large 
            number of many-to-many matches, either because of repetition in join keys or as a 
            consequence of join logic.

        - Suppose the join key in TableA has the value 'this' repreated 5 times, and the join key in
            TableB contains this same value repeated 10 times.  This leads to a cross-join of each of
            these rows, creating 50 rows in the output.

        - Row explosions can often generate enough rows to consume massive resources or cause the
            query to fail.

        - We should check to see if the query optimizer is able to reorder joins, and if it isn't,
            we should reorder them in our query to alleviate this problem



- Improving Query Performance - Use the Explain Plan

    - The query optimizer's explain plan will show you how it determined the lowest-cost query, the
        DB objects used (tables, indexes, cache, etc.), and the performance statistics at each stage.


    - In addition, we should monitor the performance of our queries.  Things to watch include:

        1. Usage of disk, memory, network
        2. Data loading time vs processing time
        3. Query execution time, number of records, size of data scanned, size of data shuffled
        4. Competing queries that might cause resource contention
        5. Number of concurrent connections and connections available



- Improving Query Performance - Avoid Full Table Scans

    - You should only query the data you need.  Avoid 'SELECT *' queries with no predicates.


    - Whenever possible, use 'pruning' to reduce the quantity of data scanned in a query.  This 
        different strategies in row- and column-oriented DBs.


    - In a column-oriented DB, only select the columns you need.  Many column-oriented OLAP
        databases provide tools for optimiation, for example Snowflake and BigQuery let you
        define a cluster key, which orders the table in a desired way.


    - In a row-oriented DB, pruning centers around table indexes.  Create indexes that will improve
        the performance of your most important queries, but don't create so many that you degrade
        performance.



- Improve Query Performance - Know How Your Database Handles Commits

    - A database 'commit' is a change within a database, such as creating, updating, or deleting
        a record, table, or other database objects.  Many DBs support 'transactions', which combine
        several operations in a way that maintains a consistent state.


    - The purpose of a transaction is to keep the database in a consistent state both when it's active
        and in the case of a failure.  Also, transactions handle isolation when concurrent events
        are happening to the same DB objects.


    - Example - Postgres

        - Relational DB that uses ACID transactions
        - Each transaction is a package of operations that will succeed or fail as a group
        - Analytics queries will always give us a consistent picture at a point in time
        - Requires row locking
        - Not optimized for scans over large amounts of data


    - Example - BigQuery

        - Utilizes point-in-time full table commit model
        - When read query is issued, reads from latest committed snapshot of the table
        - Does not lock table during reads
        - Subsequent writes create new commits and snapshots
        - To provent inconsistent state, only allows a single writer at a time
        - Writers from multiple clients are queued in order of arrival
        - This commit model is similar to Snowflake, Spark, others


    - Example - MongoDB

        - Variable consistency DB, configuration options for DB and per table
        - Extraordinary scalability and write concurrency in certain modes
        - DB will discard writes if it gets overwhelmed with traffic
        - This is suitable for IoT readings where data loss is fine, but not if we need exact data



- Improve Query Performance - Vacuum Dead Records

    - Transactions incur the overhead of creating new records while retaining old records as pointers
        the previous state of the database.  Over time, these old records accumulate in the filesystem,
        and we should remove them using a process called 'vacuuming'.


    - You can vacuum a single table, multiple tables, or all tables in a DB.  This frees up space for
        new records, makes query plans more accurate, and makes indexes more performant.



- Improve Query Performance - Leverage Cached Query Results

    - If you have a long-running query, it can save a lot of resources to save the results and return
        them on subsequent reads.  Most OLAP databases provide ways to automatically or manually
        cache query results.


    - Note that materialized views provide another form of query caching.



- Querying Streams - Basic Query Patterns

    - Since streaming data is constantly in flight, it has very different querying patterns.  We
        must adapt query patterns to reflect it's real-time nature.


    - The Fast-Follower Approach

        - Continuous CDC basically sets up an analytics DB as a fast follower to a PROD DB.

        - We need this approach, rather than just querying the OLTP, since we don't want to bog it
            down with analytics queries.

        - We could use an OLTP as the follower, but there are significant advantages to using an OLAP
            system.

        - Both Druid and BigQuery combine a streaming buffer with long-term columnar storage in a setup
            similar to a Lambda Architecture.

        - The CDC has limitations, in that it doesn't fundamentally rethink the batch pattern approach.
            You're still calling SELECT against the current table state, and missing the opportunity
            to manually trigger events off of changes in the stream.


    - The Kappa Architecture

        - The principle idea of the Kappa Architecture is to handle all data like events, and store
            these events in a stream rather than a table.

        - When OLTPs are the source, Kappa stores events from CDC.  Events can also flow from an app
            backend, IoT swarm, or any other system that generates events.

        - Instead of simply treating a streaming storage system as a buffer, Kappa retains events
            in storage during a more extended storage period, and data can be directly queried from
            this storage.  The retention period can be long (months or years).

        - The big idea of Kappa is to treat streaming storage as a real-time transport layer and a
            database for retrieving and querying historical data.  This happens either through direct
            query capabilities of the streaming storage or with the help of external tools.

        - For instance, Kakfa KSQL supports aggregation, statistical calculations, and sessionization.
            If query requirements are more complex, and data needs to be combined with other sources,
            an external tool such as Spark reads a time range from Kafka and computes the query
            results.  The streaming storage system can also feed other applications or a stream 
            processor such as Flink or Beam.



- Querying Streams - Windows, Triggers, Emitted Statistics, and Late-Arriving Data

    - One fundamental limitation of traditional batch queries is that this paradigm generally treats
        the query engine as an external observer.  An actor external to the data causes the query to
        run based on a cron job or someone loading a dashboard.


    - Most streaming systems, on the other hand, support the notion of computations triggered directly
        from the data itself.  They might emit median and mean statistics every time a certain number
        of records are collected in the buffer or output a summary when a user session closes.


    - Windows are an essential feature in streaming queries and processing.  'Windows' are small 
        batches that are processed based on dynamic triggers.  Windows are generated dynamically over
        time in several ways.



- Session Windows

    - A 'session window' groups events that occur close together, and filters out periods of
        inactivity when no events occur.  We might say that a user session is any time interval
        with no inactivity gap of 5 minutes or more.


    - Our batch system collects data by a user ID key, orders events, determines the gaps and session
        boundaries, and calculates statistics for each session. Data engineers often sessionize data 
        retrospectively by applying time conditions to user activity on web and desktop apps.


    - In a streaming session, this process can happen dynamically.  Note that session windows are
        per key.  In this example, each user gets their own set of windows.


    - The system accumulates data per user. If a five-minute gap with no activity occurs, the system 
        closes the window, sends its calculations, and flushes the data. If new events arrive for the 
        use, the system starts a new session window.


    - Session windows may also make a provision for late-arriving data. Allowing data to arrive up
        to five minutes late to account for network conditions and system latency, the system will
        open the window if a late-arriving event indicates activity less than five minutes after the
        last event.


    - Making sessionization dynamic and near real-time fundamentally changes its utility. With 
        retrospective sessionization, we could automate specific actions a day or an hour after a user
        session closed (e.g., a follow-up email with a coupon for a product viewed by the user).

      With dynamic sessionization, the user could get an alert in a mobile app that is immediately useful
        based on their activity in the last 15 minutes.



- Fixed-time Windows

    - A 'fixed time (aka tumbling) window' has fixed time periods that run on a fixed schedule and
        processes all data since the previous window is closed.  


    - For example, we might close a window every 20 seconds and process all data arriving from the
        previous window to give a mean and median statistic. Statistics would be emitted as
        soon as they could be calculated after the window closed.


    - This is similar to traditional batch ETL processing, where we might run a data update job once
        every hour or day.  The streaming systems allows us to generate windows more frequently and
        deliver results with lower latency.  Batch is a special case of streaming!



- Sliding Windows

    - Events in a 'sliding window' (aka 'hopping window') are bucketed into windows of fixed time length.  
        For example, we could generate a new 60-second window every 30 seconds. Just as we did before, 
        we can emit mean and median statistics.


        |----Events every 60s------|
                     |----Events every 60s-------|
                                   |----Events every 60s--------|


        -------------|-------------|-------------|--------------|
            30 s          30 s          30 s            30 s
        |_______________________________________________________|
                              2 mins


    - The sliding can vary.  We might think of the window as sliding continuously but emitting statistics
        only when certain conditions (triggers) are met.


    - Suppose we used a 30-second continuously sliding window but calculated a statistic only when a 
        user clicked a particular banner. This would lead to an extremely high rate of output when many 
        users click the banner, and no calculations during a lull.



- Watermarks

    - Sometimes, data is ingested out of the order from which it originated.  A 'watermark' is a 
        threshold used by a window to determine whether data in a window is within the established time
        interval or whether it's considered late.


    - If data arrives that is new to the window but older than the timestamp of the watermark, it is
        considered late-arriving data.


           Event                      |
                        Event         |   Event
                  Event               |           Event
                                      |
        ------------------------------|--------------------------> Time
                                      |
                                  Watermark



- Querying Streams - Combining Streams with Other Data

    - We often derive value from data by combining it with other data.  With streaming data, multiple
        streams can be combined, or a stream can be combined with batch historical data.


    - Conventional Table Joins

        - Some tables may be fed by streams.  The most basic approach to comining data is to just
            join these 2 tables in a database.


            Stream 1  ->  Table 1  ->  New Table (1 & 2)
            Stream 2  ->  Table 2  ->


    - Enrichment

        - 'Enrichment' means that we join a stream to other data.  Typically, this is done to provide
            enhanced data into another stream.

        - For examle, an online retailer receives an event stream from a partner business containing
            Product and User Ids.  The retailer wishes to enhance these events with product details
            and demographic information on the users.

        - The retailer feeds these events into a serverless function that looks up the product and user 
            in an in-memory DB (ie a cache), adds the additional information, then outputs the events
            to another stream.

            Stream  ---------------------->
                                                Enriched data
            Data in Object Storage   ----->


        - In practice, the enrichment source could be almost anywhere - a table in a cloud DW, a RDBMS,
            an object in file storage, etc.



    - Stream-to-Stream Joining

        - Increasingly, streaming systems support direct stream-to-stream joining.  For instance, an
            online retailer might join its web event data with streaming data from an ad platform.

        - The company can feed both streams into Spark, but a variety of complications arise.  For
            instance, the streams may have different latencies.  Certain events may be significantly
            delayed.

        - Because of this, streaming join architectures rely on streaming buffers.  The buffer
            retention interval is configurable.  A longer retention interval requires more storage and
            other resouces.


            Ad impression (ad_id)  ->  Stream Buffer \
                                                       ->  Join Processor  ->  Site Events w/ 
            Site events (ad_id)    ->  Stream Buffer /                         Ad impression details
