-----------------------------------------------------------------------------
| CHAPTER 8 - QUERIES, MODELING, & TRANSFORMATION                           |
-----------------------------------------------------------------------------

- Queries

    - A 'query' allows you to retrieve and act on data.  It gives you CRUD capabilities.


    - To create the database objects, we use DDL.

        CREATE DATABASE bar;
        CREATE TABLE bar_table;
        DROP TABLE bar_table;


    - To add and alter data within these database objects, we use DML.

        SELECT
        INSERT
        UPDATE
        DELETE
        COPY
        MERGE


    - To limit access to database objects, we use DCL (Data Control Language).

        GRANT
        DENY
        REVOKE


    - To control the details of transactions, we use TCL (Transaction Control Language).

        COMMIT
        ROLLBACK



- The Life of a Query

    - When we execute a SQL query, this is a summary of what happens:


        SQL Query  ->  Parsing and    ->  Query Planning    ->  Query      ->  Results
        Issued         Conversion to      and Optimization      Execution      Returned
                       Byte code


        1. The DB engine compiles the SQL, parses the code to check for proper semantics, and ensures
             the DB objects referenced exist and the current user has access to them.

        2. The SQL code is converted into byte code.  This bytecode expresses the steps that must be 
             executed on the database engine in an efficient, machine-readable format.

        3. The database’s query optimizer analyzes the bytecode to determine how to execute the query,
             reordering and refactoring steps to use available resources as efficiently as possible.

        4. The query is executed, and results are produced.


    - A query optimizer's job is to optimize query performance by performing steps in an efficient
        order.  It will assess joins, indexes, data scan size, and other factors.



- Improving Query Performance - Optimizing JOIN Strategy and Schema

    - A single table is rarely useful on it's own.  We create value by combining it with other datasets.
        Joins are the most common way of combining datasets and creating new ones.


    - A common technique for improving query performance is to pre-join data.  If analytics queries 
        are joining the same data repeatedly, it makes sense to join the data in advance and have
        queries read from the pre-joined data.

      This may mean relaxing normalization to widen the tables.  We could also create materialized
        views with the joined data and have users query the views.


    - We should also consider the complexity of our join conditions.  We can improve complex joins
        in a few ways.

        1. Many row-oriented DBs allow you to index a result computed from a row.  For instance,
             Postgres allows you to create an index on a string field converted to lowercase.
             When the query optimizer sees a 'lower()' in the query, it will use the indes.

        2. We can use CTEs (Common Table Expressions) instead of nested subqueries or temporary
             tables.  This increases readability, and also can improve performance by not having
             to create intermediate tables.


    - Row Explosion

        - One obscure but frustrating problem is 'row explosion'.  This occurs when we have a large 
            number of many-to-many matches, either because of repetition in join keys or as a 
            consequence of join logic.

        - Suppose the join key in TableA has the value 'this' repreated 5 times, and the join key in
            TableB contains this same value repeated 10 times.  This leads to a cross-join of each of
            these rows, creating 50 rows in the output.

        - Row explosions can often generate enough rows to consume massive resources or cause the
            query to fail.

        - We should check to see if the query optimizer is able to reorder joins, and if it isn't,
            we should reorder them in our query to alleviate this problem



- Improving Query Performance - Use the Explain Plan

    - The query optimizer's explain plan will show you how it determined the lowest-cost query, the
        DB objects used (tables, indexes, cache, etc.), and the performance statistics at each stage.


    - In addition, we should monitor the performance of our queries.  Things to watch include:

        1. Usage of disk, memory, network
        2. Data loading time vs processing time
        3. Query execution time, number of records, size of data scanned, size of data shuffled
        4. Competing queries that might cause resource contention
        5. Number of concurrent connections and connections available



- Improving Query Performance - Avoid Full Table Scans

    - You should only query the data you need.  Avoid 'SELECT *' queries with no predicates.


    - Whenever possible, use 'pruning' to reduce the quantity of data scanned in a query.  This 
        different strategies in row- and column-oriented DBs.


    - In a column-oriented DB, only select the columns you need.  Many column-oriented OLAP
        databases provide tools for optimiation, for example Snowflake and BigQuery let you
        define a cluster key, which orders the table in a desired way.


    - In a row-oriented DB, pruning centers around table indexes.  Create indexes that will improve
        the performance of your most important queries, but don't create so many that you degrade
        performance.



- Improve Query Performance - Know How Your Database Handles Commits

    - A database 'commit' is a change within a database, such as creating, updating, or deleting
        a record, table, or other database objects.  Many DBs support 'transactions', which combine
        several operations in a way that maintains a consistent state.


    - The purpose of a transaction is to keep the database in a consistent state both when it's active
        and in the case of a failure.  Also, transactions handle isolation when concurrent events
        are happening to the same DB objects.


    - Example - Postgres

        - Relational DB that uses ACID transactions
        - Each transaction is a package of operations that will succeed or fail as a group
        - Analytics queries will always give us a consistent picture at a point in time
        - Requires row locking
        - Not optimized for scans over large amounts of data


    - Example - BigQuery

        - Utilizes point-in-time full table commit model
        - When read query is issued, reads from latest committed snapshot of the table
        - Does not lock table during reads
        - Subsequent writes create new commits and snapshots
        - To provent inconsistent state, only allows a single writer at a time
        - Writers from multiple clients are queued in order of arrival
        - This commit model is similar to Snowflake, Spark, others


    - Example - MongoDB

        - Variable consistency DB, configuration options for DB and per table
        - Extraordinary scalability and write concurrency in certain modes
        - DB will discard writes if it gets overwhelmed with traffic
        - This is suitable for IoT readings where data loss is fine, but not if we need exact data



- Improve Query Performance - Vacuum Dead Records

    - Transactions incur the overhead of creating new records while retaining old records as pointers
        the previous state of the database.  Over time, these old records accumulate in the filesystem,
        and we should remove them using a process called 'vacuuming'.


    - You can vacuum a single table, multiple tables, or all tables in a DB.  This frees up space for
        new records, makes query plans more accurate, and makes indexes more performant.



- Improve Query Performance - Leverage Cached Query Results

    - If you have a long-running query, it can save a lot of resources to save the results and return
        them on subsequent reads.  Most OLAP databases provide ways to automatically or manually
        cache query results.


    - Note that materialized views provide another form of query caching.



- Querying Streams - Basic Query Patterns

    - Since streaming data is constantly in flight, it has very different querying patterns.  We
        must adapt query patterns to reflect it's real-time nature.


    - The Fast-Follower Approach

        - Continuous CDC basically sets up an analytics DB as a fast follower to a PROD DB.

        - We need this approach, rather than just querying the OLTP, since we don't want to bog it
            down with analytics queries.

        - We could use an OLTP as the follower, but there are significant advantages to using an OLAP
            system.

        - Both Druid and BigQuery combine a streaming buffer with long-term columnar storage in a setup
            similar to a Lambda Architecture.

        - The CDC has limitations, in that it doesn't fundamentally rethink the batch pattern approach.
            You're still calling SELECT against the current table state, and missing the opportunity
            to manually trigger events off of changes in the stream.


    - The Kappa Architecture

        - The principle idea of the Kappa Architecture is to handle all data like events, and store
            these events in a stream rather than a table.

        - When OLTPs are the source, Kappa stores events from CDC.  Events can also flow from an app
            backend, IoT swarm, or any other system that generates events.

        - Instead of simply treating a streaming storage system as a buffer, Kappa retains events
            in storage during a more extended storage period, and data can be directly queried from
            this storage.  The retention period can be long (months or years).

        - The big idea of Kappa is to treat streaming storage as a real-time transport layer and a
            database for retrieving and querying historical data.  This happens either through direct
            query capabilities of the streaming storage or with the help of external tools.

        - For instance, Kakfa KSQL supports aggregation, statistical calculations, and sessionization.
            If query requirements are more complex, and data needs to be combined with other sources,
            an external tool such as Spark reads a time range from Kafka and computes the query
            results.  The streaming storage system can also feed other applications or a stream 
            processor such as Flink or Beam.



- Querying Streams - Windows, Triggers, Emitted Statistics, and Late-Arriving Data

    - One fundamental limitation of traditional batch queries is that this paradigm generally treats
        the query engine as an external observer.  An actor external to the data causes the query to
        run based on a cron job or someone loading a dashboard.


    - Most streaming systems, on the other hand, support the notion of computations triggered directly
        from the data itself.  They might emit median and mean statistics every time a certain number
        of records are collected in the buffer or output a summary when a user session closes.


    - Windows are an essential feature in streaming queries and processing.  'Windows' are small 
        batches that are processed based on dynamic triggers.  Windows are generated dynamically over
        time in several ways.



- Session Windows

    - A 'session window' groups events that occur close together, and filters out periods of
        inactivity when no events occur.  We might say that a user session is any time interval
        with no inactivity gap of 5 minutes or more.


    - Our batch system collects data by a user ID key, orders events, determines the gaps and session
        boundaries, and calculates statistics for each session. Data engineers often sessionize data 
        retrospectively by applying time conditions to user activity on web and desktop apps.


    - In a streaming session, this process can happen dynamically.  Note that session windows are
        per key.  In this example, each user gets their own set of windows.


    - The system accumulates data per user. If a five-minute gap with no activity occurs, the system 
        closes the window, sends its calculations, and flushes the data. If new events arrive for the 
        use, the system starts a new session window.


    - Session windows may also make a provision for late-arriving data. Allowing data to arrive up
        to five minutes late to account for network conditions and system latency, the system will
        open the window if a late-arriving event indicates activity less than five minutes after the
        last event.


    - Making sessionization dynamic and near real-time fundamentally changes its utility. With 
        retrospective sessionization, we could automate specific actions a day or an hour after a user
        session closed (e.g., a follow-up email with a coupon for a product viewed by the user).

      With dynamic sessionization, the user could get an alert in a mobile app that is immediately useful
        based on their activity in the last 15 minutes.



- Fixed-time Windows

    - A 'fixed time (aka tumbling) window' has fixed time periods that run on a fixed schedule and
        processes all data since the previous window is closed.  


    - For example, we might close a window every 20 seconds and process all data arriving from the
        previous window to give a mean and median statistic. Statistics would be emitted as
        soon as they could be calculated after the window closed.


    - This is similar to traditional batch ETL processing, where we might run a data update job once
        every hour or day.  The streaming systems allows us to generate windows more frequently and
        deliver results with lower latency.  Batch is a special case of streaming!



- Sliding Windows

    - Events in a 'sliding window' (aka 'hopping window') are bucketed into windows of fixed time length.  
        For example, we could generate a new 60-second window every 30 seconds. Just as we did before, 
        we can emit mean and median statistics.


        |----Events every 60s------|
                     |----Events every 60s-------|
                                   |----Events every 60s--------|


        -------------|-------------|-------------|--------------|
            30 s          30 s          30 s            30 s
        |_______________________________________________________|
                              2 mins


    - The sliding can vary.  We might think of the window as sliding continuously but emitting statistics
        only when certain conditions (triggers) are met.


    - Suppose we used a 30-second continuously sliding window but calculated a statistic only when a 
        user clicked a particular banner. This would lead to an extremely high rate of output when many 
        users click the banner, and no calculations during a lull.



- Watermarks

    - Sometimes, data is ingested out of the order from which it originated.  A 'watermark' is a 
        threshold used by a window to determine whether data in a window is within the established time
        interval or whether it's considered late.


    - If data arrives that is new to the window but older than the timestamp of the watermark, it is
        considered late-arriving data.


           Event                      |
                        Event         |   Event
                  Event               |           Event
                                      |
        ------------------------------|--------------------------> Time
                                      |
                                  Watermark



- Querying Streams - Combining Streams with Other Data

    - We often derive value from data by combining it with other data.  With streaming data, multiple
        streams can be combined, or a stream can be combined with batch historical data.


    - Conventional Table Joins

        - Some tables may be fed by streams.  The most basic approach to comining data is to just
            join these 2 tables in a database.


            Stream 1  ->  Table 1  ->  New Table (1 & 2)
            Stream 2  ->  Table 2  ->


    - Enrichment

        - 'Enrichment' means that we join a stream to other data.  Typically, this is done to provide
            enhanced data into another stream.

        - For examle, an online retailer receives an event stream from a partner business containing
            Product and User Ids.  The retailer wishes to enhance these events with product details
            and demographic information on the users.

        - The retailer feeds these events into a serverless function that looks up the product and user 
            in an in-memory DB (ie a cache), adds the additional information, then outputs the events
            to another stream.

            Stream  ---------------------->
                                                Enriched data
            Data in Object Storage   ----->


        - In practice, the enrichment source could be almost anywhere - a table in a cloud DW, a RDBMS,
            an object in file storage, etc.



    - Stream-to-Stream Joining

        - Increasingly, streaming systems support direct stream-to-stream joining.  For instance, an
            online retailer might join its web event data with streaming data from an ad platform.

        - The company can feed both streams into Spark, but a variety of complications arise.  For
            instance, the streams may have different latencies.  Certain events may be significantly
            delayed.

        - Because of this, streaming join architectures rely on streaming buffers.  The buffer
            retention interval is configurable.  A longer retention interval requires more storage and
            other resouces.


            Ad impression (ad_id)  ->  Stream Buffer \
                                                       ->  Join Processor  ->  Site Events w/ 
            Site events (ad_id)    ->  Stream Buffer /                         Ad impression details



- What is a Data Model?

    - Well-constructed data architectures must reflect the goals and business logic of the organization
        that relies on the data.  This is cricial for making data useful for the business.


    - Data modeling (both RDBMS and DW) were almost always used until the early-to-mid 2010s, but
        modeling your data became unfashionable in the early big data era.  Sometimes this was done
        for legitimate performance reasons, and sometimes it just led to lots of redundant, mismatched,
        and wrong data.


    - The growing popularity of data management has led to a recognition that modeling is critical to
        move up the 'Data Science Hierarchy of Needs' pyramid.  We will need new data modeling 
        techniques to embrace the needs of streaming and ML.


    - A 'data model' represents the way data relates to the real world.  It reflects how the data must 
        be structured and standardized to best reflect your organization’s processes, definitions, 
        workflows, and logic. 


    - A good data model captures how communication and work naturally flow within your organization.
        In contrast, a poor data model (or nonexistent one) is haphazard, confusing, and incoherent.


    - For an example, a 'customer' might mean different things to different departments in the company.
        Is a customer someone who has bought something from you in the last 30 days?  In the last
        year?  Carefully defining this can have a massive impact on downstream reports.



- Conceptual, Logical, and Physical Data Models

    - 'Conceptual' models contain logic and rules and describe the system's schema, tables, and 
        fields (names and types).  Visualizing ER diagrams are the a helpful way to create these models.


    - 'Logical' models describe how the conceptual mode will be implemented by adding a lot more
        detail.  For example, we would add information on the types of customer ID, customer names,
        and custom addresses.


    - 'Physical' models define how the logical model will be implemented in a DB system.  We would
        add specific databases, schemas, and tables to our logical model, and add configuration
        details.


    - Successful modeling involves stakeholders from the beginning of the process.  It is a practice
        everyone must continuously participate in.


    - Another important consideration when modeling is the 'grain' of the data, the resolution at
        which data is stored and queried.  The grain is typically at the PK of the data (ie user_id,
        order_id, product_id).

        - For an example of a coarse-grained report, we create a report that includes all customers
            who made orders in a day, the total number of orders they made that day, and the total
            amount they spent.

        - For a more fine-grained report, we could break down each order by the products ordered.
            This data is at the grain of the customer-order level.

        - It would be tempting, if asked for only the coarse-grained report, to just create a
            reporting table with only the data asked for.  However, a more experienced engineer will
            probably create the tables with detailed data on customer orders and aggregate it for
            the simple report, since someone will ask for it later.

        - In general, we should strive to model data at the lowest level of grain possible.  This
            is because it is generally impossible to restore details that have been aggregated away.



- Normalization

    - 'Normalization' is a DB data modeling practive that enforces strict control over the relationship
        of tables and columns within a DB.  The goal is to remove redundancy and ensure referential
        integrity.


    - Normalization, as applied to relational DBs, was introduced by Edgar Codd in the early 1970s.
        He outlined 4 objectives for normalization:

        1. Free the collection of relations from undesirable insert, update, and delete dependencies.

        2. Reduce the need for restructuring the collection as new data types are introduced.

        3. Make the relational model more informitive to users.

        4. Make the collection neutral to query statistics.


    - He introduced 'normal forms'.  They are sequential, with each form incorporating the condition
        of prior forms.

        - Denormalized = No normalization, nested and redundant data is allowed

        - 1NF = Each column is unique and has a single value.  The table has a unique PK.

        - 2NF = Partial dependencies are removed.  Partial dependencies occur when a subset of fields
                  in a composite key can be used to determine a nonkey column of the table.

        - 3NF = Each table contains only data relevant to it's PK, and has no transitive dependencies.
                  Transitive dependencies occur when a nonkey field depends on another nonkey field.



- Normalization Example

    - Denormalized


        [ORDER_DETAILS]  PK = OrderID

        OrderID    OrderItems                    CustomerID    CustomerName     OrderDate
        --------------------------------------------------------------------------------------
        100        [{                            5             Joe Reis         2022-03-01
                      "sku": 1,
                      "price": 50,
                      "quantity": 1,
                      "name:": "Thingamajig"
                    }, 
                    {
                      "sku": 2,
                      "price": 25,
                      "quantity": 2,
                      "name:": "Whatchamacallit"
                    }]



    - 1NF (Remove Multivaled Attribute)


        [ORDER_DETAILS]  PK = OrderId

        OrderID   Sku   Price   Quantity   ProductName       CustomerID   CustomerName   OrderDate
        --------------------------------------------------------------------------------------------
        100       1     50      1          Thingamajig       5            Joe Reis       2022-03-01
        100       2     25      2          Whatchamacallit   5            Joe Reis       2022-03-01



    - 1NF (Unique PK)


    [ORDER_DETAILS]  PK = (OrderID, LineItemNumber)

    OrderID  LineItemNumber Sku  Price  Quantity  ProductName      CustomerID  CustomerName  OrderDate
    ----------------------------------------------------------------------------------------------------
    100      1               1    50     1        Thingamajig      5           Joe Reis      2022-03-01
    100      2               2    25     2        Whatchamacallit  5           Joe Reis      2022-03-01



    - 2NF (Remove Partial Dependencies)


        [ORDERS] PK = OrderId

        OrderID   CustomerID   CustomerName    OrderDate
        ----------------------------------------------------
        100       5            Joe Reis        2022-03-01
        101       7            Matt Housley    2022-03-01
        102       7            Matt Housley    2022-03-01



        [ORDER_LINE_ITEMS]  PK = (OrderID, LineItemNumber)

        OrderID   LineItemNumber   Sku   Price   Quantity   ProductName
        ------------------------------------------------------------------
        100       1                1     50      1          Thingamajig
        100       2                2     25      2          Whatchamacallit
        101       1                3     75      1          Whozeewhatzit
        102       1                1     50      1          Thingamajig



    - 3NF (Remove Transitive Dependencies)


        [ORDERS] PK = OrderId

        OrderID   CustomerID   OrderDate
        ------------------------------------
        100       5            2022-03-01
        101       7            2022-03-01
        102       7            2022-03-01


        [CUSTOMERS]  PK = CustomerID

        CustomerID   CustomerName 
        --------------------------
        5            Joe Reis     
        7            Matt Housley 
        7            Matt Housley


        [ORDER_LINE_ITEMS]  PK = (OrderID, LineItemNumber)

        OrderID   LineItemNumber   Sku   Price   Quantity
        -------------------------------------------------
        100       1                1     50      1       
        100       2                2     25      2       
        101       1                3     75      1       
        102       1                1     50      1       


        [SKUS]  PK = (Sku)

        Sku     ProductName
        ----------------------
        1       Thingamajig
        2       Whatchamacallit
        3       Whozeewhatzit
        1       Thingamajig




- Modeling Batch Analytical Data - Inmon

    - Bill Inmon, father of the DW, created his approach to data modeling in 1990.  Before the DW, 
        the analysis would usually occur directly on the source system.


    - Bill's definition of a DW:

        A DW is a subject-oriented, integrated, nonvolatile, and time-variant collection of data in 
          support of management’s decisions. The data warehouse contains granular corporate data. Data 
          in the data warehouse is able to be used for many different purposes, including sitting and 
          waiting for future requirements which are unknown today.

        1. Subject-Oriented = Focuses on specific area such as sales or marketing
        2. Nonvolatile = Data is unchanged after it is stored in a DW
        3. Integrated = Data from disparate sources is consolidated and normalized
        4. Time-Variant = Varying time ranges can be queried


    - Data should be modeled in support of management's decisions.  This is a break from the OLTP
        approach.  However, the DW still uses 3NF normalization.


    - Example - Ecommerce


       Orders OLTP     ->            DW             -> Sales Data Mart
       Inventory OLTP  ->  ELT  ->  (3NF)  --  ETL  -> Marketing Data Mart
       Marketing OLTP  ->                           -> Purchasing Data Mart


    - In this example, Sales, Marketing, and Purchasing each have their own star schema.  This allows
        each department to have it's own data structure that's unique and optimized to its specific
        needs.



- Modeling Batch Analytical Data - Kimball

    - Ralph Kimball's approach, created in the early 1990s, takes the opposite approach.  It focuses
        less on normalization, and sometimes allows denormalization.


    - The Inmon model integrates data from across the business in the DW, and serves department-specific
        analytics via data marts.  The Kimball model is bottom-up, encouraging you to model and serve
        department or business analytics in the data warehouse itself.

      This enables faster iteration and modeling, but can lead to looser data integration, redundancy,
        and duplication.


    - Data is modeled with 2 general types of tables: facts and dimensions.  A 'fact table' can be
        thought of as a table of numbers, and 'dimension tables' are qualitative data referencing a
        fact.


    - Dimension tables surround a single fact table in a relationship called a 'star schema'.

         Dimension1  \            / Dimension3
                       Fact Table
         Dimension2  /            \ Dimension4



- Fact Tables

    - Fact bales contain factual, quantitative, and event-related data.


    - The data in a fact table is immutable, since facts relate to events.  They are append-only.


    - Fact tables are typically narrow and long, and should be at the lowest grain possible.


    - Queries against a star schema start with the fact table.

        
    - Each row of the table should represent the grain of the data.  Avoid aggregating or deriving
        data within a fact table.  Perform aggregations or derivations in a downstream query,
        a data mart table, or a view.


    - Fact tables don't reference other fact tables, only dimension tables.


    - Here is a fact table for sales events:

        OrderID   CustomerKey   DateKey    GrossSalesAmt
        ----------------------------------------------------
        100       5             20220301   100.00
        101       7             20220301   75.00
        102       7             20220301   50.00



- Dimension Tables

    - Dimension tables provide the reference data, attributes, and relational context for the 
        events stored in the fact tables.


    - Dimension tables are smaller, and are typically wide and short.


    - When joined to a fact table, dimensions describe the what/where/why of the events.


    - Dimensions are denormalized, with the possibility of duplicated data.

        
    - Dates are typically stored in a date dimention, allowing you to reference a DateKey between
        the fact and date dimension table.  This allows you to quickly answer questions like 
        "How many customers shop on Tuesday vs Wednesday?"

        DateKey   Date-ISO    Year   Quarter   Month   Day-of-week
        -----------------------------------------------------------
        20220301  2022-03-01  2022   1         3       Tuesday
        20220302  2022-03-02  2022   1         3       Wednesday
        20220303  2022-03-03  2022   1         3       Thursday


    - Here, we look at the customer dimension table.  In addition to the fields you expect, there are
        several date fields, which are 'Type 2 slowly changing dimensions'.  They define how long the
        customer has been in the system.

        CustomerKey   FirstName   LastName   ZipCode   EFF_StartDate   EFF_EndDate
        ----------------------------------------------------------------------------
        5             Joe         Reis       84108     2019-01-04      9999-01-01
        7             Matt        Housley    84101     2020-05-04      2021-09-19
        7             Matt        Housley    84123     2021-09-19      9999-01-01
        11            Lana        Belle      90210     2022-02-04      9999-01-01


    - We can see that a new record was created when the customer updated their zip code.  This
        'SCD (Slowly Changing Dimension)' is necessary to track changes in dimensions.  This is a
        type-2 SCD.


    - There are 7 levels of SCDs, but we'll look at the most common ones here:

        Type 1 = Overwrite existing dimension records.  This is simple, but it means you have no access
                   to historical dimension records.

        Type 2 = Keep a full history of dimension records.  When a record changes, that specific record
                   is flagged as changed, and a new dimension record is created that reflects the
                   current status of the attributes.  This is done with the 'EFF_StartDate' and
                   'EFF_EndDate' fields in our example

        Type 3 = Similar to Type 2, but a new field is added to the existing record, instead of
                   creating a new record.


    - Here is an example of a Type 3 SCD:

        CustomerKey   FirstName   LastName   Original ZipCode   Current ZipCode   CurrentDate
        ---------------------------------------------------------------------------------------
        7             Matt        Housley    84101              84123             2021-09-19


    - Type 1 is the default in most DWs, and Type 2 is the one most commonly used in practice.



- Star Schema

    - The 'star schema' represents the data model of the business.  Unlike highly normalized approaches,
        the star schema is a fact table surrounded by the necessary dimensions.  This results in fewer
        joins, which speeds up queries, and is easier for business users to understand.


    - The star schema doesn't reflect a particular report, but a report can be modeled in a downstream
        or directly in a BI tool.  The star schema should reflect your business logic, and be
        flexible enough to answer critical questions.


    - Since a star schema only has one fact table, sometimes you have multiple star schemas for different
        events in the business.  Sometimes, dimensions can be used across multiple fact tables, these
        are called 'conformed dimensions'.

      Redundant data is OK, but avoid repeating the same dimension tables to avoid data integrity
        problems.



- Modeling Batch Analytical Data - Data Vault

    - Whereas Inmon and Kimball focus on the structure of business logic in the DW, the 'Data Vault'
        offers a different approach.  It was created in the 1990s by Dan Linstedt.


    - The data vault model separates the structural aspects of a source system's data from it's
        attributes.  Instead of representing business logic in facts, dimensions, or highly-normalized
        tables, a data vault simply loads data from source systems directly into a handful of 
        purpose-built tables in an insert-only manner.


    - There is no notion of good, bad, or conformed data.  Data moves fast, and this requires a
        agile, flexible, and scalable approach.  The purpose of this model is to keep data aligned
        as closely to the business as possible as it evolves.


    - A data vault consists of hubs, links, and satellites.  A 'hub' stores business keys, a 'link'
        maintains relationships among business keys, and a 'satellite' represents a business key's
        attributes and context.

      A user will query a hub, which will link to a satellite table containing the query's relevant
        attributes.


          Satellite 1  ->  Hub 1  ->  Link  <-  Hub 2  <-  Satellite 2



- Hubs

    - Queries involve searching by a business key, such as CustomerID or OrderID.  A hub is the central
        entity of a data vault that retains a record of all business keys loaded into the data vault.


    - The hub should reflect the state of the source system from which it loads data.


    - A hub always has the following fields:

        Hash key = The PK used to join data between systems.  This is a calculated field (ie MD5).

        Load date = The date the data was loaded into the hub

        Record source = The source system

        Business key = The business key, or record ID, in the source system


    - A hub is insert-only, and data from source systems is not altered in a hub.  Once data is loaded
        into a hub, it is permanent.


    - Here is an example of a hub for products:

        [HubProduct]

        ProductHashKey           LoadDate     RecordSource   ProductID
        --------------------------------------------------------------------------
        4041fd80ab68e267490522   2020-01-02   ERP            1
        de8435530d6ca93caabc00   2021-03-09   ERP            2
        cf27369bd80f53d0e60d39   2021-03-09   ERP            3


    - Here is a hub for orders:

        [HubOrder]

        OrderHashKey             LoadDate     RecordSource   OrderID
        --------------------------------------------------------------------------
        f899139df5e10593964314   2022-03-01   Website        100
        38b3eff8baf56627478ec7   2022-03-01   Website        101
        ec8956637a99787bd197ea   2022-03-01   Website        102



- Links

    - A 'link table' tracks the relationships of business keys between hubs.  A link table must connect
        at least 2 hubs.  They are many-to-many tables.


    - This approach is very flexible.  If you need to create a new hub, just create it and add a new
        column to the link table.


    - Here is a link table for products and orders:

        [LinkOrderProduct]

        OrderProductHashKey       LoadDate     RecordSource ProductHashKey         OrderHashKey
        -------------------------------------------------------------------------------------------------
        ff64ec193d7bacf05e8b97    2022-03-01   Website      4041fd80ab68e267490522 f899139df5e10593964314
        ff64ec193d7bacf05e8b97    2022-03-01   Website      de8435530d6ca93caabc00 f899139df5e10593964314
        e232628c251e3cad0f53f7    2022-03-01   Website      cf27369bd80f53d0e60d39 38b3eff8baf56627478ec7
        26166a5871b6d21ae12e9c    2022-03-01   Website      4041fd80ab68e267490522 48f5ec8956637a99787bd1



- Satellites

    - Satellites are descriptive attributes that give meaning and context to hubs.  Satellites can 
        connect to either hubs or links.


    - The only required fields in a satellite are a primary key consisting of the business key of the 
        parent hub and a load date. Beyond that, a satellite can contain however many attributes that 
        make sense.


    - Here is an example of a satellite table for the product hub:

        [SatelliteProduct]

        ProductHashKey           LoadDate     RecordSource   ProductName          Price
        ---------------------------------------------------------------------------------
        4041fd80ab68e267490522   2020-01-02   ERP            Thingamajig          50
        de8435530d6ca93caabc00   2021-03-09   ERP            Whatchamacallit      25
        cf27369bd80f53d0e60d39   2021-03-09   ERP            Whozeewhatzit        75


    - The entire architecture, joined together:

          HubProduct  ->  LinkOrderProduct  <-  HubOrders
              ^
              |
       SatelliteProduct



- Modeling Batch Analytical Data - Wide Denormalized Tables

    - The strict modeling approaches of Inmon and Kimball were developed when DWs were expensive,
        on-premises, and heavily resource-constrained with tightly coupled compute and storage.


    - For a few reasons, more relaxed approaches are becoming more common:

        1. The cloud makes storage dirt cheap.  It's cheaper to store data than to agonize about the
             correct format.

        2. Nested data (ie JSON) means schemas are flexible in source and analytical systems.


    - You can rigidly model your data using all the techniques above, or you can just throw it all into
        a single wide table.  A 'wide table' a highly denormalized and very wide collection of many 
        fields, typically created in a columnar database. A field may be a single value or contain 
        nested data.


    - Data is organized along with one or multiple keys.  These keys are closely tied to the grain
        of the table.


    - A wide table can have thousands of columns, most of which are sparse.  This contrasts from a
        relational DB approach, where NULLs would be required in all the empty columns.


    - Whereas adding a new column in a RDBMS is a slow and resource-heavy process, in wide tables it's
        a simple metadata change.


    - Removing joins can lead to a huge performance speedup on analytics queries.  You don't have to
        join facts to dimensions.


    - Here is a wide table example:

        OrderID   OrderItems                 CustomerID  CustomerName  OrderDate  Site    SiteRegion
        --------------------------------------------------------------------------------------------
        100       [{                         5           Joe Reis      2022-03-01 abc.com  US
                    "sku": 1,
                    "price": 50,
                    "quantity": 1,
                    "name:": "Thingamajig"
                    }, 
                    {
                    "sku": 2,
                    "price": 25,
                    "quantity": 2,
                    "name:": "Whatchamacallit"
                  }]



- Modeling Streaming Data

    - Because of the unbounded and continuous nature of streaming data, traditional batch techniques
        are tricky, if not impossible, to use.  The world is moving from batch and on-premises to
        streaming and cloud, and some constraints no longer apply.


    - There isn't a consensus yet from DEs about how to model streaming data.  Many suggest a data
        vault as an option.


    - 2 main types of streams exist:

        1. Event streams
        2. CDC

      Most of the time, this data is semistructured (ie JSON).  The challenge is that the payload's
        schema might change on a whim.  An IoT device might add a field and break downstream
        processing.  A CDC system might recast a field to a different type.


    - Streaming data experts overwhelmingly suggest you anticipate changes in the source data and
        keep a flexible schema.  This means there is no rigid model in the analytical DB.  Assume the
        source systems are providing you with the right logic, as it exists today.


    - In a world in the not-too-distant future, we may see a new data modeling paradigm that 
        incorporates metrics and semantic layers, data pipelines, and traditional analytics workflows
        in a streaming layer that sits directly on top of the source system.



- Transformations

    - Transformations manipulate, enhance, and save data for downstream use, increasing it's value
        in a scalable, reliable, and cost-effective manner.


    - Whereas a query retrieves data based on filtering and join logic, a transformation persists
        the results of computation.  The results may be stored ephemerally or permanently.

      Also, complexity can be managed with CTEs or subqueries in a query, but it is much easier to
        deal with in a pipeline where you can have multiple steps.


    - Transformations rely on orchestration.  Orchestration combines many discrete operations, such
        as intermediate transformations, that store data temporarily or permanently for consumption
        by downstream transformations and serving.



- Batch Transformations

    - 'Batch transformations' run on discrete chunks of data, unlike streaming transformations, where
        data is processed continuously as it arrives.


    - Batch transformations can run on a fixed schedule to support ongoing reporting, analytics, and
        ML models.



- Distributed Joins

    - The basic idea behind distributed joins is that we need to break a 'logical join' (the join 
        defined by query logic) into much smaller 'node joins' that run on individual servers in the
        cluster.


    - The basic distributed join patterns apply whether one is in MapReduce, BigQuery, Snowflake, or
        Spark.  The details of intermediate storage vary between them (on disk or in memory).


    - In the best-case scenario, the data on one side of the join is small enough to fit on a single
        node (broadcast join).  Often, a more resouce-intensive shufle hash join is required.


    - Broadcast Joins

        - A 'broadcast join' is generally asymmetric, with one large table distributed across nodes,
            and one small table that can easily fit on a single node.

        - The query engine 'broadcasts' the small table out to all nodes, where it gets joined to the
            parts of the large table.


        - Logical Join

            TableA   <-->  TableB
            (small)        (large)


        - Node Joins

            TableA    TableA    TableA    TableA
            TableB1   TableB2   TableB3   TableB4  ...


        - In practice, TableA is often a down-filtered larger table that the query engine collects and
            broadcasts.  Join reordering is one of the top priorities of query optimizers.  Early
            application of filters can dramatically improve performance.


    - Shuffle Hash Joins

        - If neither table is small enough to fit on a single node, the query engine will use a 
            shuffle hash join.  A hasing algorithm is used to repartition the data by join key.


        - In this example, the hashing scheme will partition the join key into 3 parts, with each
            part assigned to a node.  The data is then reshuffled to the appropriate node, and new
            partitions for tables A and B on each node are joined.

            TableA-1    TableA-2    TableA-3
            TableB-1    TableB-2    TableB-3
              /|\         /|\         /|\

            TableA-1`   TableA-2`   TableA-3`
            TableB-1`   TableB-2`   TableB-3`
            


- ELT, ETL, and Data Pipelines

    - Batch ETL is a transformation pattern dating back to the early days of RDBMS.  Traditional ETL
        relies on an external transformation system to pull, transform, and clean data while 
        preparing it for a target schema (ie data mart or star schema).

      The transformed data would be loaded into a target system, such as a DW, where business analytics
        would be performed.


    - The ETL pattern itself was driven by the limitations of source and target systems.  The extract
        phase was a bottleneck due to the constraints of the source RDBMS.  And, the transformation
        was handled in a dedicated system because the target system was extremely resource-constrained
        in CPU and storage.


    - ELT is a now-popular evolution.  As DW systems have grown in performance and storage capacity,
        it has become common to simply extract data from a source system, import it into a DW with
        minimal transformation, and then clean and transform it directly in the DW system.


    - A second, slighly different form of ELT became popular with data lakes.  The data is loaded into
        the lake with no transformations at all.  The assumption is that transformation will happen
        at some time in the future.


    - The line has also blurred between ETL and ELT in the data lakehouse environment.  With object
        storage at the base layer, it's no longer clear what's in the database and what isn't.
        Data federation, virtualization, and live tables also complicate the picture.


    - Increasingly, the best practice is to apply ETL and ELT at the micro level (data pipelines)
        rather than the macro level (entire organization).  That way, we can determine the proper
        technique on a case-by-case basis.



- SQL and General-Purpose Code-Transformation Tools

    - Since the introduction of Hive into the Hadoop platform, SQL is a first-class citizen in the
        big data ecosystem.  For example, SparkSQL was an early feature of Spark.  Streaming frameworks
        like Kafka, Flink, and Beam also support SQL.


    - SQL is declaritive, but can still build complex workflows.  SQL can be effectively used to build
        complex DAGs using CTEs, SQL scripts, or an orchestration tool.


    - SQL has it's limitations, but we often see DE's doing things in Python and Spark that could more
        easily be done in SQL.


    - Example - When to Avoid SQL For Batch Transformations in Spark

        - When we decide between using Python/PySpark or SQL/SparkSQL, we should think about:

            1. How difficult is the transformation to write in SQL?
            2. How readable and maintainable will the resulting SQL be?
            3. Should some of the transformation code be pushed into a library for reuse?

        - If you need any external libraries for processing data, Python is a better choice.

        - If it needs to be reusable, Python is a better choice.



- Optimizing Spark and Other Processing Frameworks

    - Spark acolytes often complain the SQL doesn't give them full control over data processing.  The
        SQL engine takes your statements, optimizes, and runs them.  With Spark and other code-heavy
        frameworks, the code writer becomes responsible for much of the optimization.


    - Tips for coding in native Spark:

        1. Filter early and often

        2. Rely heavily on the core API, and try to rely on well-maintained public libraries if the
             native Spark API doesn't support your use case.

        3. Be careful with UDFs.

        4. Consider intermixing SQL.


    - Spark is a big data processing framework, and it is resource-heavy.  Only use it if you need it.


    - PySpark is an API wrapper for Scala Spark.  Your PySpark code pushes work into native Scala
        code running on the JVM by calling the API.  Running Python UDFs forces data to be passed
        to Python, which is much less efficient, so this is best to avoid.  If you must use UDFs, 
        write them in Java or Scala.


    - Using SQL allows us to take advantage of the Spark Catalyst optimizer, which may be able to
        squeeze out more performance than native code.



- Update Patterns

    - Since transformations persist data, we will often update persisted data in place.  Updating data
        is a major pain point for DE teams, especially as they transition between data engineering
        technologies.


    - The original data lake concept didn't really account for updating data, which was a major
        shortcoming.  It's silly to rerun transformations in queries over and over again, because we
        have no update capabilities.  GDPR now requires organizations to delete data in a targeted
        fashion.


    - Truncate and Reload

        - In this case, we simply wipe out the old data.  Transformations are rerun and loaded into
            the table, generating a new table version.


    - Insert Only

        - In this case, we insert new records without changing or deleting old records.

        - Insert-only patterns can be used to maintain a current view of the data.  A query of view
            can present the current data state by finding the newest record by primary key.

        - The downside is that it can be very computationally expensive to find the latest record at
            query time.

        - Alternatively, we can use a materialized view with a table is insert-only, while the view
            is a truncate-and-reload target table that holds the current state for serving data.

        - Note that inserting one row at a time is an antipattern in OLAP systems, and is very 
            inefficient.  One exception to this rule is the Lambda architecture used by BigQuery and
            Apache Druid, which hybridizes a streaming buffer with columnar storage.  Deletes and
            in-place updates can still be expensive, though.


    - Delete

        - Deletion is critical when a source system deletes data and satisfies regulatory changes.

        - A 'hard delete' permanently removes a record from a database, while a 'soft delete' just
            marks data as deleted.

        - Hard deletes are useful if you need a delete for performance reasons or to satisfy regulatory
            compliance.

        - Soft deletes are useful if you don't want to delete a record, just exclude it from query
            results.


    - Upsert / Merge

        - Upserting takes a set of source records and looks for matches against a target table using
            a PK.  When a key match occurs, the target record gets updated.  When no match occurs,
            a new record is inserted.  The merge pattern adds the ability to delete records.

        - The upsert pattern was created for row-based systems, where it is a natural pattern.  
            File-based systems, on the other hand, don't actually support in-place updates.  All of
            these systems use Copy-On-Write.  If one record in a file is changed or deleted, the
            entire file must be rewritten with the changes.

        - This is one reason why early big data technologies and data lakes rejected updates.  They
            assumed data consumers would just determine the current state at query time or in
            downstream transformations.

        - Columnar databases such as Vertica have long supported in-place updates by hiding the
            complexity of COW from users.  They scan files, change relevant records, write new files,
            and change pointers for the table.  Most major columnar cloud DWs support this as well,
            but you should ckeck before you decide to use a technology.

        - Even if updates are supported, we should be aware the performance impact of an update or
            delete is quite high.

        - Trying to run real-time merges or updates from CDC, just like you did on your old systems,
            will cause your columnar DB to bog down and fall behind, know matter how good your CDC
            system is.  A better approach would be to just run a merge once an hour.

        - BigQuery solves this problem by allowing us to stream insert new records into a table, then
            supports special materialized views that give us a near-real-time deduplicated table view.

        - Druid solves this problem by using two-tier storage with SSDs to support real-time queries.



- Schema Updates

    - Data has entropy, and may change without your control or consent.  External sources may change
        their schema, or app developers may add new fields.


    - Columnar DBs make this easier by allowing you to add/delete/rename columns on the fly.  In spite
        of this, practical schema management remains difficult.  Schema updates could be automated
        (FiveTran does this when replication from sources).  However, this risks having something
        break downstream:

        - Is there a schema update request process?  How will these requests be reviewed?

        - Will downstream processes break?  We shouldn't be running 'SELECT *' in columnar DBs.

        - How long will it take to implement the change?  Should we create a table fork?


    - One idea has emerged from semistructured data.  Borrowing the idea from document stores, we allow
        arbitrary JSON as a data type.  Then, we can extract fields out of the raw JSON.  This takes
        up additional space, but does allow for more flexibility.



- Data Wrangling

    - 'Data wrangling' takes messy, malformed data and turns it into useful, clean data.  This is
        generally a batch transformation process.

        1. We may need to do a good deal of text preprocessing just to ingest the data if it is
             malformed.  We might ingest it as a single field to do this.

        2. Then, we need to write queries to parse and break apart the data.

        3. Next, we'll need to handle anomalies and edge cases.  Eventually, we'll get the data into
             rough shape.


    - Data wrangling tools aim to simplify significant parts of this process.  Graphical data-wrangling
        tools typically present a sample of data in a visual interface, with inferred types, statistics 
        including distributions, anomalous data, outliers, and nulls.  Users can then add processing 
        steps to fix data issues.



- Example - Data Transformation in Spark

    - Suppose we build a pipeline that ingests data from 3 API sources in JSON format.  This initial
        step is handled in Airflow.  Each data source gets it's prefix (file path) in an S3 bucket.


    - Airflow then triggers a Spark job by calling an API.  The Spark job ingests each of the 3
        sources into a DataFrame, and converts the data into a relational format, with nesting in
        certain columns.  The Spark job combines the 3 sources into a single table, then filters the
        results with a SQL statement.  Finally, the results are written out to a Parquet-formatted
        Delta Lake table stored in S3.


    - In practice, Spark creates a DAG of steps based on the code that we write for ingesting, joining, 
        and writing out the data.


    - The basic ingestion of data happens in cluster memory, although one of the data sources is 
        large enough that it must spill to disk during the ingestion process.


    - The join requires a shuffle operation.  A key is used to redistribute data across the cluster.
        Once again, a spill to disk occurs as the data is written to each node.  The SQL transformation 
        filters through the rows in memory and discards the unused rows.


    - Finally, Spark converts the data into Parquet format, compresses it, and writes it back to S3. 
        Airflow periodically calls back to Spark to see if the job is completed. Once it confirms that 
        the job has finished, it marks the full Airflow DAG as completed. 



- Business Logic and Derived Data

    - One of the most common use cases for transformation is to render business logic.  This happens
        in batch transformations most frequently, but could also happen in a streaming pipeline.


    - Suppose that a company uses multiple specialized internal profit calculations. One version might 
        look at profits before marketing costs, and another might look at a profit after subtracting 
        marketing costs.


    - Profits before marketing costs:

        - Might need to account for fraudulent orders
        - Determining profit estimate for previous day entails estimating % of order fraudulent
        - Is there a special flag in the DB that indicates a high probability of fraud?
        - Is there a special flag for cancelled orders?
        - Does the business just assume a certain % of order are fraudulent?


    - Profits after marketing costs:

        - Does the company have a naive attribution model that just takes a certain % of all price?
        - Are the costs attributed by product line or department or something else?
        - If there is a nuanced version of attribution, is it available in a file or external DB?


    - This type of reporting data is an example of 'derived data', which is computed from other data
        stored in a data system.


    - Derived data critics mention that it is difficult for ETL to maintain consistency in the
        derived metrics.  If the attribution model is changed, how do our scripts get updated?

      However, this is the preferred approach.  Otherwise, analysts need to maintain the model itself
        and use it in every query.


    - One interesting alternative is to push business logic into a 'metrics' layer, but still leverage
        the DW to do the computational heavy lifting.  A metrics layer encodes business logic and
        allows analysts and dashboard users to build complex analytics form a library of defined
        metrics.



- MapReduce

    - MapReduce was the defining batch data transformation pattern of the big data era.  It is still
        influential today, even though it is much less often used.


    - MapReduce was introduced by Google in a follow-up paper to it's GFS paper.  It was the de facto
        processing pattern of Hadoop, the open source implementation of GFS.


    - A simple MapReduce job consists of a collection of map tasks that read individual data blocks
        scattered across the nodes, followed by a shuffle that redistributes result data across the 
        cluster and a reduce step that aggregates data on each node. 


    - For example, suppose that we wanted to run the following SQL query:

        SELECT COUNT(*), user_id
        FROM user_events
        GROUP BY user_id;


    - The table is spread across nodes in data blocks.  The MapReduce job generates one map task per
        block.  Each map tasks essentially runs the query on a single block (which might be hundreds
        of MB).  This map portion of the task is embarassingly parallel.


    - Then, we need to aggregate (reduce) to gather results from the full cluster.  We don't gather
        results to a single node.  We redistribute results by key so that each key ends up on only
        one node.  This is the shuffle step.

      Once the results have been shuffled, we sum the results for each key.  The key/count pairs
        can be written to local disk on the node where they are computed.


    - We collect the results stored across nodes to view the full query results.



- After MapReduce

    - The original MapReduce model is extremely powerful, but is now thought of as too rigid.  No
        intermediate state is preserved in memory.  This simplifies state and workflow management,
        but it causes high disk-utilization and increases processing time.


    - Post-MapReduce processing keeps the elements of map, shuffle, and reduce, but it relaxes the
        constraints of MapReduce to allow in-memory caching.


    - For example, Spark and BigQuery were designed around in-memory processing.  These frameworks
        treat data as a distributed set that resides in memory.  If data overflows available memory,
        this causes a spill to disk.  Disk is treated as a second-class storage layer.


    - Cloud has driven the adoption of these technologies, since it's much cheaper to rent memory
        when you need it than to buy it.



- Materialized Views, Federations, and Query Virtualization

    - Here, we look at several techniques for virtualizing query results by presenting them as
        table-like objects.  These techniques can become part of a transformation pipeline, or sit
        right before end-user consumption.


    - Views

        - A 'view' is a DB object that we can select from just like any other table.  A view is just
            a query that references other tables.

        - When we select from a view, that database creates a new query that combines the view subquery 
            with our query. The query optimizer then optimizes and runs the full query.

        - Views can serve a security role to restrict access to data.

        - Views can be used to provide a current deduplicated picture of data.  This is especially
            useful if we have an insert-only pattern.

        - Views can be used to present common data access patterns.  For instance, the marketing team
            always runs queries that join the same 5 tables.  So, we create a query that joins the
            5 tables for them.


    - Materialized Views

        - Where as views don't do any precomputation, a materialized view does the view computation
            in advance.

        - For instance, we could update the materialized view with the 5 tables joined together each
            time a change occurs in the source tables.


    - Composable Materialized Views

        - In general, materialized views don't allow composition.  A materialized view cannot select
            from another materialized view.

        - However, there are tools emerging that support this capability.  Databricks has a concept
            of 'live tables', where each table is updated as data arrives from sources.  The udpates
            are performed asynchronously.


    - Federated Queries

        - 'Federated queries' are a database feature that allows an OLAP DB to select from an external
            data source, such as object storage or an RDBMS.

        - For instance, let's say you need to join on tables in MySQL and Postgres DBs.  Your DW can 
            issue a federated query to these sources and return the combined results.


    - Data Virtualization

        - 'Data virtualization' is related to federated queries, but this entails a query system that
            doesn't store data internally.  Any query/processing engine that supports external tables
            can serve as a data virtualization engine.

        - Trino and Presto are the most common examples of this kind of system.

        - 'Query pushdown' is a closely related concept.  In this case, the query engine looks for
            ways to push filtering predicates to the source system for better performance.  This approach
            offloads the filtering to the RDBMS, and also reduces the amount of data returned.

        - This approach should be used carefully to avoid haphazard complexity, but it can be useful
            to abstract away data siloed in various parts of the company.



- Streaming Transformations and Processing

    - There are subtle differences between streaming queries and streaming transformations.


    - Basics

        - Streaming queries run dynamically to present a current view of data, while streaming
            transformations aim to prepare data for downstream consumption.

        - For instance, a DE team may have an incoming stream carrying events from an IoT source.
            These events carry a device_id and event data.  We wish to dynamically enrich the
            events with device metadata, which is stored in a separate DB.

        - The stream-processing engine queries a separate DB containing this metadata by device_id,
            generates new events with the added data, and passes it on to another stream.

        - Live queries and triggered metrics run on this enriched stream.


    - Transformations and queries are a continuum

        - The line between transformations and queries is blurry in batch processing, but the 
            differences become even more subtle in the streaming domain.

        - For instance, if we dynamically compute roll-up statistics on windows, and then send the
            output to a target stream, is this a transformation or a query?


    - Streaming DAGs

        - Though orchestration is thought of as a batch processing topic, what if we could create a
            DAG to enrich, merge, and split multiple streams in real time?

        - Suppose we wanted to combine website clickstream data with IoT data.  This will allow us to
            get a unified view of user activity.  Furthermore, each data stream needs to be preprocessed
            into a standard format.

        - This has long been possible by combining a streaming store (ie Kafka) with a stream processor
            (ie Flink).  Creating the DAG amounted to building a Rube Goldberg machine, with numerous
            topics and processing jobs connected.

        - Pulsar dramatically simplifies this process by treaing DAGs as a core streaming abstraction.
            Rather than managing flows across several systems, DEs can define their DAGs as code inside
            a single system.


    - Micro-Batch vs True Streaming

        - Microbatching is a way to take a batch-oriented framework and apply it in a streaming situation.
            A micro-batch might run anywhere from once a minute to once a second.

        - Some microbatch frameworks (ie Spark Streaming) are designed for this use case, and will 
            perform well at high frequencies.

        - True streaming systems (ie Beam and Flink) are designed to process one event at a time.
            However, this has significant overhead.

        - If you're just using windows and triggers, that is the window frequency?  If you are collecting
            sales metrics and updating them once every few minutes, micro-batches are probably fine.

        - If you are computing metrics every second to detect DDoS attacks, true streaming is likely
            the better choice.