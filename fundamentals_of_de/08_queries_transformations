-----------------------------------------------------------------------------
| CHAPTER 8 - QUERIES, MODELING, & TRANSFORMATION                           |
-----------------------------------------------------------------------------

- Queries

    - A 'query' allows you to retrieve and act on data.  It gives you CRUD capabilities.


    - To create the database objects, we use DDL.

        CREATE DATABASE bar;
        CREATE TABLE bar_table;
        DROP TABLE bar_table;


    - To add and alter data within these database objects, we use DML.

        SELECT
        INSERT
        UPDATE
        DELETE
        COPY
        MERGE


    - To limit access to database objects, we use DCL (Data Control Language).

        GRANT
        DENY
        REVOKE


    - To control the details of transactions, we use TCL (Transaction Control Language).

        COMMIT
        ROLLBACK



- The Life of a Query

    - When we execute a SQL query, this is a summary of what happens:


        SQL Query  ->  Parsing and    ->  Query Planning    ->  Query      ->  Results
        Issued         Conversion to      and Optimization      Execution      Returned
                       Byte code


        1. The DB engine compiles the SQL, parses the code to check for proper semantics, and ensures
             the DB objects referenced exist and the current user has access to them.

        2. The SQL code is converted into byte code.  This bytecode expresses the steps that must be 
             executed on the database engine in an efficient, machine-readable format.

        3. The database’s query optimizer analyzes the bytecode to determine how to execute the query,
             reordering and refactoring steps to use available resources as efficiently as possible.

        4. The query is executed, and results are produced.


    - A query optimizer's job is to optimize query performance by performing steps in an efficient
        order.  It will assess joins, indexes, data scan size, and other factors.



- Improving Query Performance - Optimizing JOIN Strategy and Schema

    - A single table is rarely useful on it's own.  We create value by combining it with other datasets.
        Joins are the most common way of combining datasets and creating new ones.


    - A common technique for improving query performance is to pre-join data.  If analytics queries 
        are joining the same data repeatedly, it makes sense to join the data in advance and have
        queries read from the pre-joined data.

      This may mean relaxing normalization to widen the tables.  We could also create materialized
        views with the joined data and have users query the views.


    - We should also consider the complexity of our join conditions.  We can improve complex joins
        in a few ways.

        1. Many row-oriented DBs allow you to index a result computed from a row.  For instance,
             Postgres allows you to create an index on a string field converted to lowercase.
             When the query optimizer sees a 'lower()' in the query, it will use the indes.

        2. We can use CTEs (Common Table Expressions) instead of nested subqueries or temporary
             tables.  This increases readability, and also can improve performance by not having
             to create intermediate tables.


    - Row Explosion

        - One obscure but frustrating problem is 'row explosion'.  This occurs when we have a large 
            number of many-to-many matches, either because of repetition in join keys or as a 
            consequence of join logic.

        - Suppose the join key in TableA has the value 'this' repreated 5 times, and the join key in
            TableB contains this same value repeated 10 times.  This leads to a cross-join of each of
            these rows, creating 50 rows in the output.

        - Row explosions can often generate enough rows to consume massive resources or cause the
            query to fail.

        - We should check to see if the query optimizer is able to reorder joins, and if it isn't,
            we should reorder them in our query to alleviate this problem



- Improving Query Performance - Use the Explain Plan

    - The query optimizer's explain plan will show you how it determined the lowest-cost query, the
        DB objects used (tables, indexes, cache, etc.), and the performance statistics at each stage.


    - In addition, we should monitor the performance of our queries.  Things to watch include:

        1. Usage of disk, memory, network
        2. Data loading time vs processing time
        3. Query execution time, number of records, size of data scanned, size of data shuffled
        4. Competing queries that might cause resource contention
        5. Number of concurrent connections and connections available



- Improving Query Performance - Avoid Full Table Scans

    - You should only query the data you need.  Avoid 'SELECT *' queries with no predicates.


    - Whenever possible, use 'pruning' to reduce the quantity of data scanned in a query.  This 
        different strategies in row- and column-oriented DBs.


    - In a column-oriented DB, only select the columns you need.  Many column-oriented OLAP
        databases provide tools for optimiation, for example Snowflake and BigQuery let you
        define a cluster key, which orders the table in a desired way.


    - In a row-oriented DB, pruning centers around table indexes.  Create indexes that will improve
        the performance of your most important queries, but don't create so many that you degrade
        performance.



- Improve Query Performance - Know How Your Database Handles Commits

    - A database 'commit' is a change within a database, such as creating, updating, or deleting
        a record, table, or other database objects.  Many DBs support 'transactions', which combine
        several operations in a way that maintains a consistent state.


    - The purpose of a transaction is to keep the database in a consistent state both when it's active
        and in the case of a failure.  Also, transactions handle isolation when concurrent events
        are happening to the same DB objects.


    - Example - Postgres

        - Relational DB that uses ACID transactions
        - Each transaction is a package of operations that will succeed or fail as a group
        - Analytics queries will always give us a consistent picture at a point in time
        - Requires row locking
        - Not optimized for scans over large amounts of data


    - Example - BigQuery

        - Utilizes point-in-time full table commit model
        - When read query is issued, reads from latest committed snapshot of the table
        - Does not lock table during reads
        - Subsequent writes create new commits and snapshots
        - To provent inconsistent state, only allows a single writer at a time
        - Writers from multiple clients are queued in order of arrival
        - This commit model is similar to Snowflake, Spark, others


    - Example - MongoDB

        - Variable consistency DB, configuration options for DB and per table
        - Extraordinary scalability and write concurrency in certain modes
        - DB will discard writes if it gets overwhelmed with traffic
        - This is suitable for IoT readings where data loss is fine, but not if we need exact data



- Improve Query Performance - Vacuum Dead Records

    - Transactions incur the overhead of creating new records while retaining old records as pointers
        the previous state of the database.  Over time, these old records accumulate in the filesystem,
        and we should remove them using a process called 'vacuuming'.


    - You can vacuum a single table, multiple tables, or all tables in a DB.  This frees up space for
        new records, makes query plans more accurate, and makes indexes more performant.



- Improve Query Performance - Leverage Cached Query Results

    - If you have a long-running query, it can save a lot of resources to save the results and return
        them on subsequent reads.  Most OLAP databases provide ways to automatically or manually
        cache query results.


    - Note that materialized views provide another form of query caching.



- Querying Streams - Basic Query Patterns

    - Since streaming data is constantly in flight, it has very different querying patterns.  We
        must adapt query patterns to reflect it's real-time nature.


    - The Fast-Follower Approach

        - Continuous CDC basically sets up an analytics DB as a fast follower to a PROD DB.

        - We need this approach, rather than just querying the OLTP, since we don't want to bog it
            down with analytics queries.

        - We could use an OLTP as the follower, but there are significant advantages to using an OLAP
            system.

        - Both Druid and BigQuery combine a streaming buffer with long-term columnar storage in a setup
            similar to a Lambda Architecture.

        - The CDC has limitations, in that it doesn't fundamentally rethink the batch pattern approach.
            You're still calling SELECT against the current table state, and missing the opportunity
            to manually trigger events off of changes in the stream.


    - The Kappa Architecture

        - The principle idea of the Kappa Architecture is to handle all data like events, and store
            these events in a stream rather than a table.

        - When OLTPs are the source, Kappa stores events from CDC.  Events can also flow from an app
            backend, IoT swarm, or any other system that generates events.

        - Instead of simply treating a streaming storage system as a buffer, Kappa retains events
            in storage during a more extended storage period, and data can be directly queried from
            this storage.  The retention period can be long (months or years).

        - The big idea of Kappa is to treat streaming storage as a real-time transport layer and a
            database for retrieving and querying historical data.  This happens either through direct
            query capabilities of the streaming storage or with the help of external tools.

        - For instance, Kakfa KSQL supports aggregation, statistical calculations, and sessionization.
            If query requirements are more complex, and data needs to be combined with other sources,
            an external tool such as Spark reads a time range from Kafka and computes the query
            results.  The streaming storage system can also feed other applications or a stream 
            processor such as Flink or Beam.



- Querying Streams - Windows, Triggers, Emitted Statistics, and Late-Arriving Data

    - One fundamental limitation of traditional batch queries is that this paradigm generally treats
        the query engine as an external observer.  An actor external to the data causes the query to
        run based on a cron job or someone loading a dashboard.


    - Most streaming systems, on the other hand, support the notion of computations triggered directly
        from the data itself.  They might emit median and mean statistics every time a certain number
        of records are collected in the buffer or output a summary when a user session closes.


    - Windows are an essential feature in streaming queries and processing.  'Windows' are small 
        batches that are processed based on dynamic triggers.  Windows are generated dynamically over
        time in several ways.



- Session Windows

    - A 'session window' groups events that occur close together, and filters out periods of
        inactivity when no events occur.  We might say that a user session is any time interval
        with no inactivity gap of 5 minutes or more.


    - Our batch system collects data by a user ID key, orders events, determines the gaps and session
        boundaries, and calculates statistics for each session. Data engineers often sessionize data 
        retrospectively by applying time conditions to user activity on web and desktop apps.


    - In a streaming session, this process can happen dynamically.  Note that session windows are
        per key.  In this example, each user gets their own set of windows.


    - The system accumulates data per user. If a five-minute gap with no activity occurs, the system 
        closes the window, sends its calculations, and flushes the data. If new events arrive for the 
        use, the system starts a new session window.


    - Session windows may also make a provision for late-arriving data. Allowing data to arrive up
        to five minutes late to account for network conditions and system latency, the system will
        open the window if a late-arriving event indicates activity less than five minutes after the
        last event.


    - Making sessionization dynamic and near real-time fundamentally changes its utility. With 
        retrospective sessionization, we could automate specific actions a day or an hour after a user
        session closed (e.g., a follow-up email with a coupon for a product viewed by the user).

      With dynamic sessionization, the user could get an alert in a mobile app that is immediately useful
        based on their activity in the last 15 minutes.



- Fixed-time Windows

    - A 'fixed time (aka tumbling) window' has fixed time periods that run on a fixed schedule and
        processes all data since the previous window is closed.  


    - For example, we might close a window every 20 seconds and process all data arriving from the
        previous window to give a mean and median statistic. Statistics would be emitted as
        soon as they could be calculated after the window closed.


    - This is similar to traditional batch ETL processing, where we might run a data update job once
        every hour or day.  The streaming systems allows us to generate windows more frequently and
        deliver results with lower latency.  Batch is a special case of streaming!



- Sliding Windows

    - Events in a 'sliding window' (aka 'hopping window') are bucketed into windows of fixed time length.  
        For example, we could generate a new 60-second window every 30 seconds. Just as we did before, 
        we can emit mean and median statistics.


        |----Events every 60s------|
                     |----Events every 60s-------|
                                   |----Events every 60s--------|


        -------------|-------------|-------------|--------------|
            30 s          30 s          30 s            30 s
        |_______________________________________________________|
                              2 mins


    - The sliding can vary.  We might think of the window as sliding continuously but emitting statistics
        only when certain conditions (triggers) are met.


    - Suppose we used a 30-second continuously sliding window but calculated a statistic only when a 
        user clicked a particular banner. This would lead to an extremely high rate of output when many 
        users click the banner, and no calculations during a lull.



- Watermarks

    - Sometimes, data is ingested out of the order from which it originated.  A 'watermark' is a 
        threshold used by a window to determine whether data in a window is within the established time
        interval or whether it's considered late.


    - If data arrives that is new to the window but older than the timestamp of the watermark, it is
        considered late-arriving data.


           Event                      |
                        Event         |   Event
                  Event               |           Event
                                      |
        ------------------------------|--------------------------> Time
                                      |
                                  Watermark



- Querying Streams - Combining Streams with Other Data

    - We often derive value from data by combining it with other data.  With streaming data, multiple
        streams can be combined, or a stream can be combined with batch historical data.


    - Conventional Table Joins

        - Some tables may be fed by streams.  The most basic approach to comining data is to just
            join these 2 tables in a database.


            Stream 1  ->  Table 1  ->  New Table (1 & 2)
            Stream 2  ->  Table 2  ->


    - Enrichment

        - 'Enrichment' means that we join a stream to other data.  Typically, this is done to provide
            enhanced data into another stream.

        - For examle, an online retailer receives an event stream from a partner business containing
            Product and User Ids.  The retailer wishes to enhance these events with product details
            and demographic information on the users.

        - The retailer feeds these events into a serverless function that looks up the product and user 
            in an in-memory DB (ie a cache), adds the additional information, then outputs the events
            to another stream.

            Stream  ---------------------->
                                                Enriched data
            Data in Object Storage   ----->


        - In practice, the enrichment source could be almost anywhere - a table in a cloud DW, a RDBMS,
            an object in file storage, etc.



    - Stream-to-Stream Joining

        - Increasingly, streaming systems support direct stream-to-stream joining.  For instance, an
            online retailer might join its web event data with streaming data from an ad platform.

        - The company can feed both streams into Spark, but a variety of complications arise.  For
            instance, the streams may have different latencies.  Certain events may be significantly
            delayed.

        - Because of this, streaming join architectures rely on streaming buffers.  The buffer
            retention interval is configurable.  A longer retention interval requires more storage and
            other resouces.


            Ad impression (ad_id)  ->  Stream Buffer \
                                                       ->  Join Processor  ->  Site Events w/ 
            Site events (ad_id)    ->  Stream Buffer /                         Ad impression details



- What is a Data Model?

    - Well-constructed data architectures must reflect the goals and business logic of the organization
        that relies on the data.  This is cricial for making data useful for the business.


    - Data modeling (both RDBMS and DW) were almost always used until the early-to-mid 2010s, but
        modeling your data became unfashionable in the early big data era.  Sometimes this was done
        for legitimate performance reasons, and sometimes it just led to lots of redundant, mismatched,
        and wrong data.


    - The growing popularity of data management has led to a recognition that modeling is critical to
        move up the 'Data Science Hierarchy of Needs' pyramid.  We will need new data modeling 
        techniques to embrace the needs of streaming and ML.


    - A 'data model' represents the way data relates to the real world.  It reflects how the data must 
        be structured and standardized to best reflect your organization’s processes, definitions, 
        workflows, and logic. 


    - A good data model captures how communication and work naturally flow within your organization.
        In contrast, a poor data model (or nonexistent one) is haphazard, confusing, and incoherent.


    - For an example, a 'customer' might mean different things to different departments in the company.
        Is a customer someone who has bought something from you in the last 30 days?  In the last
        year?  Carefully defining this can have a massive impact on downstream reports.



- Conceptual, Logical, and Physical Data Models

    - 'Conceptual' models contain logic and rules and describe the system's schema, tables, and 
        fields (names and types).  Visualizing ER diagrams are the a helpful way to create these models.


    - 'Logical' models describe how the conceptual mode will be implemented by adding a lot more
        detail.  For example, we would add information on the types of customer ID, customer names,
        and custom addresses.


    - 'Physical' models define how the logical model will be implemented in a DB system.  We would
        add specific databases, schemas, and tables to our logical model, and add configuration
        details.


    - Successful modeling involves stakeholders from the beginning of the process.  It is a practice
        everyone must continuously participate in.


    - Another important consideration when modeling is the 'grain' of the data, the resolution at
        which data is stored and queried.  The grain is typically at the PK of the data (ie user_id,
        order_id, product_id).

        - For an example of a coarse-grained report, we create a report that includes all customers
            who made orders in a day, the total number of orders they made that day, and the total
            amount they spent.

        - For a more fine-grained report, we could break down each order by the products ordered.
            This data is at the grain of the customer-order level.

        - It would be tempting, if asked for only the coarse-grained report, to just create a
            reporting table with only the data asked for.  However, a more experienced engineer will
            probably create the tables with detailed data on customer orders and aggregate it for
            the simple report, since someone will ask for it later.

        - In general, we should strive to model data at the lowest level of grain possible.  This
            is because it is generally impossible to restore details that have been aggregated away.



- Normalization

    - 'Normalization' is a DB data modeling practive that enforces strict control over the relationship
        of tables and columns within a DB.  The goal is to remove redundancy and ensure referential
        integrity.


    - Normalization, as applied to relational DBs, was introduced by Edgar Codd in the early 1970s.
        He outlined 4 objectives for normalization:

        1. Free the collection of relations from undesirable insert, update, and delete dependencies.

        2. Reduce the need for restructuring the collection as new data types are introduced.

        3. Make the relational model more informitive to users.

        4. Make the collection neutral to query statistics.


    - He introduced 'normal forms'.  They are sequential, with each form incorporating the condition
        of prior forms.

        - Denormalized = No normalization, nested and redundant data is allowed

        - 1NF = Each column is unique and has a single value.  The table has a unique PK.

        - 2NF = Partial dependencies are removed.  Partial dependencies occur when a subset of fields
                  in a composite key can be used to determine a nonkey column of the table.

        - 3NF = Each table contains only data relevant to it's PK, and has no transitive dependencies.
                  Transitive dependencies occur when a nonkey field depends on another nonkey field.



- Normalization Example

    - Denormalized


        [ORDER_DETAILS]  PK = OrderID

        OrderID    OrderItems                    CustomerID    CustomerName     OrderDate
        --------------------------------------------------------------------------------------
        100        [{                            5             Joe Reis         2022-03-01
                      "sku": 1,
                      "price": 50,
                      "quantity": 1,
                      "name:": "Thingamajig"
                    }, 
                    {
                      "sku": 2,
                      "price": 25,
                      "quantity": 2,
                      "name:": "Whatchamacallit"
                    }]



    - 1NF (Remove Multivaled Attribute)


        [ORDER_DETAILS]  PK = OrderId

        OrderID   Sku   Price   Quantity   ProductName       CustomerID   CustomerName   OrderDate
        --------------------------------------------------------------------------------------------
        100       1     50      1          Thingamajig       5            Joe Reis       2022-03-01
        100       2     25      2          Whatchamacallit   5            Joe Reis       2022-03-01



    - 1NF (Unique PK)


    [ORDER_DETAILS]  PK = (OrderID, LineItemNumber)

    OrderID  LineItemNumber Sku  Price  Quantity  ProductName      CustomerID  CustomerName  OrderDate
    ----------------------------------------------------------------------------------------------------
    100      1               1    50     1        Thingamajig      5           Joe Reis      2022-03-01
    100      2               2    25     2        Whatchamacallit  5           Joe Reis      2022-03-01



    - 2NF (Remove Partial Dependencies)


        [ORDERS] PK = OrderId

        OrderID   CustomerID   CustomerName    OrderDate
        ----------------------------------------------------
        100       5            Joe Reis        2022-03-01
        101       7            Matt Housley    2022-03-01
        102       7            Matt Housley    2022-03-01



        [ORDER_LINE_ITEMS]  PK = (OrderID, LineItemNumber)

        OrderID   LineItemNumber   Sku   Price   Quantity   ProductName
        ------------------------------------------------------------------
        100       1                1     50      1          Thingamajig
        100       2                2     25      2          Whatchamacallit
        101       1                3     75      1          Whozeewhatzit
        102       1                1     50      1          Thingamajig



    - 3NF (Remove Transitive Dependencies)


        [ORDERS] PK = OrderId

        OrderID   CustomerID   OrderDate
        ------------------------------------
        100       5            2022-03-01
        101       7            2022-03-01
        102       7            2022-03-01


        [CUSTOMERS]  PK = CustomerID

        CustomerID   CustomerName 
        --------------------------
        5            Joe Reis     
        7            Matt Housley 
        7            Matt Housley


        [ORDER_LINE_ITEMS]  PK = (OrderID, LineItemNumber)

        OrderID   LineItemNumber   Sku   Price   Quantity
        -------------------------------------------------
        100       1                1     50      1       
        100       2                2     25      2       
        101       1                3     75      1       
        102       1                1     50      1       


        [SKUS]  PK = (Sku)

        Sku     ProductName
        ----------------------
        1       Thingamajig
        2       Whatchamacallit
        3       Whozeewhatzit
        1       Thingamajig




- Modeling Batch Analytical Data - Inmon

    - Bill Inmon, father of the DW, created his approach to data modeling in 1990.  Before the DW, 
        the analysis would usually occur directly on the source system.


    - Bill's definition of a DW:

        A DW is a subject-oriented, integrated, nonvolatile, and time-variant collection of data in 
          support of management’s decisions. The data warehouse contains granular corporate data. Data 
          in the data warehouse is able to be used for many different purposes, including sitting and 
          waiting for future requirements which are unknown today.

        1. Subject-Oriented = Focuses on specific area such as sales or marketing
        2. Nonvolatile = Data is unchanged after it is stored in a DW
        3. Integrated = Data from disparate sources is consolidated and normalized
        4. Time-Variant = Varying time ranges can be queried


    - Data should be modeled in support of management's decisions.  This is a break from the OLTP
        approach.  However, the DW still uses 3NF normalization.


    - Example - Ecommerce


       Orders OLTP     ->            DW             -> Sales Data Mart
       Inventory OLTP  ->  ELT  ->  (3NF)  --  ETL  -> Marketing Data Mart
       Marketing OLTP  ->                           -> Purchasing Data Mart


    - In this example, Sales, Marketing, and Purchasing each have their own star schema.  This allows
        each department to have it's own data structure that's unique and optimized to its specific
        needs.



- Modeling Batch Analytical Data - Kimball

    - Ralph Kimball's approach, created in the early 1990s, takes the opposite approach.  It focuses
        less on normalization, and sometimes allows denormalization.


    - The Inmon model integrates data from across the business in the DW, and serves department-specific
        analytics via data marts.  The Kimball model is bottom-up, encouraging you to model and serve
        department or business analytics in the data warehouse itself.

      This enables faster iteration and modeling, but can lead to looser data integration, redundancy,
        and duplication.


    - Data is modeled with 2 general types of tables: facts and dimensions.  A 'fact table' can be
        thought of as a table of numbers, and 'dimension tables' are qualitative data referencing a
        fact.


    - Dimension tables surround a single fact table in a relationship called a 'star schema'.

         Dimension1  \            / Dimension3
                       Fact Table
         Dimension2  /            \ Dimension4



- Fact Tables

    - Fact bales contain factual, quantitative, and event-related data.


    - The data in a fact table is immutable, since facts relate to events.  They are append-only.


    - Fact tables are typically narrow and long, and should be at the lowest grain possible.


    - Queries against a star schema start with the fact table.

        
    - Each row of the table should represent the grain of the data.  Avoid aggregating or deriving
        data within a fact table.  Perform aggregations or derivations in a downstream query,
        a data mart table, or a view.


    - Fact tables don't reference other fact tables, only dimension tables.


    - Here is a fact table for sales events:

        OrderID   CustomerKey   DateKey    GrossSalesAmt
        ----------------------------------------------------
        100       5             20220301   100.00
        101       7             20220301   75.00
        102       7             20220301   50.00



- Dimension Tables

    - Dimension tables provide the reference data, attributes, and relational context for the 
        events stored in the fact tables.


    - Dimension tables are smaller, and are typically wide and short.


    - When joined to a fact table, dimensions describe the what/where/why of the events.


    - Dimensions are denormalized, with the possibility of duplicated data.

        
    - Dates are typically stored in a date dimention, allowing you to reference a DateKey between
        the fact and date dimension table.  This allows you to quickly answer questions like 
        "How many customers shop on Tuesday vs Wednesday?"

        DateKey   Date-ISO    Year   Quarter   Month   Day-of-week
        -----------------------------------------------------------
        20220301  2022-03-01  2022   1         3       Tuesday
        20220302  2022-03-02  2022   1         3       Wednesday
        20220303  2022-03-03  2022   1         3       Thursday


    - Here, we look at the customer dimension table.  In addition to the fields you expect, there are
        several date fields, which are 'Type 2 slowly changing dimensions'.  They define how long the
        customer has been in the system.

        CustomerKey   FirstName   LastName   ZipCode   EFF_StartDate   EFF_EndDate
        ----------------------------------------------------------------------------
        5             Joe         Reis       84108     2019-01-04      9999-01-01
        7             Matt        Housley    84101     2020-05-04      2021-09-19
        7             Matt        Housley    84123     2021-09-19      9999-01-01
        11            Lana        Belle      90210     2022-02-04      9999-01-01


    - We can see that a new record was created when the customer updated their zip code.  This
        'SCD (Slowly Changing Dimension)' is necessary to track changes in dimensions.  This is a
        type-2 SCD.


    - There are 7 levels of SCDs, but we'll look at the most common ones here:

        Type 1 = Overwrite existing dimension records.  This is simple, but it means you have no access
                   to historical dimension records.

        Type 2 = Keep a full history of dimension records.  When a record changes, that specific record
                   is flagged as changed, and a new dimension record is created that reflects the
                   current status of the attributes.  This is done with the 'EFF_StartDate' and
                   'EFF_EndDate' fields in our example

        Type 3 = Similar to Type 2, but a new field is added to the existing record, instead of
                   creating a new record.


    - Here is an example of a Type 3 SCD:

        CustomerKey   FirstName   LastName   Original ZipCode   Current ZipCode   CurrentDate
        ---------------------------------------------------------------------------------------
        7             Matt        Housley    84101              84123             2021-09-19


    - Type 1 is the default in most DWs, and Type 2 is the one most commonly used in practice.



- Star Schema

    - The 'star schema' represents the data model of the business.  Unlike highly normalized approaches,
        the star schema is a fact table surrounded by the necessary dimensions.  This results in fewer
        joins, which speeds up queries, and is easier for business users to understand.


    - The star schema doesn't reflect a particular report, but a report can be modeled in a downstream
        or directly in a BI tool.  The star schema should reflect your business logic, and be
        flexible enough to answer critical questions.


    - Since a star schema only has one fact table, sometimes you have multiple star schemas for different
        events in the business.  Sometimes, dimensions can be used across multiple fact tables, these
        are called 'conformed dimensions'.

      Redundant data is OK, but avoid repeating the same dimension tables to avoid data integrity
        problems.
