-----------------------------------------------------------------------------
| CHAPTER 6 - STORAGE                                                       |
-----------------------------------------------------------------------------

- Storage

    - Storage systems underpin all the major stages (ingegestion, transformation, and serving) of
        the DE lifecycle.


    - Storage Abstractions (Abstractions of storage systems)

        - Data Lake
        - Data Lakehouse
        - Data Platform
        - Cloud Data Warehouse


    - Storage Systems (Abstractions of raw ingredients)

        - RDBMS
        - HDFS
        - Redis
        - Object Storage
        - Kafka


    - Raw Ingredients

        - HDD
        - SDD
        - RAM
        - Networking
        - Serialization
        - Compression
        - CPU



- Magnetic Disk Drives

    - Developed by IBM in the 1950s.  The first commercial magnetic drive, the IBM 350, had a capacity
        of 3.75 MB.


    - Magnetic disks utilize spinning platters coated with a ferromagnetic film. This film is
        magnetized by a read/write head during write operations to physically encode binary data. 

      The read/write head detects the magnetic field and outputs a bitstream during read operations.


    - Magnetic disk drives (HDDs) still form the backbone of bulk data storage systems because they are 
        significantly cheaper than SSDs.  However, SSDs dramatically outperform them.


    - Currently, HDD drives cost ~ $0.03 per GB of storage.  Drives as large as 20 TB are commercially
        available.


    - HDDs have seen a lot of innovations and improvements, but are constrained by physical limits:

        1. 'Disk transfer speed', the rate at which data can be read and written, grows linearly while
             capacity grows areally (GB per in**2).  Current data center drives support maximum
             transfer speeds of 300 MB/s.

        2. 'Seek time' is limited by the ability of the drive to physically relocate the read/write
             heads to the appropriate track on the disk.

        3. 'Rotational latency' is how long the disk must wait for the desired sector to rotate under
             read/write heads.  Commercial drives typically spin at 7200 rpm.

        4. IOPs is another limitation, crucial for transactional databases.  A magnetic drive supports
             50-100 IOPs.


    - Magnetic disks can sustain extremely high data rates through parallelism.  For instance, object
        storage distributes data across thousands of drives in clusters.

      In this case, data transfer rates are limited by network performance rather than disk transfer
        rate.  This makes network components and CPUs crucial also.



- Solid-State Drives

    - SSDs store data as charges in flash memory cells, which eliminates the mechanical components of
        magnetic drives.

        - Look up random data in less than 0.1 ms

        - Commercial drives have ransfer rates of many GB/s and tens of thousands of IOPs


    - SSDs have revolutionized transactional databases, and are now standard for commercial OLTP 
        systems.  Allow RDBMS's to handle thousands of transactions per second.


    - For cost reasons, magenetic drives are still the standard for high-scale analytics data storage.
        SSDs can be used to cache frequently accessed data.



- RAM

    - RAM, also just called 'memory' has specific characteristics:

        - Attached to a CPU and mapped into the CPU address space

        - Stores the code the CPUs execute and the data it directly processes

        - Volatile

        - DDR5 (the latest standard) offers data retrieval latency on the order of 100 ns (roughly
            1,000x faster than SSDs)

        - A typical CPU can support 100 GB/s bandwidth to attached memory and millions of IOPs

        - Is much more expensive than SSDs (~ $10/GB)

        - Is limited in the amount of RAM attached to an individual CPU and memory controller.
            High-memory servers typically utilize many interconnected CPUs on one board, each with a 
            block of attached RAM.

        - Significantly slower than CPU cache


    - When we talk about system memory, we mean DRAM (Dynamic RAM), a high-density and low-cost form
        of memory.  DRAM stores data as charges in capacitors.  These capacitors leak over time, so
        data must be frequently refreshed to avoid data loss.

      Other forms of memory, such as static RAM, are used in specialized applications such as CPU
        caches.


    - Current CPUs use von Neumann architecture, with code and data stored together in the same memory
        space.


    - RAM is used by data systems for caching, data processing, and indexing.  When using it, we must
        always remember it is volatile and a power outage could lead to data loss.



- Networking and CPU

    - In distributed systems, network performance is often the bottleneck.  While storage standards such
        as RAID parallelize on a single server, cloud object storage clusters operate both within and
        across multiple data centers.


    - CPUs handle the details of servicing requests, aggregating reads, and distributing writes.
        Storage becomes a web app with an API, backend service components, and load balancing.



- Serialization

    - 'Serialization' is the process of flattening and packing data into a standard format the a reader
        will be able to decode.  It is a critical element in database design, and affects network
        performance, CPU overhead, query latency, and more.


    - Row-based formats such as XML, JSON, and CSV are the most popular formats.


    - A serialization algorithm has logic for handling types, imposes rules on data structure, and
        checks for cyclic references.



- Compression

    - Highly efficient compression has 3 main advantages in storage systems.

        1. The data is smaller and takes up less space on disk

        2. Increases the practical scan speed per disk

        3. Network performance


    - Compression and decompression data have costs, though.  Extra time and resources are needed to 
        read or write data.



- Caching

    - The core idea of caching is to store frequently or recently accessed data in a fast access layer. 

      The faster the cache, the higher the cost and the less storage space available. Less frequently 
        accessed data is stored in cheaper, slower storage.


    - Here is a cache hierarchy (note that a microsecond is 1,000 nanoseconds, and a millisecond is 
        1,000 microseconds):

        Storage type     Data fetch latency     Bandwidth                     Price
        ----------------------------------------------------------------------------------
        CPU cache        1 nanosecond           1 TB/s
        
        RAM              0.1 microseconds       100 GB/s                      $10/GB

        SSD              0.1 milliseconds       4 GB/s                        $0.20/GB

        HDD              4 milliseconds         300 MB/s                      $0.03/GB
        
        Object storage   100 milliseconds       3 GB/s per instance           $0.02/GB per month

        Archival storage 12 hours               Same as object storage once   $0.004/GB per month
                                                  data is available



- Single Machine vs Distributed Storage

    - When data is distributed across multiple servers, it is known as 'distributed storage'.
        Distributed storage coordinates the activities of multiple servers.


    - Distributed storage is used for both redundancy and scalability.


    - Consistency issues arise when distributed storage is used.



- Eventual vs Strong Consistency

    - Since it takes time to replicate changes across the nodes of a system, a balance exists between
        getting current data and getting 'sort of current' data in a distributed database.


    - The BASE model is defined as:

        Basically Available = Consistent data is available most of the time

        Soft State = The state of a transaction is fuzzy, it is uncertain whether committed or not

        Eventual Consistency = At some point, reading will return consistent values


    - DEs often make consistency decisions in 3 places:

        1. Choosing the DB technology
        2. Configuration parameters for the DB
        3. On a per-query basis



- File Storage

    - A 'file' is a data entity with specific read, write, and reference characteristics used by 
        software and operating systems. It has these characteristics:

        - Finite length = a file is a finite-length stream of bytes

        - Append operations = can append bytes to the file up to the limits of the host storage system

        - Random access = can read from or write to any location in the file


    - File storage systems organize files into a directory tree.  To avoid hard-coding file paths,
        it is usually better to use object storage as an intermediary.

      Manual, low-level file handling processes are best left to one-time ingestion steps or 
        exploratory stages of the pipeline.


    - The most familiar types of file storage are OS-managed filesystems on a local partition of an
        SSD or magnetic disk.  NTFS (Windows) and ext4 (Linux) are the most popular.  The OS handles
        the details of storing directory entities, files, and metadata.

      Local filesystems usually have read/write consistency, and OS's employ locking strategies to
        management concurrent attempts to write a file.


    - 'Network-Attached Storage (NAS)' systems provide a file storage system to clients over a network.
        Servers often ship with built-in NAS-dedicated hardware.

      While there are performance penalties to accessing a filesystem over a network, the advantages
        include redundancy, reliability, storage pooling, and file sharing.


    - Cloud filesystem services provide a fully-managed filesystem to use with multiple VMs and
        applications.  This is different from standard storage attached to VMs (block storage managed
        by the VM's OS).  They behave mostly like a NAS.  AWS EFS is a popular example.



- Block Storage

    - Fundamentally, 'block storage' is the type of raw storage provided by HDDs and SSDs.  A 'block'
        is the smallest addressable unit of data supported by a disk (512 B on older disks, 4096 B
        on modern disks).  Blocks typically contain extra bits for error detection/correction and
        other metadata.


    - In the cloud, virtualized block storage is the standard for VMs.  These block storage abstractions
        allow fine control of storage size, scalability, and data durability beyond that offered by raw 
        disks.


    - Transactional database systems usually access disks at a block level to lay out data for optimal
        performance.


    - Block storage also remains the default option for OS boot disks on cloud VMs.


    - 'RAID (Redudant Array of Independent Disks)' simultaneously controls multiple disks to improve
        data durability, enhance performance, and combine capacity from multiple drives.  An array
        can appear to an OS as a single block device.  Many different schemes are available to fine-tune
        the bandwidth vs fault tolerance balance.


    - 'SANs (Storage Area Networks)' provide virtualized block storage devices over a network, typically
        from a storage pool.  They can allow for fine-grained scaling, performance, availability, and
        durability.  You may encounter them either on-premises or in the cloud.



- Cloud Virtualized Block Storage

    - 'Cloud Virtualized Block Storage' solutions are similar to SANs, but free engineers from dealing
        with SAN clusters and networking details.

      
    - AWS EBS is a popular implementation.  It is the default storage for EC2 VMs.


    - EBS volumes store data separate from the instance host server but in the same zone to support
        high performance and low latency.  This allows EBS volumes to persist after an EC2 instance
        is shut down, a host fails, or even when the instance is deleted.

      
    - It is also suitable for applications like databases where durability is a high priority.  Also,
        EBS replicates all data to at least 2 host machines.


    - EBS storage virtualization also suports some advanced features:

        - Can take point-in-time snapshots while the drive is in use and write them to S3
        - Snapshots after full backup are diffs
        - EBS volumes can scale to 64 TB, 250K IOPs



- Local Instance Volumes

    - Cloud providers also offer block storage volumes that are physically attached to the host server
        running a VM.  These storage volumes are generally very low cost (included with the cost of a
        VM in EC2).


    - Instance store volumes behave esentially like a disk physically attached to a server.  One key
        difference, however, is that when a VM shuts down or is deleted, the contents of locally
        attached storage are lost.  This ensures a new VM cannot read disk contents belonging to 
        another customer.


    - Locally attached disks don't support advanced features, like replication, snapshots, or other
        backup features.


    - We can still use them for lots of things, like local caches that don't need features of a service
        like EBS.  For instance, if we are running EMR on EC2 instances, we just need a job that
        consumes data from S3, stores it temporarily in the distributed filesystem across the
        instances, processes it, then writes it back to S3.



- Object Storage

    - Object storage contains 'objects' of all shapes and sizes.  In this case, an object is any type
        of file - TXT, CSV, JSON, images, audio, or videos.


    - AWS S3, Azure Blob Storage, and Google Cloud Storage are widely used object stores.  Also, many
        cloud data warehouses and data lakes sit on object stores.


    - Object storage is easy to manage and use.  It was one of the first 'serverless' services, which
        frees engineers from considering server clusters or disks.


    - An object store is a key-value store for immutable data objects.  Objects don't support random
        writes or appends.  They are written once as a stream of bytes.  To change data, we rewrite
        the entire object.  Random reads are supported through range requests, but they perform much
        worse than data stored on an SSD.


    - Object stores donâ€™t need to support locks or change synchronization, allowing data storage across 
        massive disk clusters.  Object stores support extremely performant parallel stream writes and 
        reads across many disks, and this parallelism is hidden from engineers, who can simply deal with 
        the stream rather than communicating with individual disks.


    - Typical cloud object stores save data in several availability zones, dramatically reducing the 
        odds that storage will go fully offline or be lost in an unrecoverable way. This durability and
        availability are built into the cost.



- Object Stores for DE Applications

    - Object stores provide excellent performance for large batch reads and writes.  This corresponds
        well to the use case for OLAP systems.


    - Object stores are a bad fit for transactional systems or any system with lots of updates per
        second.  Block storage is a much better fit.  Object stores work well for a low rate of updates,
        with each update on a large volume of data.


    - Object stores are now the gold standard for data lakes.  In the early days of data lakes, the
        WORM (Write Once, Read Many) pattern was the operational standard.

      Since then, systems such as Apache Hudi and Delta Lake have emerged to manage the complexity of
        updates, and regulations like GDPR/CCPA have made deletes and updates imperative.  Updates to
        object storage are also the idea behind data lakehouses.



- Object Lookup

    - Unlike file stores, there is no directory tree, only a bucket and key:

        s3://oreilly-data-engineering-book/data-example.json

        Bucket:   s3://oreilly-data-engineering-book/
        Key:      data-example.json


    - Note, you can have keys that look like a directory, but there is no actual directory structure.

        s3://oreilly-data-engineering-book/project-data/11/23/2021/data.txt


    - Many object stores allow directory-like commands such as 'ls', but we need to understand we are
        scanning the entire bucket when we do this, and it could lead to performance issues.



- Object Consistency and Versioning

    - The only way to change an object is to replace it completely.  Object stores may be eventually
        or strongly consistent.


    - Until recently, S3 was eventually consistent.  After a new version of an object was written
        under the same key, the object store might still sometimes return the old object.


    - To impose strong consistency on the object store, we can use a strongly consistent database
        (ie Postgres).  To read an object, a reader will:

        1. Fetch the latest object metadata from Postgres.

        2. Query the object metadata using the object key.  Read the data if it matches the metadata
             from Postgres.

        3. If the metadata doesn't match, repeat step 2 until it does.


    - When we turn on 'object versioning', we add additional metadata to the object that stipulates
        a version.  In this case, the key and version metadata together point to a specific version
        of an object, so we'll always get that exact version back.


    - If we do turn on object versioning, we can end up with many versions of the same object, which
        may be expensive if the objects are large.  We can implement some kind of 'lifecycle policy'
        to delete old objects if newer versions are available.



- Cache and Memory-Based Storage Systems

    - RAM offers excellent latency and transfer speeds, but it is volatile.  RAM-based storage systems
        are generally focused on caching, and data must generally be written to a more durable medium
        for retention.


    - memcached is a key/value store designed for caching DB query results, API call responses, etc.
        It supports only string and integer types.  It can deliver results with very low latency.


    - Redis is also a key/value store, but it supports more complex data types (ie lists or sets).
        It also builds in multiple persistence mechanisms, including snapshotting and journaling.

      With a typical configuration, Redis writes data once every 2 seconds.  It is suitable for 
        extremely high performance applications that can tolerate a small amount of data loss.



- HDFS

    - HDFS is based on the 'GFS (Google File System)' and was initially engineered to process data
        with the MapReduce programming model.  Hadoop is similar to object storage, except that
        Hadoop combines compute and storage on the same node.


    - Hadoop breaks large files into 'blocks' (chunks of data a few hundred MB in size).  The filesystem
        is managed by the 'NameNode', which maintains directories, file metadata, and a detailed 
        catalog of all blocks.


    - In a typical Hadoop cluster, the replication factor is set to 3.


    - Hadoop is often claimed to be 'dead'.  It is not a bleeding-edge tool any more, and tools like
        Pig are relegated to legacy jobs.  MapReduce has fallen by the wayside.

      However, Hadoop still appears in many legacy installations, and many companies with massive
        clusters have no plans to migrate to newer technologies.


    - HDFS is still a key ingredient in many big data engines, including AWS EMR.  Spark is also still
        commonly run on HDFS clusters.



- Streaming Storage

    - Streaming data has different requirements from non-streaming data.  Traditional message queues
        delete data after a certain duration, but distributed frameworks like Kafka allow long-duration
        retention.  Competitors Kinesis, Pulsar, and GCP Pub/Sub also support long retention.


    - The notion of 'replay' is closely related to retention.  It allows a streaming system to return
        a range of historical stored data.



- Indexes, Partitioning, and Clustering

    - Indexes provide a map of the table for particular fields and allow extremely fast lookup of 
        individual records. Without indexes, a database would need to scan an entire table to find the 
        records satisfying a WHERE condition.


    - In most RDBMS's, indexes are used for PKs, FKs, and other columns used to serve the needs of
        specific applications.  Using indexes, an RDBMS can read and write thousands of rows per
        second.


    - In the early days of MPP systems, they were still row-oriented and used indexes to support
        joins and condition checking.


    - Columnar indexes can be used to speed up analytics queries.  In the past, we denormalized data
        and avoid joins in columnar databases, but join performance has been drastically improved.


    - Since we still want to minimize the data we have to read with columnar databases, we can
        partition a table into multiple subtables by splitting it on a field.  Date and time-based
        partitioning are extremely common.

      'Clusters' allow finer-grained organization of data within partitions.
