-----------------------------------------------------------------------------
| CHAPTER 6 - STORAGE                                                       |
-----------------------------------------------------------------------------

- Storage

    - Storage systems underpin all the major stages (ingegestion, transformation, and serving) of
        the DE lifecycle.


    - Storage Abstractions (Abstractions of storage systems)

        - Data Lake
        - Data Lakehouse
        - Data Platform
        - Cloud Data Warehouse


    - Storage Systems (Abstractions of raw ingredients)

        - RDBMS
        - HDFS
        - Redis
        - Object Storage
        - Kafka


    - Raw Ingredients

        - HDD
        - SDD
        - RAM
        - Networking
        - Serialization
        - Compression
        - CPU



- Magnetic Disk Drives

    - Developed by IBM in the 1950s.  The first commercial magnetic drive, the IBM 350, had a capacity
        of 3.75 MB.


    - Magnetic disks utilize spinning platters coated with a ferromagnetic film. This film is
        magnetized by a read/write head during write operations to physically encode binary data. 

      The read/write head detects the magnetic field and outputs a bitstream during read operations.


    - Magnetic disk drives (HDDs) still form the backbone of bulk data storage systems because they are 
        significantly cheaper than SSDs.  However, SSDs dramatically outperform them.


    - Currently, HDD drives cost ~ $0.03 per GB of storage.  Drives as large as 20 TB are commercially
        available.


    - HDDs have seen a lot of innovations and improvements, but are constrained by physical limits:

        1. 'Disk transfer speed', the rate at which data can be read and written, grows linearly while
             capacity grows areally (GB per in**2).  Current data center drives support maximum
             transfer speeds of 300 MB/s.

        2. 'Seek time' is limited by the ability of the drive to physically relocate the read/write
             heads to the appropriate track on the disk.

        3. 'Rotational latency' is how long the disk must wait for the desired sector to rotate under
             read/write heads.  Commercial drives typically spin at 7200 rpm.

        4. IOPs is another limitation, crucial for transactional databases.  A magnetic drive supports
             50-100 IOPs.


    - Magnetic disks can sustain extremely high data rates through parallelism.  For instance, object
        storage distributes data across thousands of drives in clusters.

      In this case, data transfer rates are limited by network performance rather than disk transfer
        rate.  This makes network components and CPUs crucial also.



- Solid-State Drives

    - SSDs store data as charges in flash memory cells, which eliminates the mechanical components of
        magnetic drives.

        - Look up random data in less than 0.1 ms

        - Commercial drives have ransfer rates of many GB/s and tens of thousands of IOPs


    - SSDs have revolutionized transactional databases, and are now standard for commercial OLTP 
        systems.  Allow RDBMS's to handle thousands of transactions per second.


    - For cost reasons, magenetic drives are still the standard for high-scale analytics data storage.
        SSDs can be used to cache frequently accessed data.



- RAM

    - RAM, also just called 'memory' has specific characteristics:

        - Attached to a CPU and mapped into the CPU address space

        - Stores the code the CPUs execute and the data it directly processes

        - Volatile

        - DDR5 (the latest standard) offers data retrieval latency on the order of 100 ns (roughly
            1,000x faster than SSDs)

        - A typical CPU can support 100 GB/s bandwidth to attached memory and millions of IOPs

        - Is much more expensive than SSDs (~ $10/GB)

        - Is limited in the amount of RAM attached to an individual CPU and memory controller.
            High-memory servers typically utilize many interconnected CPUs on one board, each with a 
            block of attached RAM.

        - Significantly slower than CPU cache


    - When we talk about system memory, we mean DRAM (Dynamic RAM), a high-density and low-cost form
        of memory.  DRAM stores data as charges in capacitors.  These capacitors leak over time, so
        data must be frequently refreshed to avoid data loss.

      Other forms of memory, such as static RAM, are used in specialized applications such as CPU
        caches.


    - Current CPUs use von Neumann architecture, with code and data stored together in the same memory
        space.


    - RAM is used by data systems for caching, data processing, and indexing.  When using it, we must
        always remember it is volatile and a power outage could lead to data loss.



- Networking and CPU

    - In distributed systems, network performance is often the bottleneck.  While storage standards such
        as RAID parallelize on a single server, cloud object storage clusters operate both within and
        across multiple data centers.


    - CPUs handle the details of servicing requests, aggregating reads, and distributing writes.
        Storage becomes a web app with an API, backend service components, and load balancing.



- Serialization

    - 'Serialization' is the process of flattening and packing data into a standard format the a reader
        will be able to decode.  It is a critical element in database design, and affects network
        performance, CPU overhead, query latency, and more.


    - Row-based formats such as XML, JSON, and CSV are the most popular formats.


    - A serialization algorithm has logic for handling types, imposes rules on data structure, and
        checks for cyclic references.



- Compression

    - Highly efficient compression has 3 main advantages in storage systems.

        1. The data is smaller and takes up less space on disk

        2. Increases the practical scan speed per disk

        3. Network performance


    - Compression and decompression data have costs, though.  Extra time and resources are needed to 
        read or write data.



- Caching

    - The core idea of caching is to store frequently or recently accessed data in a fast access layer. 

      The faster the cache, the higher the cost and the less storage space available. Less frequently 
        accessed data is stored in cheaper, slower storage.


    - Here is a cache hierarchy (note that a microsecond is 1,000 nanoseconds, and a millisecond is 
        1,000 microseconds):

        Storage type     Data fetch latency     Bandwidth                     Price
        ----------------------------------------------------------------------------------
        CPU cache        1 nanosecond           1 TB/s
        
        RAM              0.1 microseconds       100 GB/s                      $10/GB

        SSD              0.1 milliseconds       4 GB/s                        $0.20/GB

        HDD              4 milliseconds         300 MB/s                      $0.03/GB
        
        Object storage   100 milliseconds       3 GB/s per instance           $0.02/GB per month

        Archival storage 12 hours               Same as object storage once   $0.004/GB per month
                                                  data is available



- Single Machine vs Distributed Storage

    - When data is distributed across multiple servers, it is known as 'distributed storage'.
        Distributed storage coordinates the activities of multiple servers.


    - Distributed storage is used for both redundancy and scalability.


    - Consistency issues arise when distributed storage is used.



- Eventual vs Strong Consistency

    - Since it takes time to replicate changes across the nodes of a system, a balance exists between
        getting current data and getting 'sort of current' data in a distributed database.


    - The BASE model is defined as:

        Basically Available = Consistent data is available most of the time

        Soft State = The state of a transaction is fuzzy, it is uncertain whether committed or not

        Eventual Consistency = At some point, reading will return consistent values


    - DEs often make consistency decisions in 3 places:

        1. Choosing the DB technology
        2. Configuration parameters for the DB
        3. On a per-query basis



- File Storage

    - A 'file' is a data entity with specific read, write, and reference characteristics used by 
        software and operating systems. It has these characteristics:

        - Finite length = a file is a finite-length stream of bytes

        - Append operations = can append bytes to the file up to the limits of the host storage system

        - Random access = can read from or write to any location in the file


    - File storage systems organize files into a directory tree.  To avoid hard-coding file paths,
        it is usually better to use object storage as an intermediary.

      Manual, low-level file handling processes are best left to one-time ingestion steps or 
        exploratory stages of the pipeline.


    - The most familiar types of file storage are OS-managed filesystems on a local partition of an
        SSD or magnetic disk.  NTFS (Windows) and ext4 (Linux) are the most popular.  The OS handles
        the details of storing directory entities, files, and metadata.

      Local filesystems usually have read/write consistency, and OS's employ locking strategies to
        management concurrent attempts to write a file.


    - 'Network-Attached Storage (NAS)' systems provide a file storage system to clients over a network.
        Servers often ship with built-in NAS-dedicated hardware.

      While there are performance penalties to accessing a filesystem over a network, the advantages
        include redundancy, reliability, storage pooling, and file sharing.


    - Cloud filesystem services provide a fully-managed filesystem to use with multiple VMs and
        applications.  This is different from standard storage attached to VMs (block storage managed
        by the VM's OS).  They behave mostly like a NAS.  AWS EFS is a popular example.



- Block Storage

    - Fundamentally, 'block storage' is the type of raw storage provided by HDDs and SSDs.  A 'block'
        is the smallest addressable unit of data supported by a disk (512 B on older disks, 4096 B
        on modern disks).  Blocks typically contain extra bits for error detection/correction and
        other metadata.


    - In the cloud, virtualized block storage is the standard for VMs.  These block storage abstractions
        allow fine control of storage size, scalability, and data durability beyond that offered by raw 
        disks.


    - Transactional database systems usually access disks at a block level to lay out data for optimal
        performance.


    - Block storage also remains the default option for OS boot disks on cloud VMs.


    - 'RAID (Redudant Array of Independent Disks)' simultaneously controls multiple disks to improve
        data durability, enhance performance, and combine capacity from multiple drives.  An array
        can appear to an OS as a single block device.  Many different schemes are available to fine-tune
        the bandwidth vs fault tolerance balance.


    - 'SANs (Storage Area Networks)' provide virtualized block storage devices over a network, typically
        from a storage pool.  They can allow for fine-grained scaling, performance, availability, and
        durability.  You may encounter them either on-premises or in the cloud.



- Cloud Virtualized Block Storage

    - 'Cloud Virtualized Block Storage' solutions are similar to SANs, but free engineers from dealing
        with SAN clusters and networking details.

      
    - AWS EBS is a popular implementation.  It is the default storage for EC2 VMs.


    - EBS volumes store data separate from the instance host server but in the same zone to support
        high performance and low latency.  This allows EBS volumes to persist after an EC2 instance
        is shut down, a host fails, or even when the instance is deleted.

      
    - It is also suitable for applications like databases where durability is a high priority.  Also,
        EBS replicates all data to at least 2 host machines.


    - EBS storage virtualization also suports some advanced features:

        - Can take point-in-time snapshots while the drive is in use and write them to S3
        - Snapshots after full backup are diffs
        - EBS volumes can scale to 64 TB, 250K IOPs



- Local Instance Volumes

    - Cloud providers also offer block storage volumes that are physically attached to the host server
        running a VM.  These storage volumes are generally very low cost (included with the cost of a
        VM in EC2).


    - Instance store volumes behave esentially like a disk physically attached to a server.  One key
        difference, however, is that when a VM shuts down or is deleted, the contents of locally
        attached storage are lost.  This ensures a new VM cannot read disk contents belonging to 
        another customer.


    - Locally attached disks don't support advanced features, like replication, snapshots, or other
        backup features.


    - We can still use them for lots of things, like local caches that don't need features of a service
        like EBS.  For instance, if we are running EMR on EC2 instances, we just need a job that
        consumes data from S3, stores it temporarily in the distributed filesystem across the
        instances, processes it, then writes it back to S3.



- Object Storage

    - Object storage contains 'objects' of all shapes and sizes.  In this case, an object is any type
        of file - TXT, CSV, JSON, images, audio, or videos.


    - AWS S3, Azure Blob Storage, and Google Cloud Storage are widely used object stores.  Also, many
        cloud data warehouses and data lakes sit on object stores.


    - Object storage is easy to manage and use.  It was one of the first 'serverless' services, which
        frees engineers from considering server clusters or disks.


    - An object store is a key-value store for immutable data objects.  Objects don't support random
        writes or appends.  They are written once as a stream of bytes.  To change data, we rewrite
        the entire object.  Random reads are supported through range requests, but they perform much
        worse than data stored on an SSD.


    - Object stores don’t need to support locks or change synchronization, allowing data storage across 
        massive disk clusters.  Object stores support extremely performant parallel stream writes and 
        reads across many disks, and this parallelism is hidden from engineers, who can simply deal with 
        the stream rather than communicating with individual disks.


    - Typical cloud object stores save data in several availability zones, dramatically reducing the 
        odds that storage will go fully offline or be lost in an unrecoverable way. This durability and
        availability are built into the cost.



- Object Stores for DE Applications

    - Object stores provide excellent performance for large batch reads and writes.  This corresponds
        well to the use case for OLAP systems.


    - Object stores are a bad fit for transactional systems or any system with lots of updates per
        second.  Block storage is a much better fit.  Object stores work well for a low rate of updates,
        with each update on a large volume of data.


    - Object stores are now the gold standard for data lakes.  In the early days of data lakes, the
        WORM (Write Once, Read Many) pattern was the operational standard.

      Since then, systems such as Apache Hudi and Delta Lake have emerged to manage the complexity of
        updates, and regulations like GDPR/CCPA have made deletes and updates imperative.  Updates to
        object storage are also the idea behind data lakehouses.



- Object Lookup

    - Unlike file stores, there is no directory tree, only a bucket and key:

        s3://oreilly-data-engineering-book/data-example.json

        Bucket:   s3://oreilly-data-engineering-book/
        Key:      data-example.json


    - Note, you can have keys that look like a directory, but there is no actual directory structure.

        s3://oreilly-data-engineering-book/project-data/11/23/2021/data.txt


    - Many object stores allow directory-like commands such as 'ls', but we need to understand we are
        scanning the entire bucket when we do this, and it could lead to performance issues.



- Object Consistency and Versioning

    - The only way to change an object is to replace it completely.  Object stores may be eventually
        or strongly consistent.


    - Until recently, S3 was eventually consistent.  After a new version of an object was written
        under the same key, the object store might still sometimes return the old object.


    - To impose strong consistency on the object store, we can use a strongly consistent database
        (ie Postgres).  To read an object, a reader will:

        1. Fetch the latest object metadata from Postgres.

        2. Query the object metadata using the object key.  Read the data if it matches the metadata
             from Postgres.

        3. If the metadata doesn't match, repeat step 2 until it does.


    - When we turn on 'object versioning', we add additional metadata to the object that stipulates
        a version.  In this case, the key and version metadata together point to a specific version
        of an object, so we'll always get that exact version back.


    - If we do turn on object versioning, we can end up with many versions of the same object, which
        may be expensive if the objects are large.  We can implement some kind of 'lifecycle policy'
        to delete old objects if newer versions are available.



- Cache and Memory-Based Storage Systems

    - RAM offers excellent latency and transfer speeds, but it is volatile.  RAM-based storage systems
        are generally focused on caching, and data must generally be written to a more durable medium
        for retention.


    - memcached is a key/value store designed for caching DB query results, API call responses, etc.
        It supports only string and integer types.  It can deliver results with very low latency.


    - Redis is also a key/value store, but it supports more complex data types (ie lists or sets).
        It also builds in multiple persistence mechanisms, including snapshotting and journaling.

      With a typical configuration, Redis writes data once every 2 seconds.  It is suitable for 
        extremely high performance applications that can tolerate a small amount of data loss.



- HDFS

    - HDFS is based on the 'GFS (Google File System)' and was initially engineered to process data
        with the MapReduce programming model.  Hadoop is similar to object storage, except that
        Hadoop combines compute and storage on the same node.


    - Hadoop breaks large files into 'blocks' (chunks of data a few hundred MB in size).  The filesystem
        is managed by the 'NameNode', which maintains directories, file metadata, and a detailed 
        catalog of all blocks.


    - In a typical Hadoop cluster, the replication factor is set to 3.


    - Hadoop is often claimed to be 'dead'.  It is not a bleeding-edge tool any more, and tools like
        Pig are relegated to legacy jobs.  MapReduce has fallen by the wayside.

      However, Hadoop still appears in many legacy installations, and many companies with massive
        clusters have no plans to migrate to newer technologies.


    - HDFS is still a key ingredient in many big data engines, including AWS EMR.  Spark is also still
        commonly run on HDFS clusters.



- Streaming Storage

    - Streaming data has different requirements from non-streaming data.  Traditional message queues
        delete data after a certain duration, but distributed frameworks like Kafka allow long-duration
        retention.  Competitors Kinesis, Pulsar, and GCP Pub/Sub also support long retention.


    - The notion of 'replay' is closely related to retention.  It allows a streaming system to return
        a range of historical stored data.



- Indexes, Partitioning, and Clustering

    - Indexes provide a map of the table for particular fields and allow extremely fast lookup of 
        individual records. Without indexes, a database would need to scan an entire table to find the 
        records satisfying a WHERE condition.


    - In most RDBMS's, indexes are used for PKs, FKs, and other columns used to serve the needs of
        specific applications.  Using indexes, an RDBMS can read and write thousands of rows per
        second.


    - In the early days of MPP systems, they were still row-oriented and used indexes to support
        joins and condition checking.


    - Columnar indexes can be used to speed up analytics queries.  In the past, we denormalized data
        and avoid joins in columnar databases, but join performance has been drastically improved.


    - Since we still want to minimize the data we have to read with columnar databases, we can
        partition a table into multiple subtables by splitting it on a field.  Date and time-based
        partitioning are extremely common.

      'Clusters' allow finer-grained organization of data within partitions.



- DE Storage Abstractions

    - DE storage absractions and query patterns are at the heart of the DE lifecycle.  You should
        use the following considerations when choosing abstractions:

            1 Pattern and use case

            2. Update patterns

            3. Cost

            4. Separate storage and compute


    - Separation of storage and compute is blurring the lines between OLAP databases and data lakes,
        with cloud versions of those things rapidly converging.



- The Data Warehouse

    - Data warehouses are a standard OLAP data architecture.  The term 'data warehouse' can refer to:

        - Technology platforms (GCP BigQuery and Teradata)
        - An architecture for data centralization
        - An organizational pattern within a company


    - In terms of storage needs, we've gone from:

        1. Using conventional transactional databases

        2. Row-based MPP systems (Teradata and IBM Netezza)

        3. Columnar NPP systems (Vertica and Teradata Columnar)

        4. Cloud data warehouses and data platforms


    - In practices, since cloud data warehouses can be used to store massive amounts of data, they can
        also be orgainzed into data lakes.  One difference is that cloud data warehouses cannot handle
        truly unstructured data like images, audio, or video.



- The Data Lake

    - The 'data lake' was conceived as a massive data store, where data was retained in raw,
        unprocessed form.  Initially, they were built primarily in Hadoop systems.


    - A massive migration toward separating storage from compute led to movement away from Hadoop 
        toward cloud object storage.


    - Also, engineers discovered that much of the functionality offered by MPP systems, like schema
        management and update/delete/merge capabilities, was extremely useful.  This led to the
        data lakehouse approach.



- The Data Lakehouse

    - The 'data lakehouse' combines aspects of the data warehouse and data lake.  It stores data in
        object storage, while adding schema and table support and facilities for managing updates
        and deletes.


    - Lakehouses also typically support table history and rollback, which is accomplished by retaining
        old versions of files and metadata.


    - Databricks heavily promoted the 'lakehouse' concept with Delta Lake, an open source storage
        management system.  BigQuery and Snowflake also use very similar architectures.


    - Apache Hudi and Apache Iceberg are new competitors to Delta Lake.



- Data Platforms

    - Increasingly, many vendors are styling their products as 'data platforms', where they create
        an ecosystem of core data storage and tools with tight integration to it.  


    - This idea has not been fully fleshed out, but the race is on to create a walled garden of data
        tools, simplifying engineering and generating vendor lock-in.



- Stream-to-Batch Storage Architecture

    - The 'stream-to-batch storage architecture' has many similarities to the Lambda architecture.
        Essentially, data flowing through a topic in the streaming storage system is written out to
        multiple consumers.


    - Some of these consumers might be real-time processing systems that generate statistics on the
        stream.

      In addition, a batch storage consumer writes data for long-term retention and batch queries.


    - The batch consumer could be AWS Kinesis Firehose, which can generate S3 objects based on
        configurable triggers (ie time and batch size).


    - Systems like BigQuery ingest streaming data into a streaming buffer, which is automatically
        reserialized into columnar object storage.  The query engine supports seamless querying of
        both the streaming buffer and object data to provide a current, near-real-time view of the
        table.



- Data Catalog

    - A 'data catalog' is a centralized metadata store for all data across an organization.  It
        integrates with various systems and abstractions.  


    - Data catalogs typically work across operational and analytics data sources, integrating data
        lineage, presentation of data relationships, and allow user editing of data descriptions.


    - Data catalogs are often used to provide a central place where people can view their data, 
        queries, and data storage. As a data engineer, you’ll likely be responsible for setting up 
        and maintaining the various data integrations of data pipeline and storage systems that will 
        integrate with the data catalog and the integrity of the data catalog itself.


    - It makes a lot of sense to provide an API that allows programmatic integration with the data
        catalog.


    - In practice, cataloging systems need some kind of automated scanning system that collects
        metadata from all the various systems.  It can copy existing metadata or infer it for itself.


    - Many catalogs provide some kind of UI that is usually similar to a Wiki system.  This allows 
        users to provide extra information.


    - Catalogs are used for both organizational (allow analysts, data scientists, engineers to discover
        data) and technical (create tables automatically for a data lakehouse) purposes.



- Data Sharing

    - Data sharing allows organizations and individuals to share specific data and carefully defined
        permissions with specific entities. 


    - Data sharing allows data scientists to share data from a sandbox with their collaborators within
        an organization. Across organizations, data sharing facilitates collaboration between partner 
        businesses. For example, an ad tech company can share advertising data with its customers.


    - A cloud multitenant environment makes interorganizational collaboration much easier. However, it 
        also presents new security challenges.  Organizations must carefully control policies that govern 
        who can share data with whom to prevent accidental exposure or deliberate exfiltration.



- Schema

    - Schemas desribe the form, file format, structure, and expected data types of the data.  It 
        doesn't need to be relational.  Data becomes more useful when we have information about it's
        structure and organization.


    - Tranditional data warehouses use schema-on-write.  All writes to the table must conform to the
        existing schema.


    - With schema-on-read, the reader must determine what the schema is supposed to be.  This approach
        allows for maximum flexibility.



- Separation of Compute from Storage

    - Clocation of compute and storage has long been a standard method to improve database performance. 
        For transactional databases, data colocation allows fast, low-latency disk reads and high 
        bandwidth. Even when we virtualize storage (e.g., using Amazon EBS), data is located relatively 
        close to the host machine.


    - The same basic idea applies for analytics query systems running across a cluster of machines. For
        example, with HDFS and MapReduce, the standard approach is to locate data blocks that need to 
        be scanned in the cluster, and then push individual map jobs out to these blocks.  The data scan 
        and processing for the map step are strictly local. 

      The reduce step involves shuffling data across the cluster, but keeping map steps local 
        effectively preserves more bandwidth for shuffling, delivering better overall performance; map 
        steps that filter heavily also dramatically reduce the amount of data to be shuffled.


    - Despite the performance gains of colocation, we have shifted towards separating compute and
        storage, for a few reasons:

        1. It is efficient to use ephemeral servers, which you can scale up and down according to 
             business needs.

        2. Cloud object stores significantly mitigate the risk of data loss and have very high uptime.
             If a natural disaster destroys a zone, the data will remain in the remaining zones.


    - In practice, we constanty take hybrid approaches between colocation and separation.

      With 'multitier caching', we use object storage for long-term data retention and access, but 
        but spin up local storage to be used during queries and various stages of the data pipeline.
        Both AWS and GCP support hybrid object storage.


    - Hybrid Example - AWS EMR with S3 and HDFS

        - EMR spins up temporary HDFS clusters to process data.  DE's can reference both S3 and HDFS
            as a filesystem.

        - A common pattern is to stand up HDFS on SSD drives, pull from S3, and save data from
            intermediate processing steps on HDFS.  This improves performance over just using S3.

        - After processing is complete, results are stored back to S3.


    - Hybrid Example - Apache Spark

        - In practice, Spark generally runs jobs on HDFS or some other ephemeral distributed
            filesystem to support performant storage of data between processing steps.

        - In addition, Spark relies heavily on in-memory storage of data to improve processing.

        - Since DRAM is very expensive, it makes sense to rent large quantities of memory in the
            cloud, then release it when we are finished.


    - Hybrid Example - Apache Druid

        - Druid relies heavily on SSDs to achieve high performance.  Since SSDs are much more expensive
            than HDDs, Druid just stores one copy of data in the cluster.

        - To maintain durability, Druid uses object storage as it's durability layer.

        - When data is ingested, it's processed, serialized into compressed columns, and written to
            cluster SSDs and object storage.

        - If a node fails, data can be automatically recovered to a new node.


    - Hybrid Example - Hybrid Object Storage

        - Google Colossus file system supports fine-grained control of data block location, although
            this is not directly exposed to users.  BigQuery uses it to colocate customer tables in a
            single location.

        - We call this approach 'hybrid object storage' because it combines the clean abstractions of
            object storage with some advantages of colocation.


    - Zero Copy Cloning

        - Cloud based systems based around object storage support 'zero copy cloning', which means a
            new copy of an object is created (ie a new table) without necessarily physically copying
            the underlying data.

        - New pointers are created to the raw data files, and future changes will not be recorded in
            the old table.  This is like shallow copying in Python.

        - This approach needs to be used very carefully.  For instance, we need to be careful not to
            accidentally delete the original object.



- Data Storage Lifecycle

    - We also need to think about the data storage lifecycle and data retention.  For instance, should
        we keep data around forever, or discard it after a certain time frame?


    - Hot Data

        - Accessed very frequently
        - High storage cost, Low retrieval cost
        - Store in SSDs or memory
        - Example - product recommendations and product page results
        - Example - query results cache


    - Warm Data

        - Accessed infrequently
        - Medium storage cost, medium retrieval cost
        - S3 'Infrequently Accessed Tier'


    - Cold Data

        - Accessed infrequently
        - Cheap storage cost, high retrieval cost
        - Store in HDDs, tape storage, cloud-based archival systems


    - DEs should be mindful of what happens when hotter storage runs out of space and spills over to
        colder storage (ie when memory contents are dumped to disk).


    - If you use cloud strage, you can create automated lifecycle policies, which can drastically
        reduce your storage cost.



- Data Retention

    - In the early days of big data, there was a tendency to err on the side of 'keep everything forever
        in case you might need it'.  This led to a swamp situation with regulatory nightmares.


    - How much value does the data have?  Could it recreated from other data if needed?  What is the
        impact to downstream systems of deleting it?


    - Newer data is usually more valuable than older data.


    - Certain regulations (ie HIPAA and PCI) might require you to keep data for a certain time.


    - Data is an asset that has a ROI.  Delete it if it has no purpose.



- Single-Tenant vs Multitenant Storage

    - Using single-tenant storage means that every tenant gets their own storage (ie their own DB).
        No data is shared, and all storage is isolated.  For instance, if you give each customer
        their own database.  Schema variation among customers makes applications harder to write,
        but provides a lot of flexibility.


    - Multitenant storage allows for multiple customers' data to reside in the same database.



- Security

    - Robust security and fine-grained access controls mean that data will be allowed to be shared
        and consumed more widely within a business.  This raises the value of data.


    - Use the principle of least privilege.  Don't give full database access to anyone unless
        explicitly required.  Most DE's don't need full access.  Pay attention to row, column, and
        cell-level access.  Give users the information they need and no more.