-----------------------------------------------------------------------------
| CHAPTER 5 - DATA GENERATION IN SOURCE SYSTEMS                             |
-----------------------------------------------------------------------------

- Sources of Data

    - 'Analog data' creation occurs in the real world (ie speech, sign language, writing, music).
        It is often transient.


    - 'Digital data' is either created by converting analog data to digital form, or it is the native
        product of a digital system.



- Source Systems

    - A 'file' is a sequence of bytes, typically stored on disk.  They are a universal medium of data
        exchange.  The most common files we'll run into are CSV, TXT, JSON, XML, and XLS.  Data
        engineers also use Parquet, ORC, and Avro.


    - APIs are a standard way of exchanging data between systems.  DEs often invest a lot of energy
        into maintaining custom API connections.


    - OLTP databases support a business's ongoing transactions and operations.  They can handle
        thousands of reads and writes per second.  They commonly support ACID transactions.


    - To avoid affecting the operations of OLTP databases, we use OLAP databases to run analytics
        processing.  These systems are optimized to handle queries that scan lots of records, and
        are inefficient at lookups of individual records.  DE's will frequently need to read from
        them for ML or reverse ETL.


    - 'Change Data Capture (CDC)' is a method for extracting each change event (insert, update, delete)
        that occurs in a database.  It is used for replication or generating an event stream for
        downstream processing.  The method is based on the database technology.


    - Logs

        - A 'log' captures information about events that occur in systems.  Common sources of logs 
            include OS, applications, servers, containers, networks, and IoT devices.  

        - Logs should capture who, what, and when at a minimum.

        - Logs are encoded in binary form (database logs), semistructured form (JSON), or plain text
            (stdout from an application).

        - Logs have a 'log level' that refers to the amount of event data captured (the 'resolution').

        - Log messages can be written in batches or in real-time.  Individual log entries can be
            written to a messaging system such as Kafka or Pulsar in real-time applications.


    - Databases often use WALs (usually binary files stored in a database-native format).  They are
        used for recovery, but they also play an important role in generating CDC events in DE
        pipelines.


    - The 'insert-only' pattern can be used to preserve the entire history of changes.  This has to
        be used carefully, since tables can grow quite large and this can become inefficient.



- Messages and Streams

    - A 'message' is raw data communicated across 2 or more systems.  A message is typically sent
        through a 'message queue' from a publisher to a consumer.  Once it is delivered, it is removed
        from the queue.


    - Messages are discrete and singular signals in an event-driven system.  For example, a temperature
        reading from an IoT device.


    - By contrast, a 'stream' is an append-only log of event records.  Streams are ingested and stored
        in 'event-streaming platforms'.


    - As events occur, they are accumulated in an ordered sequence.  A timestamp or ID might order
        events.  Note that events aren't always delivered in exact order because of subtleties of
        distribued systems.


    - We use streams when we care about what happened over many events.  Because of the append-only
        nature of streams, records in a stream are persisted over a long retention window (weeks or
        months).  This allows for complex aggregations and the ability to rewind to a point in time.



- Types of Time

    - Time is an essential consideration for all data ingestion, and becomes even more critical in the
        context of streaming.


        Event Time      Ingestion Time       Process Time
        --------------------------------------------------->
           (Processing Time)


    - 'Event time' indicates when an event is generated in a source system.  'Ingestion time' indicates
        when an event was brought into some kind of storage.  'Process time' indicates when the event
        was processed.


    - You typically want to record all 3 of these times for monitoring purposes.



- Databases

    - The main characteristics to look for when evaluating a database technology:

        - DBMS = storage engine, query optimizer, disaster recovery, other key components
        - Lookups = how does the DB index and retrieve data
        - Query Optimizer
        - Scaling and distribution
        - Modeling patterns
        - CRUD
        - Consistency


    - RDBMS Characteristics

        - Relations (rows) and fields (columns)
        - Rows stored as contiguous sequence of bytes on disk
        - Tables indexed by PK
        - FKs for joins
        - Normalized schema (typically)
        - ACID


    - Key/Value Stores

        - Records retrieved using unique key
        - In-memory key/value stores used for caching session data for web applications
        - Durable key/value stores used to save and update massive amount of state changes
            (ie ecommerce application shopping cart)


    - Document Stores

        - Specialized key/value store
        - Documents are typically JSON objects
        - Documents are stored in collections (similar to tables) and retrieved by key
        - Joins are not supported, which makes normalization difficult
        - Flexibility of JSON without enforced schema
        - Need to be very careful with schema changes and downstream impacts
        - Most document stores allow you to create indexes on fields other than key
        - Typically need to run a full scan to use for analytics


    - Wide-Column

        - Optimized for storing massive amounts of data, high transaction rates, low latency
        - Can support PB of data and millions of requests per second
        - Popular in ecommerce, fintech, ad tech, IoT, real-time applications
        - Do not support complex queries, no index besides row keys
        - Typically need to export data to an analytics DB for complex queries


    - Graph Databases

        - Store data as a set of nodes and edges
        - Used to analyze connectivity between elements
        - Example: how many users can be reached by traversing 2 direct connections
    

    - Search

        - Used to search data's complex and straightforward semantic and structural characteristics
        - 2 prominent use cases: text search and log analysis
        - Text search is searching a body of text for keywords or phrases
        - Log analysis is used for anomaly detection, real-time monitoring, security analytics


    - Time Series

        - A 'time series' is a series of values organized by time (ie stock prices throughout the day)
        - A 'time series database' is optimized for statistical analysis of this data
        - High-velocity data from IoT, event and application logs, ad tech, fintech
        - Write-heavy workloads
        - 'Measurement data' is generated at regular intervals (ie temperature readings)
        - 'Event-based data' is generated at irregular intervals



- APIs

    - REST

        - Dominant paradigm, laid out by Roy Fielding in 2000
        - Interactions are stateless
        - Widely supported with client libraries and connector support


    - GraphQL

        - Created by Facebook as a REST alternative to allow more flexible and expressive queries
        - Build around returning JSON


    - Webhooks

        - When events happen in the source system, trigger a call to an HTTP endpoint hosted by consumer
        - Source can be application backend, web page, or mobile app
        - Often called 'reverse APIs' since data goes from source system to data sync
    

    - RPC and gRPC

        - An 'RPC' is a pattern that allows you to run a procedure on a remote system
        - 'gRPC' is an RPC library developed by Google in 2015
        - Built around Protocol Buffers format, also from Google
        - Efficient bidirectional exchange of data over HTTP/2



- Message Queues and Event-Streaming Platforms

    - Message Queues

        - A 'message queue' is a mechanism to send data (usually small messages) asynchronously
        - Allow for decoupling of systems, widely used in microservices architectures
        - Queue buffers messages to handle load spikes
        - Can use replication to add durability


    - Message Ordering and Delivery

        - Order in distributed message queues is a tricky problem
        - Most queues try their best to be FIFO, this can be strongly guaranteed with extra overhead
        - Don't assume messages will delivered in order unless technology guarantees it


    - Delivery Frequency

        - If a message is sent 'excatly once', the message disappears after the consumer acks it
        - If it is sent 'at least once', it can be consumed by multiple subscribers or more than once
        - Ideally, systems should be idempotent, which helps ease the difficulty of exactly once


    - Event-Streaming Platforms

        - Used to ingest and process data in an ordered log of records
        - Data is retained and can commonly be replayed


    - Topics

        - A producer sends events to a topic, a collection of related events
        - A topic can have 0, 1, or multiple producers and consumers


          Order Processing System   ->   Web Orders Topic  ->  Marketing
                                                           ->  Fulfillment


    - Stream Partitions

        - Stream partitions are subdivisions of a stream into multiple streams
        - Messages are distributed across partitions by using the parition key
        - Messages that should be processed together have the same partiton key
        - For instance, all messages from a device go to same partition key, use device_id as key



- Security

    - Is the source system architected so data is secure and encrypted, both with data at rest and 
        while data is transmitted?


    - Do you have to access the source system over the public internet, or are you using a virtual 
        private network (VPN)?


    - Keep passwords, tokens, and credentials to the source system securely locked away. For example, 
        if you’re using Secure Shell (SSH) keys, use a key manager to protect your keys; the same rule
        applies to passwords—use a password manager or a single sign-on (SSO) provider.


    - Do you trust the source system? Always be sure to trust but verify that the source system is 
        legitimate. You don’t want to be on the receiving end of data from a malicious actor.



- Data Management

    - How do you ensure data integrity in upstream systems?


    - How will you be notified of schema changes?


    - Do we have access to raw data, or will it be obfuscated?  How long should it be retained?  Are
        we able to access it based on regulations?



- DataOps

    - How will you be notified of an outage in a source system?  It may be a good idea to have 
        monitoring on all source systems.


    - If something bad does happen, how will your data pipeline be affected?  Can you skip the step?
        What is the plan to backfill the data when the system is back online?